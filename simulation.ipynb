{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b7cc4-c1da-49fb-85a5-29eee064dd0d",
   "metadata": {},
   "source": [
    "# Some preliminary checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14976ee1-cfb2-4052-a40b-0f316a54d317",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "import ray \n",
    "print(\"Ray version :\", ray.__version__)\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "print(\"onnx Available:\", torch.onnx.is_onnxrt_backend_supported())\n",
    "torch._dynamo.list_backends()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e711d-75a9-4f97-ac7f-092552034613",
   "metadata": {},
   "source": [
    "### Important : Number of CPUs and GPUs available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367a277-f21e-4ccb-9fc1-c5c1c1dd292b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# print number of gpus / CPUs\n",
    "print(\"Number of CPUs: \", psutil.cpu_count())\n",
    "\n",
    "num_cpus = 16\n",
    "num_gpus = 1\n",
    "num_learner = 1\n",
    "\n",
    "assert num_cpus <= psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421e6f2-e64c-4f7c-8ef2-cd969f9888fe",
   "metadata": {},
   "source": [
    "# Environement and algorithm configuration\n",
    "\n",
    "Some of the commented lines are preparation work to use a futur feature of RLLib\n",
    "\n",
    "Note: In multi-agent environments, `rollout_fragment_lenght` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec\n",
    "#from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\n",
    "#from ray.rllib.core.rl_module.marl_module import MultiAgentRLModuleSpec\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from particle_2d_env import Particle2dEnvironment\n",
    "from particle_2d_env import MetricsCallbacks\n",
    "from config import run_config\n",
    "\n",
    "ALGO = \"PPO\"        \n",
    "FRAMEWORK= \"torch\" # \"tf2\" or \"torch\"\n",
    "env = Particle2dEnvironment(run_config[\"env\"])\n",
    "\n",
    "config = (\n",
    "    get_trainable_cls(ALGO).get_default_config()\n",
    "    .environment(Particle2dEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(\n",
    "        FRAMEWORK,\n",
    "    )\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=False,\n",
    "    )\n",
    "    .callbacks(MetricsCallbacks)\n",
    "    .resources(\n",
    "        # learner workers when using learner api - doesn't work on arm (mac) yet\n",
    "        num_learner_workers=num_learner,\n",
    "        num_gpus_per_learner_worker=num_gpus/num_learner, # always 1 for PPO\n",
    "    )\n",
    "    # Specify the learner's hyperparameters.\n",
    "    .training(\n",
    "        num_sgd_iter=5,          \n",
    "        sgd_minibatch_size=512,             # the batch size (only for PPO)\n",
    "        #minibatch_size = 256,                # the batch size (only for IMPALA)\n",
    "        train_batch_size=524288,             # the number of step collected\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [128, 128, 128], \n",
    "            \"use_attention\": True,\n",
    "            #\"use_lstm\": False,\n",
    "            #\"max_seq_len\": 5,\n",
    "            #\"lstm_cell_size\": 16,\n",
    "        },\n",
    "        #lr=tune.grid_search([0.01, 0.001, 0.0001])\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"],\n",
    "        count_steps_by=\"agent_steps\",\n",
    "    )\n",
    "    .env_runners(\n",
    "        rollout_fragment_length=\"auto\", #\"auto\" for PPO explained here : https://docs.ray.io/en/latest/rllib/rllib-sample-collection.html\n",
    "        batch_mode= 'truncate_episodes',\n",
    "        num_env_runners=num_cpus-num_learner*0-1, # need 2 for IMPALA, 1 for PPO\n",
    "        num_envs_per_env_runner=2,\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd95c1-c2d5-439c-9478-cf1f15c10244",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfa99f-260e-4407-814c-b2f6d715797e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "path_to_checkpoint = None #os.getcwd() + \"/ray_results\" + \"PPO_2024-05-18_00-08-19/PPO_Particle2dEnvironment_bb60c_00000_0_2024-05-18_00-08-19/checkpoint_000001\"\n",
    "\n",
    "def restore_weights(path, policy_type):\n",
    "    checkpoint_path = os.path.join(path, f\"policies/{policy_type}\")\n",
    "    restored_policy = Policy.from_checkpoint(checkpoint_path)\n",
    "    return restored_policy.get_weights()\n",
    "\n",
    "if path_to_checkpoint is not None: \n",
    "    class RestoreWeightsCallback(DefaultCallbacks):\n",
    "        def on_algorithm_init(self, *, algorithm: \"Algorithm\", **kwargs) -> None:\n",
    "            algorithm.set_weights({\"predator\": restore_weights(path_to_checkpoint, \"predator\")})\n",
    "            algorithm.set_weights({\"prey\": restore_weights(path_to_checkpoint, \"prey\")})\n",
    "\n",
    "    config.callbacks(RestoreWeightsCallback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5385022-05b2-42ae-a406-87b46736f257",
   "metadata": {},
   "source": [
    "## Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056eb565-f91d-4c3c-bced-93a56886dca0",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "from ray.tune import Tuner\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "import os\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=num_cpus, \n",
    "    num_gpus=num_gpus\n",
    ")\n",
    "\n",
    "# Stop criterium\n",
    "stop = {  \n",
    "    \"training_iteration\": 1500,\n",
    "    #\"timesteps_total\": 200000000,\n",
    "}\n",
    "\n",
    "# Read the API key from the file to use Wanddb\n",
    "with open('wandb_api_key.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "callbacks = [\n",
    "    WandbLoggerCallback(                   \n",
    "        project=\"marl-rllib\", \n",
    "        group=ALGO,\n",
    "        api_key=api_key,\n",
    "        log_config=True,\n",
    "        upload_checkpoints=True\n",
    "    ), \n",
    "]\n",
    "\n",
    "# When to save the models \n",
    "checkpoint_config = train.CheckpointConfig(         \n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_frequency=10,\n",
    ")\n",
    "\n",
    "# Where to save \n",
    "# absolute path + ray_results directory\n",
    "storage_path=os.getcwd() + \"/ray_results\"\n",
    "\n",
    "if path_to_checkpoint is None : \n",
    "    tuner = tune.Tuner(\n",
    "        ALGO,                                                 # Defined before\n",
    "        param_space=config,                                   # Defined before\n",
    "        run_config=train.RunConfig(    \n",
    "            storage_path=storage_path,\n",
    "            stop=stop,\n",
    "            verbose=3,\n",
    "            callbacks=callbacks,\n",
    "            checkpoint_config=checkpoint_config,\n",
    "        ),\n",
    "    )\n",
    "    # Run the experiment \n",
    "    results = tuner.fit()\n",
    "\n",
    "# If we instantiate previously trained neural network\n",
    "else: \n",
    "    callbacks.append(RestoreWeightsCallback)\n",
    "\n",
    "    results = tune.run(\n",
    "        ALGO,\n",
    "        config=config.to_dict(),\n",
    "        storage_path=storage_path,\n",
    "        stop=stop,\n",
    "        verbose=3,\n",
    "        callbacks=callbacks,\n",
    "        checkpoint_config=checkpoint_config,\n",
    "    )\n",
    "\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43c80fe3-9e29-46f5-95b4-38e1f582eb27",
   "metadata": {},
   "source": [
    "from ray import tune\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=num_cpus, \n",
    "    num_gpus=num_gpus\n",
    ")\n",
    "\n",
    "experiment_dir = os.getcwd() + \"/ray_results\" + \"PPO_2023-12-10_17-58-05/\"\n",
    "# Restore the training\n",
    "tuner = tune.Tuner.restore(\n",
    "    experiment_dir, \n",
    "    trainable=tune.with_resources(\n",
    "        tune.with_parameters(self.model),\n",
    "        resources={\"cpu\": self.cpuFrac, \"gpu\": self.gpuFrac}\n",
    "    ),\n",
    "    resume_unfinished=True, \n",
    "    resume_errored=True,\n",
    "    restart_errored=False,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d90a6-5207-4d0c-86bb-c1bef3ae8ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39109cdb-4e78-4dee-b0bc-fb0d61b98b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9e5d6-7321-4fb2-ba7c-a8f35dfd5074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_env",
   "language": "python",
   "name": "collective_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
