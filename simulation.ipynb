{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b7cc4-c1da-49fb-85a5-29eee064dd0d",
   "metadata": {},
   "source": [
    "# Some preliminary checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14976ee1-cfb2-4052-a40b-0f316a54d317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T21:02:20.453653Z",
     "iopub.status.busy": "2024-12-01T21:02:20.452853Z",
     "iopub.status.idle": "2024-12-01T21:02:21.625459Z",
     "shell.execute_reply": "2024-12-01T21:02:21.625198Z",
     "shell.execute_reply.started": "2024-12-01T21:02:20.453613Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray version : 2.39.0\n",
      "PyTorch Version: 2.5.1\n",
      "CUDA Available: False\n",
      "MPS Available: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'tvm']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "#os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "import ray \n",
    "\n",
    "print(\"Ray version :\", ray.__version__)\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "\n",
    "torch._dynamo.list_backends()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e711d-75a9-4f97-ac7f-092552034613",
   "metadata": {},
   "source": [
    "### Important : Number of CPUs and GPUs available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9367a277-f21e-4ccb-9fc1-c5c1c1dd292b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T21:02:21.626199Z",
     "iopub.status.busy": "2024-12-01T21:02:21.626083Z",
     "iopub.status.idle": "2024-12-01T21:02:21.628130Z",
     "shell.execute_reply": "2024-12-01T21:02:21.627915Z",
     "shell.execute_reply.started": "2024-12-01T21:02:21.626192Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs:  12\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# print number of gpus / CPUs\n",
    "print(\"Number of CPUs: \", psutil.cpu_count())\n",
    "\n",
    "num_cpus = 12\n",
    "num_gpus = 0\n",
    "num_learner = 1\n",
    "\n",
    "assert num_cpus <= psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421e6f2-e64c-4f7c-8ef2-cd969f9888fe",
   "metadata": {},
   "source": [
    "# Environement and algorithm configuration\n",
    "\n",
    "Some of the commented lines are preparation work to use a futur feature of RLLib\n",
    "\n",
    "Note: In multi-agent environments, `rollout_fragment_lenght` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T21:02:21.628476Z",
     "iopub.status.busy": "2024-12-01T21:02:21.628403Z",
     "iopub.status.idle": "2024-12-01T21:02:24.511709Z",
     "shell.execute_reply": "2024-12-01T21:02:24.511400Z",
     "shell.execute_reply.started": "2024-12-01T21:02:21.628468Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2024-12-01 22:02:24,509\tWARNING deprecation.py:50 -- DeprecationWarning: `config.training(num_sgd_iter=..)` has been deprecated. Use `config.training(num_epochs=..)` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec\n",
    "#from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\n",
    "#from ray.rllib.core.rl_module.marl_module import MultiAgentRLModuleSpec\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from particle_2d_env import Particle2dEnvironment\n",
    "from particle_2d_env import MetricsCallbacks, RenderingCallbacks\n",
    "from config import run_config\n",
    "\n",
    "ALGO = \"PPO\"        \n",
    "FRAMEWORK= \"torch\" # always \"torch\"\n",
    "env = Particle2dEnvironment(run_config[\"env\"])\n",
    "\n",
    "config = (\n",
    "    get_trainable_cls(ALGO).get_default_config()\n",
    "    .environment(Particle2dEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(FRAMEWORK,)\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "    #.callbacks(MetricsCallbacks)\n",
    "    #.callbacks(RenderingCallbacks)\n",
    "    # Specify the learner's hyperparameters.\n",
    "    .training(\n",
    "        num_sgd_iter=5,          \n",
    "        num_epochs=10,\n",
    "        train_batch_size_per_learner=512,             # the number of step collected\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [128, 128, 128], \n",
    "            \"use_attention\": True,\n",
    "            #\"use_lstm\": False,\n",
    "            #\"max_seq_len\": 5,\n",
    "            #\"lstm_cell_size\": 16,\n",
    "        },\n",
    "        #lr=tune.grid_search([0.01, 0.001, 0.0001])\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.particule_agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"],\n",
    "        count_steps_by=\"agent_steps\",\n",
    "    )\n",
    "    .learners(\n",
    "        num_learners=4,  # or >2\n",
    "        num_cpus_per_learner=1,  # <- default\n",
    "        num_gpus_per_learner=0,  # <- default\n",
    "    )\n",
    "    .resources(num_cpus_for_main_process=1)  # default is 1\n",
    "    .env_runners(\n",
    "        rollout_fragment_length=\"auto\", #\"auto\" for PPO explained here : https://docs.ray.io/en/latest/rllib/rllib-sample-collection.html\n",
    "        batch_mode= 'truncate_episodes',\n",
    "        num_env_runners=1, # need 2 for IMPALA, 1 for PPO\n",
    "        num_envs_per_env_runner=1,\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd95c1-c2d5-439c-9478-cf1f15c10244",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3cfa99f-260e-4407-814c-b2f6d715797e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T21:02:24.513153Z",
     "iopub.status.busy": "2024-12-01T21:02:24.512948Z",
     "iopub.status.idle": "2024-12-01T21:02:24.515973Z",
     "shell.execute_reply": "2024-12-01T21:02:24.515710Z",
     "shell.execute_reply.started": "2024-12-01T21:02:24.513145Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "path_to_checkpoint = None #os.getcwd() + \"/ray_results\" + \"PPO_2024-05-18_00-08-19/PPO_Particle2dEnvironment_bb60c_00000_0_2024-05-18_00-08-19/checkpoint_000001\"\n",
    "\n",
    "def restore_weights(path, policy_type):\n",
    "    checkpoint_path = os.path.join(path, f\"policies/{policy_type}\")\n",
    "    restored_policy = Policy.from_checkpoint(checkpoint_path)\n",
    "    return restored_policy.get_weights()\n",
    "\n",
    "if path_to_checkpoint is not None: \n",
    "    class RestoreWeightsCallback(DefaultCallbacks):\n",
    "        def on_algorithm_init(self, *, algorithm: \"Algorithm\", **kwargs) -> None:\n",
    "            algorithm.set_weights({\"predator\": restore_weights(path_to_checkpoint, \"predator\")})\n",
    "            algorithm.set_weights({\"prey\": restore_weights(path_to_checkpoint, \"prey\")})\n",
    "\n",
    "    config.callbacks(RestoreWeightsCallback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5385022-05b2-42ae-a406-87b46736f257",
   "metadata": {},
   "source": [
    "## Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056eb565-f91d-4c3c-bced-93a56886dca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-01T21:02:24.516478Z",
     "iopub.status.busy": "2024-12-01T21:02:24.516299Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 22:02:25,152\tINFO worker.py:1819 -- Started a local Ray instance.\n",
      "2024-12-01 22:02:25,794\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-12-01 22:06:50</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:25.13        </td></tr>\n",
       "<tr><td>Memory:      </td><td>24.1/64.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 6.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      num_env_steps_sample\n",
       "d_lifetime</th><th style=\"text-align: right;\">   num_episodes_lifetim\n",
       "e</th><th style=\"text-align: right;\">      num_env_steps_traine\n",
       "d_lifetime</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Particle2dEnvironment_91321_00000</td><td>RUNNING </td><td>127.0.0.1:34265</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         239.373</td><td style=\"text-align: right;\">26624</td><td style=\"text-align: right;\">38</td><td style=\"text-align: right;\">26624</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 22:02:25,811\tWARNING algorithm_config.py:4484 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "2024-12-01 22:02:25,813\tWARNING ppo.py:305 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2024-12-01 22:02:25,813\tWARNING algorithm_config.py:4484 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "2024-12-01 22:02:25,814\tWARNING ppo.py:305 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(PPO pid=34265)\u001b[0m 2024-12-01 22:02:29,767\tWARNING algorithm_config.py:4484 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "\u001b[36m(PPO pid=34265)\u001b[0m 2024-12-01 22:02:29,767\tWARNING ppo.py:305 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(MultiAgentEnvRunner pid=34271)\u001b[0m 2024-12-01 22:02:33,899\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=34271)\u001b[0m 2024-12-01 22:02:33,899\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=34265)\u001b[0m 2024-12-01 22:02:33,956\tWARNING algorithm_config.py:4484 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "\u001b[36m(PPO pid=34265)\u001b[0m 2024-12-01 22:02:33,957\tWARNING ppo.py:305 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(_WrappedExecutable pid=34278)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(PPO pid=34265)\u001b[0m 2024-12-01 22:02:33,941\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001b[36m(PPO pid=34265)\u001b[0m 2024-12-01 22:02:33,941\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(_WrappedExecutable pid=34278)\u001b[0m 2024-12-01 22:02:39,762\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001b[36m(_WrappedExecutable pid=34278)\u001b[0m 2024-12-01 22:02:39,763\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=34265)\u001b[0m Trainable.setup took 10.040 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[36m(PPO pid=34265)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m wandb: Currently logged in as: tanguy-cazalets (tcazalet_airo). Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WrappedExecutable pid=34279)\u001b[0m 2024-12-01 22:02:39,765\tWARNING rl_module.py:427 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. Some algos already use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_WrappedExecutable pid=34279)\u001b[0m 2024-12-01 22:02:39,766\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m wandb: Tracking run with wandb version 0.18.7\n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m wandb: Run data is saved locally in /private/tmp/ray/session_2024-12-01_22-02-24_528317_34233/artifacts/2024-12-01_22-02-25/PPO_2024-12-01_22-02-25/driver_artifacts/PPO_Particle2dEnvironment_91321_00000_0_2024-12-01_22-02-25/wandb/run-20241201_220244-91321_00000\n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m wandb: Syncing run PPO_Particle2dEnvironment_91321_00000\n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/tcazalet_airo/marl-rllib\n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m wandb: 🚀 View run at https://wandb.ai/tcazalet_airo/marl-rllib/runs/91321_00000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>date               </th><th>done  </th><th>env_runners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </th><th>fault_tolerance                                                                           </th><th>hostname                   </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>learners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </th><th>node_ip  </th><th>num_agent_steps_sampled_lifetime                                                                                                                                                                                                                                                                                                                                                                                      </th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_env_steps_trained_lifetime</th><th style=\"text-align: right;\">  num_episodes_lifetime</th><th>perf                                                                                                    </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                                                                                                                                                              </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Particle2dEnvironment_91321_00000</td><td>2024-12-01_22-06-43</td><td>False </td><td>{&#x27;agent_episode_returns_mean&#x27;: {&#x27;22&#x27;: -0.3422, &#x27;17&#x27;: -0.047700000000000006, &#x27;11&#x27;: -0.1582, &#x27;12&#x27;: -0.09910000000000001, &#x27;14&#x27;: -0.2768, &#x27;29&#x27;: -0.1886, &#x27;0&#x27;: -0.09559999999999999, &#x27;5&#x27;: -0.033, &#x27;15&#x27;: -0.12140000000000001, &#x27;20&#x27;: -0.34090000000000004, &#x27;7&#x27;: -0.4571, &#x27;23&#x27;: -0.10880000000000001, &#x27;2&#x27;: -0.011100000000000004, &#x27;1&#x27;: -0.39880000000000004, &#x27;4&#x27;: -0.4071, &#x27;28&#x27;: -0.0917, &#x27;25&#x27;: -0.48700000000000004, &#x27;30&#x27;: 2.733600000000001, &#x27;16&#x27;: -0.1284, &#x27;10&#x27;: -0.25189999999999996, &#x27;19&#x27;: -0.2658, &#x27;9&#x27;: -0.0105, &#x27;8&#x27;: -0.20740000000000003, &#x27;24&#x27;: -0.3827, &#x27;18&#x27;: -0.26289999999999997, &#x27;27&#x27;: -0.1081, &#x27;13&#x27;: -0.012600000000000002, &#x27;21&#x27;: -0.22149999999999997, &#x27;31&#x27;: 2.4890000000000017, &#x27;26&#x27;: 0.0, &#x27;6&#x27;: -0.20299999999999996, &#x27;3&#x27;: -0.251}, &#x27;num_agent_steps_sampled&#x27;: {&#x27;23&#x27;: 2048, &#x27;2&#x27;: 2048, &#x27;1&#x27;: 2048, &#x27;19&#x27;: 2048, &#x27;9&#x27;: 2048, &#x27;24&#x27;: 2048, &#x27;18&#x27;: 1619, &#x27;27&#x27;: 2048, &#x27;16&#x27;: 2048, &#x27;13&#x27;: 2048, &#x27;10&#x27;: 2048, &#x27;31&#x27;: 2048, &#x27;6&#x27;: 2048, &#x27;8&#x27;: 1360, &#x27;3&#x27;: 2048, &#x27;21&#x27;: 2048, &#x27;17&#x27;: 2048, &#x27;11&#x27;: 2048, &#x27;26&#x27;: 2048, &#x27;29&#x27;: 2048, &#x27;0&#x27;: 2048, &#x27;15&#x27;: 2048, &#x27;20&#x27;: 2048, &#x27;7&#x27;: 2048, &#x27;22&#x27;: 2039, &#x27;4&#x27;: 1381, &#x27;12&#x27;: 2030, &#x27;28&#x27;: 2048, &#x27;25&#x27;: 2048, &#x27;14&#x27;: 2048, &#x27;30&#x27;: 2048, &#x27;5&#x27;: 2048}, &#x27;episode_return_max&#x27;: 0.0, &#x27;num_agent_steps_sampled_lifetime&#x27;: {&#x27;7&#x27;: 179482, &#x27;23&#x27;: 179219, &#x27;2&#x27;: 186368, &#x27;1&#x27;: 175889, &#x27;4&#x27;: 177269, &#x27;19&#x27;: 177811, &#x27;25&#x27;: 175960, &#x27;18&#x27;: 183893, &#x27;27&#x27;: 180552, &#x27;16&#x27;: 184816, &#x27;13&#x27;: 186368, &#x27;10&#x27;: 182402, &#x27;31&#x27;: 186368, &#x27;6&#x27;: 179155, &#x27;9&#x27;: 186368, &#x27;8&#x27;: 181040, &#x27;24&#x27;: 178818, &#x27;21&#x27;: 176928, &#x27;17&#x27;: 183508, &#x27;11&#x27;: 179900, &#x27;26&#x27;: 186368, &#x27;29&#x27;: 181506, &#x27;15&#x27;: 178218, &#x27;3&#x27;: 178439, &#x27;20&#x27;: 177645, &#x27;22&#x27;: 179485, &#x27;12&#x27;: 184010, &#x27;28&#x27;: 186332, &#x27;14&#x27;: 179590, &#x27;30&#x27;: 186368, &#x27;0&#x27;: 185060, &#x27;5&#x27;: 185120}, &#x27;episode_return_min&#x27;: -1.95999999999996, &#x27;agent_steps&#x27;: {&#x27;22&#x27;: 655.64, &#x27;21&#x27;: 651.82, &#x27;17&#x27;: 682.87, &#x27;11&#x27;: 687.31, &#x27;29&#x27;: 672.61, &#x27;0&#x27;: 686.95, &#x27;5&#x27;: 697.94, &#x27;15&#x27;: 677.5, &#x27;20&#x27;: 652.87, &#x27;7&#x27;: 665.86, &#x27;2&#x27;: 700.0, &#x27;1&#x27;: 662.26, &#x27;4&#x27;: 635.0, &#x27;12&#x27;: 692.84, &#x27;28&#x27;: 699.66, &#x27;25&#x27;: 641.35, &#x27;14&#x27;: 672.8, &#x27;30&#x27;: 700.0, &#x27;23&#x27;: 680.36, &#x27;10&#x27;: 688.54, &#x27;19&#x27;: 664.27, &#x27;9&#x27;: 700.0, &#x27;24&#x27;: 669.4, &#x27;18&#x27;: 679.94, &#x27;27&#x27;: 677.92, &#x27;16&#x27;: 694.21, &#x27;13&#x27;: 700.0, &#x27;31&#x27;: 700.0, &#x27;26&#x27;: 700.0, &#x27;6&#x27;: 680.76, &#x27;8&#x27;: 680.08, &#x27;3&#x27;: 646.81}, &#x27;num_env_steps_sampled_lifetime&#x27;: 186368, &#x27;module_episode_returns_mean&#x27;: {&#x27;predator&#x27;: 2.9235000000000015, &#x27;prey&#x27;: -1.1293}, &#x27;episode_len_max&#x27;: 700, &#x27;episode_return_mean&#x27;: -0.7482999999999972, &#x27;num_env_steps_sampled&#x27;: 2048, &#x27;num_module_steps_sampled&#x27;: {&#x27;prey&#x27;: 59629, &#x27;predator&#x27;: 4096}, &#x27;episode_len_min&#x27;: 700, &#x27;episode_duration_sec_mean&#x27;: 3.5693788301096356, &#x27;num_module_steps_sampled_lifetime&#x27;: {&#x27;prey&#x27;: 5437519, &#x27;predator&#x27;: 372736}, &#x27;num_episodes&#x27;: 3, &#x27;episode_len_mean&#x27;: 700.0}</td><td>{&#x27;num_healthy_workers&#x27;: 1, &#x27;num_in_flight_async_reqs&#x27;: 0, &#x27;num_remote_worker_restarts&#x27;: 0}</td><td>MacBook-Pro-de-Tanguy.local</td><td style=\"text-align: right;\">                        13</td><td>{&#x27;predator&#x27;: {&#x27;entropy&#x27;: 2.8857398629188538, &#x27;curr_kl_coeff&#x27;: 1.0125000476837158, &#x27;gradients_default_optimizer_global_norm&#x27;: 2.0316648483276367, &#x27;policy_loss&#x27;: -0.0591624160297215, &#x27;vf_explained_var&#x27;: 0.045291975140571594, &#x27;default_optimizer_learning_rate&#x27;: 5e-05, &#x27;num_non_trainable_parameters&#x27;: np.float64(0.0), &#x27;vf_loss&#x27;: 0.09536299761384726, &#x27;total_loss&#x27;: 0.05085933208465576, &#x27;module_train_batch_size_mean&#x27;: np.float64(1027.0128524198406), &#x27;num_module_steps_trained&#x27;: 4110, &#x27;vf_loss_unclipped&#x27;: 0.22920488473027945, &#x27;mean_kl_loss&#x27;: 0.01447778451256454, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 12.0, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;num_trainable_parameters&#x27;: np.float64(157445.0)}, &#x27;prey&#x27;: {&#x27;default_optimizer_learning_rate&#x27;: 5e-05, &#x27;num_non_trainable_parameters&#x27;: np.float64(0.0), &#x27;vf_loss&#x27;: 0.007650006533367559, &#x27;total_loss&#x27;: 0.04594599606934935, &#x27;module_train_batch_size_mean&#x27;: np.float64(15368.419181790803), &#x27;num_module_steps_trained&#x27;: 59841, &#x27;vf_loss_unclipped&#x27;: 0.007650006533367559, &#x27;mean_kl_loss&#x27;: 0.014044933952391148, &#x27;entropy&#x27;: 2.820167601108551, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 12.0, &#x27;num_trainable_parameters&#x27;: np.float64(157445.0), &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;vf_explained_var&#x27;: -0.4392000436782837, &#x27;curr_kl_coeff&#x27;: 0.15000000223517418, &#x27;gradients_default_optimizer_global_norm&#x27;: 0.7767826318740845, &#x27;policy_loss&#x27;: 0.03620670617965516}, &#x27;__all_modules__&#x27;: {&#x27;num_trainable_parameters&#x27;: np.float64(314890.0), &#x27;num_env_steps_trained&#x27;: 2048, &#x27;learner_connector_timer&#x27;: np.float64(0.48037207465868903), &#x27;num_non_trainable_parameters&#x27;: np.float64(0.0), &#x27;num_module_steps_trained&#x27;: 63951}}</td><td>127.0.0.1</td><td>{&#x27;0&#x27;: 26188, &#x27;1&#x27;: 25363, &#x27;10&#x27;: 26240, &#x27;11&#x27;: 26029, &#x27;12&#x27;: 26372, &#x27;13&#x27;: 26624, &#x27;14&#x27;: 25611, &#x27;15&#x27;: 25839, &#x27;16&#x27;: 26430, &#x27;17&#x27;: 26052, &#x27;18&#x27;: 25539, &#x27;19&#x27;: 25431, &#x27;2&#x27;: 26624, &#x27;20&#x27;: 25050, &#x27;21&#x27;: 25015, &#x27;22&#x27;: 25136, &#x27;23&#x27;: 25795, &#x27;24&#x27;: 25600, &#x27;25&#x27;: 24665, &#x27;26&#x27;: 26624, &#x27;27&#x27;: 25886, &#x27;28&#x27;: 26606, &#x27;29&#x27;: 25536, &#x27;3&#x27;: 24847, &#x27;30&#x27;: 26624, &#x27;31&#x27;: 26624, &#x27;4&#x27;: 23955, &#x27;5&#x27;: 26520, &#x27;6&#x27;: 25971, &#x27;7&#x27;: 25273, &#x27;8&#x27;: 25500, &#x27;9&#x27;: 26624}</td><td style=\"text-align: right;\">                           26624</td><td style=\"text-align: right;\">                           26624</td><td style=\"text-align: right;\">                     38</td><td>{&#x27;cpu_util_percent&#x27;: np.float64(27.149999999999995), &#x27;ram_util_percent&#x27;: np.float64(37.357692307692304)}</td><td style=\"text-align: right;\">34265</td><td style=\"text-align: right;\">             239.373</td><td style=\"text-align: right;\">            18.291</td><td style=\"text-align: right;\">       239.373</td><td>{&#x27;env_runner_sampling_timer&#x27;: 11.129676189961314, &#x27;learner_update_timer&#x27;: 7.921476422495362, &#x27;synch_weights&#x27;: 0.003975342283382807, &#x27;synch_env_connectors&#x27;: 0.0033017806574292914, &#x27;training_iteration_time_sec&#x27;: 18.33869779109955, &#x27;restore_workers_time_sec&#x27;: 5.817413330078125e-06, &#x27;training_step_time_sec&#x27;: 18.33798348903656}</td><td style=\"text-align: right;\"> 1733087203</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">91321_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=34265)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/ray_results/PPO_2024-12-01_22-02-25/PPO_Particle2dEnvironment_91321_00000_0_2024-12-01_22-02-25/checkpoint_000000)\n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m wandb: Adding directory to artifact (/Users/tanguy/Code/Finebouche/collective_behavior/ray_results/PPO_2024-12-01_22-02-25/PPO_Particle2dEnvironment_91321_00000_0_2024-12-01_22-02-25/checkpoint_000000)... \n",
      "\u001b[36m(_WandbLoggingActor pid=34296)\u001b[0m Done. 0.2s\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "from ray.tune import Tuner\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "import os\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=num_cpus, \n",
    "    num_gpus=num_gpus\n",
    ")\n",
    "\n",
    "# Stop criterium\n",
    "stop = {  \n",
    "    \"training_iteration\": 1500,\n",
    "    #\"timesteps_total\": 200000000,\n",
    "}\n",
    "\n",
    "# Read the API key from the file to use Wanddb\n",
    "with open('wandb_api_key.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "callbacks = [\n",
    "    WandbLoggerCallback(                   \n",
    "        project=\"marl-rllib\", \n",
    "        group=ALGO,\n",
    "        api_key=api_key,\n",
    "        log_config=True,\n",
    "        upload_checkpoints=True\n",
    "    ), \n",
    "]\n",
    "\n",
    "# When to save the models \n",
    "checkpoint_config = train.CheckpointConfig(         \n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_frequency=10,\n",
    ")\n",
    "\n",
    "# Where to save \n",
    "# absolute path + ray_results directory\n",
    "storage_path=os.getcwd() + \"/ray_results\"\n",
    "\n",
    "if path_to_checkpoint is None : \n",
    "    tuner = tune.Tuner(\n",
    "        ALGO,                                                 # Defined before\n",
    "        param_space=config,                                   # Defined before\n",
    "        run_config=train.RunConfig(    \n",
    "            storage_path=storage_path,\n",
    "            stop=stop,\n",
    "            verbose=3,\n",
    "            callbacks=callbacks,\n",
    "            checkpoint_config=checkpoint_config,\n",
    "        ),\n",
    "    )\n",
    "    # Run the experiment \n",
    "    results = tuner.fit()\n",
    "\n",
    "# If we instantiate previously trained neural network\n",
    "else: \n",
    "    callbacks.append(RestoreWeightsCallback)\n",
    "\n",
    "    results = tune.run(\n",
    "        ALGO,\n",
    "        config=config.to_dict(),\n",
    "        storage_path=storage_path,\n",
    "        stop=stop,\n",
    "        verbose=3,\n",
    "        callbacks=callbacks,\n",
    "        checkpoint_config=checkpoint_config,\n",
    "    )\n",
    "\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43c80fe3-9e29-46f5-95b4-38e1f582eb27",
   "metadata": {},
   "source": [
    "from ray import tune\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=num_cpus, \n",
    "    num_gpus=num_gpus\n",
    ")\n",
    "\n",
    "experiment_dir = os.getcwd() + \"/ray_results\" + \"PPO_2023-12-10_17-58-05/\"\n",
    "# Restore the training\n",
    "tuner = tune.Tuner.restore(\n",
    "    experiment_dir, \n",
    "    trainable=tune.with_resources(\n",
    "        tune.with_parameters(self.model),\n",
    "        resources={\"cpu\": self.cpuFrac, \"gpu\": self.gpuFrac}\n",
    "    ),\n",
    "    resume_unfinished=True, \n",
    "    resume_errored=True,\n",
    "    restart_errored=False,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d90a6-5207-4d0c-86bb-c1bef3ae8ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39109cdb-4e78-4dee-b0bc-fb0d61b98b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_behavior",
   "language": "python",
   "name": "collective_behavior"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
