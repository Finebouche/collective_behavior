{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b7cc4-c1da-49fb-85a5-29eee064dd0d",
   "metadata": {},
   "source": [
    "## Some preliminary checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96221f2954eecc57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T12:31:29.776081Z",
     "iopub.status.busy": "2025-02-01T12:31:29.775800Z",
     "iopub.status.idle": "2025-02-01T12:31:31.593298Z",
     "shell.execute_reply": "2025-02-01T12:31:31.593075Z",
     "shell.execute_reply.started": "2025-02-01T12:31:29.776060Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray version : 2.41.0\n",
      "PyTorch Version: 2.5.1\n",
      "CUDA Available: False\n",
      "MPS Available: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'tvm']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import ray \n",
    "import os \n",
    "\n",
    "# Can be modified\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "print(\"Ray version :\", ray.__version__)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "\n",
    "torch._dynamo.list_backends()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e711d-75a9-4f97-ac7f-092552034613",
   "metadata": {},
   "source": [
    "## Number of CPUs and GPUs available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9367a277-f21e-4ccb-9fc1-c5c1c1dd292b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T12:31:31.593774Z",
     "iopub.status.busy": "2025-02-01T12:31:31.593657Z",
     "iopub.status.idle": "2025-02-01T12:31:31.595445Z",
     "shell.execute_reply": "2025-02-01T12:31:31.595214Z",
     "shell.execute_reply.started": "2025-02-01T12:31:31.593766Z"
    },
    "jupyter": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs:  12\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "print(\"Number of CPUs: \", psutil.cpu_count())\n",
    "\n",
    "num_cpus = 12\n",
    "num_gpus = 0\n",
    "\n",
    "assert num_cpus <= psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421e6f2-e64c-4f7c-8ef2-cd969f9888fe",
   "metadata": {},
   "source": [
    "## Algorithm configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4828732386e52c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T12:31:31.595712Z",
     "iopub.status.busy": "2025-02-01T12:31:31.595641Z",
     "iopub.status.idle": "2025-02-01T12:31:37.745057Z",
     "shell.execute_reply": "2025-02-01T12:31:37.744757Z",
     "shell.execute_reply.started": "2025-02-01T12:31:31.595705Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001B[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001B[0m\n",
      "  gym.logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001B[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001B[0m\n",
      "  gym.logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001B[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001B[0m\n",
      "  logger.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001B[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001B[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.registry import get_trainable_cls\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "from particle_2d_env import Particle2dEnvironment, MyCallbacks\n",
    "from config import env_config\n",
    "\n",
    "ALGO = \"PPO\"        \n",
    "FRAMEWORK= \"torch\"\n",
    "env = Particle2dEnvironment(env_config)\n",
    "\n",
    "ppo_config = (\n",
    "    get_trainable_cls(ALGO).get_default_config()\n",
    "    .environment(Particle2dEnvironment, env_config=env_config)\n",
    "    .framework(FRAMEWORK,)\n",
    "    .api_stack(enable_rl_module_and_learner=True, enable_env_runner_and_connector_v2=True,)\n",
    "    .callbacks(MyCallbacks)\n",
    "    .rl_module(\n",
    "        model_config={\n",
    "            \"fcnet_hiddens\" : [128, 128, 128], \n",
    "            \"use_attention\" : True,\n",
    "        }\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.particule_agents[id].entity_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"],\n",
    "        count_steps_by=\"agent_steps\",\n",
    "    )\n",
    "    .training(\n",
    "        num_epochs=10,\n",
    "        train_batch_size_per_learner=512, \n",
    "    )\n",
    "    .learners(\n",
    "        num_learners=1,          # or >2\n",
    "        num_cpus_per_learner=3,  # <- default 1\n",
    "        num_gpus_per_learner=0,  # <- default 0\n",
    "    )\n",
    "    .resources(\n",
    "        num_cpus_for_main_process=1  # <- default  1\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=1, \n",
    "        num_envs_per_env_runner=1,\n",
    "        num_cpus_per_env_runner=2,\n",
    "        rollout_fragment_length=\"auto\",\n",
    "        batch_mode= 'complete_episodes', # truncate_episodes or complete_episodes\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4774978-6096-4090-b065-3caf0a0a8ba4",
   "metadata": {},
   "source": [
    "## Wandb Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be67bb9-cb92-4cfd-aea8-005c5a47bf14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T12:31:37.747432Z",
     "iopub.status.busy": "2025-02-01T12:31:37.747326Z",
     "iopub.status.idle": "2025-02-01T12:31:37.749701Z",
     "shell.execute_reply": "2025-02-01T12:31:37.749477Z",
     "shell.execute_reply.started": "2025-02-01T12:31:37.747424Z"
    }
   },
   "outputs": [],
   "source": [
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "\n",
    "# Read the API key from the file to use Wanddb\n",
    "with open('../project/wandb_api_key.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "tune_callbacks = [\n",
    "    WandbLoggerCallback(                   \n",
    "        project=\"marl-rllib\", \n",
    "        group=None,\n",
    "        api_key=api_key,\n",
    "        log_config=True,\n",
    "        upload_checkpoints=False\n",
    "    ), \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd95c1-c2d5-439c-9478-cf1f15c10244",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b472ec59b1f433e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T12:31:37.750022Z",
     "iopub.status.busy": "2025-02-01T12:31:37.749951Z",
     "iopub.status.idle": "2025-02-02T09:32:19.327238Z",
     "shell.execute_reply": "2025-02-02T09:32:19.326685Z",
     "shell.execute_reply.started": "2025-02-01T12:31:37.750015Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 13:31:38,723\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "2025-02-01 13:31:39,209\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-02-02 10:32:18</td></tr>\n",
       "<tr><td>Running for: </td><td>21:00:39.51        </td></tr>\n",
       "<tr><td>Memory:      </td><td>26.9/64.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 6.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>status    </th><th>loc           </th><th>env_config/prey_cons\n",
       "umed      </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  num_training_step_ca\n",
       "lls_per_iteration</th><th style=\"text-align: right;\">       num_env_steps_sample\n",
       "d_lifetime</th><th style=\"text-align: right;\">        ...env_steps_sampled\n",
       "_lifetime_throughput</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Particle2dEnvironment_7c04b_00001</td><td>PENDING   </td><td>              </td><td>False</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>PPO_Particle2dEnvironment_7c04b_00000</td><td>TERMINATED</td><td>127.0.0.1:9602</td><td>True </td><td style=\"text-align: right;\">  1500</td><td style=\"text-align: right;\">         4573.27</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">508336</td><td style=\"text-align: right;\">139.559</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 13:31:39,228\tWARNING algorithm_config.py:4702 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-02-01 13:31:39,230\tWARNING algorithm_config.py:4702 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-02-01 13:31:39,231\tWARNING algorithm_config.py:4702 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-02-01 13:31:39,232\tWARNING algorithm_config.py:4702 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001B[36m(PPO pid=9602)\u001B[0m 2025-02-01 13:31:43,486\tWARNING algorithm_config.py:4702 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9613)\u001B[0m 2025-02-01 13:31:47,850\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9613)\u001B[0m 2025-02-01 13:31:47,850\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001B[36m(PPO pid=9602)\u001B[0m Install gputil for GPU system monitoring.\n",
      "\u001B[36m(PPO pid=9603)\u001B[0m 2025-02-01 13:31:47,986\tWARNING algorithm_config.py:4702 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\u001B[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001B[0m\n",
      "\u001B[36m(_WrappedExecutable pid=9625)\u001B[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m Error importing optional module moviepy.editor\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 215, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     return sys.modules[name]\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m            ~~~~~~~~~~~^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m KeyError: 'moviepy.editor'\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m During handling of the above exception, another exception occurred:\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 244, in get_module\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     return import_module_lazy(name)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 219, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     raise ModuleNotFoundError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m ModuleNotFoundError\n",
      "\u001B[36m(_WrappedExecutable pid=9625)\u001B[0m 2025-02-01 13:31:52,739\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\u001B[32m [repeated 4x across cluster]\u001B[0m\n",
      "\u001B[36m(_WrappedExecutable pid=9625)\u001B[0m 2025-02-01 13:31:52,739\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001B[32m [repeated 4x across cluster]\u001B[0m\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Currently logged in as: tanguy-cazalets (tcazalet_airo). Use `wandb login --relogin` to force relogin\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Tracking run with wandb version 0.19.4\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Run data is saved locally in /private/tmp/ray/session_2025-02-01_13-31-37_761429_9571/artifacts/2025-02-01_13-31-39/PPO_2025-02-01_13-31-39/driver_artifacts/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/wandb/run-20250201_133157-7c04b_00000\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Syncing run PPO_Particle2dEnvironment_7c04b_00000\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: ⭐️ View project at https://wandb.ai/tcazalet_airo/marl-rllib\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: 🚀 View run at https://wandb.ai/tcazalet_airo/marl-rllib/runs/7c04b_00000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                           </th><th>date               </th><th>done  </th><th>env_runner_group                               </th><th>env_runners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </th><th>fault_tolerance                                            </th><th>hostname                       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>learners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               </th><th>node_ip  </th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime_throughput</th><th style=\"text-align: right;\">  num_training_step_calls_per_iteration</th><th>perf                                                                                                   </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                                                                                                                                     </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Particle2dEnvironment_7c04b_00000</td><td>2025-02-01_14-49-51</td><td>True  </td><td>{&#x27;actor_manager_num_outstanding_async_reqs&#x27;: 0}</td><td>{&#x27;num_agent_steps_sampled&#x27;: {&#x27;16&#x27;: 22, &#x27;23&#x27;: 92, &#x27;20&#x27;: 701, &#x27;5&#x27;: 45, &#x27;21&#x27;: 123, &#x27;33&#x27;: 701, &#x27;30&#x27;: 701, &#x27;24&#x27;: 393, &#x27;14&#x27;: 262, &#x27;0&#x27;: 48, &#x27;2&#x27;: 33, &#x27;22&#x27;: 220, &#x27;12&#x27;: 0, &#x27;3&#x27;: 118, &#x27;8&#x27;: 158, &#x27;13&#x27;: 39, &#x27;1&#x27;: 132, &#x27;15&#x27;: 42, &#x27;6&#x27;: 127, &#x27;10&#x27;: 73, &#x27;17&#x27;: 105, &#x27;29&#x27;: 208, &#x27;9&#x27;: 33, &#x27;28&#x27;: 34, &#x27;26&#x27;: 250, &#x27;32&#x27;: 701, &#x27;31&#x27;: 701, &#x27;7&#x27;: 56, &#x27;25&#x27;: 506, &#x27;4&#x27;: 415, &#x27;19&#x27;: 14, &#x27;11&#x27;: 184, &#x27;18&#x27;: 302, &#x27;27&#x27;: 83}, &#x27;num_agent_steps_sampled_lifetime&#x27;: {&#x27;8&#x27;: 162398, &#x27;13&#x27;: 157473, &#x27;1&#x27;: 159983, &#x27;15&#x27;: 162997, &#x27;6&#x27;: 160842, &#x27;31&#x27;: 508620, &#x27;10&#x27;: 162014, &#x27;3&#x27;: 158719, &#x27;17&#x27;: 159416, &#x27;29&#x27;: 158343, &#x27;9&#x27;: 164284, &#x27;28&#x27;: 156992, &#x27;26&#x27;: 159559, &#x27;32&#x27;: 508620, &#x27;7&#x27;: 155292, &#x27;25&#x27;: 157906, &#x27;19&#x27;: 159588, &#x27;4&#x27;: 162540, &#x27;16&#x27;: 165869, &#x27;23&#x27;: 157918, &#x27;20&#x27;: 163776, &#x27;5&#x27;: 164158, &#x27;21&#x27;: 161100, &#x27;33&#x27;: 508620, &#x27;30&#x27;: 508620, &#x27;14&#x27;: 159171, &#x27;24&#x27;: 163958, &#x27;0&#x27;: 161484, &#x27;2&#x27;: 161535, &#x27;22&#x27;: 154471, &#x27;12&#x27;: 156066, &#x27;18&#x27;: 166504, &#x27;11&#x27;: 155368, &#x27;27&#x27;: 159705}, &#x27;agent_episode_returns_mean&#x27;: {&#x27;23&#x27;: -1.26359, &#x27;20&#x27;: -1.2841299999999998, &#x27;5&#x27;: -1.1454600000000001, &#x27;33&#x27;: 9.23, &#x27;24&#x27;: -1.1535699999999998, &#x27;13&#x27;: -1.21499, &#x27;14&#x27;: -1.20283, &#x27;1&#x27;: -1.27369, &#x27;11&#x27;: -1.3235699999999997, &#x27;12&#x27;: -1.27475, &#x27;3&#x27;: -1.17594, &#x27;8&#x27;: -1.1857, &#x27;18&#x27;: -1.18561, &#x27;27&#x27;: -1.28338, &#x27;10&#x27;: -1.15618, &#x27;15&#x27;: -1.18568, &#x27;6&#x27;: -1.2763200000000001, &#x27;17&#x27;: -1.1368899999999997, &#x27;29&#x27;: -1.24432, &#x27;31&#x27;: 9.39, &#x27;7&#x27;: -1.25256, &#x27;4&#x27;: -1.2531500000000002, &#x27;16&#x27;: -1.23377, &#x27;9&#x27;: -1.2532100000000002, &#x27;28&#x27;: -1.2266599999999999, &#x27;21&#x27;: -1.24573, &#x27;26&#x27;: -1.20716, &#x27;32&#x27;: 8.91, &#x27;30&#x27;: 8.83, &#x27;0&#x27;: -1.22372, &#x27;22&#x27;: -1.2155199999999997, &#x27;2&#x27;: -1.2934799999999997, &#x27;25&#x27;: -1.17421, &#x27;19&#x27;: -1.25403}, &#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;UnBatchToIndividualItems&#x27;: 3.833129678411242e-05, &#x27;AgentToModuleMapping&#x27;: 4.520600436544991e-06, &#x27;BatchIndividualItems&#x27;: 1.837650708361411e-05, &#x27;GetActions&#x27;: 9.121889368500947e-05, &#x27;NumpyToTensor&#x27;: 1.7328826125543432e-05, &#x27;ModuleToAgentUnmapping&#x27;: 2.5481854407737e-06, &#x27;AddObservationsFromEpisodesToBatch&#x27;: 5.286020093557301e-05, &#x27;AddStatesFromEpisodesToBatch&#x27;: 5.102603283959297e-06, &#x27;ListifyDataForVectorEnv&#x27;: 5.966468561960993e-06, &#x27;RemoveSingleTsTimeRankFromBatch&#x27;: 6.847419789095492e-07, &#x27;NormalizeAndClipActions&#x27;: 0.00014668391338332705, &#x27;TensorToNumpy&#x27;: 2.5466847587532545e-05}}, &#x27;agent_steps&#x27;: {&#x27;29&#x27;: 122.56, &#x27;31&#x27;: 383.43, &#x27;7&#x27;: 97.21, &#x27;4&#x27;: 111.05, &#x27;27&#x27;: 96.35, &#x27;16&#x27;: 88.78, &#x27;9&#x27;: 94.19, &#x27;28&#x27;: 107.55, &#x27;26&#x27;: 107.5, &#x27;32&#x27;: 383.43, &#x27;0&#x27;: 124.34, &#x27;22&#x27;: 108.01, &#x27;25&#x27;: 103.84, &#x27;19&#x27;: 97.29, &#x27;23&#x27;: 111.36, &#x27;20&#x27;: 113.6, &#x27;5&#x27;: 122.3, &#x27;21&#x27;: 98.74, &#x27;33&#x27;: 383.43, &#x27;30&#x27;: 383.43, &#x27;24&#x27;: 117.37, &#x27;13&#x27;: 115.01, &#x27;14&#x27;: 107.87, &#x27;2&#x27;: 88.93, &#x27;11&#x27;: 103.92, &#x27;12&#x27;: 98.51, &#x27;3&#x27;: 105.53, &#x27;8&#x27;: 113.0, &#x27;18&#x27;: 112.69, &#x27;1&#x27;: 106.01, &#x27;10&#x27;: 122.86, &#x27;15&#x27;: 99.65, &#x27;6&#x27;: 101.98, &#x27;17&#x27;: 114.66}, &#x27;episode_len_mean&#x27;: 383.43, &#x27;mean_dos&#x27;: np.float64(0.26486855840129225), &#x27;num_episodes_lifetime&#x27;: 1485, &#x27;episode_len_max&#x27;: 701, &#x27;max_dos&#x27;: 0.8363477934304366, &#x27;num_env_steps_sampled&#x27;: 701, &#x27;max_doa&#x27;: 0.24683915988549995, &#x27;env_to_module_sum_episodes_length_out&#x27;: 327.15858221091224, &#x27;episode_duration_sec_mean&#x27;: 0.942505417490056, &#x27;episode_return_max&#x27;: -0.22699999999999676, &#x27;mean_doa&#x27;: np.float64(0.0910370534873915), &#x27;episode_return_mean&#x27;: -0.4398000000000007, &#x27;env_to_module_sum_episodes_length_in&#x27;: 327.15858221091224, &#x27;num_module_steps_sampled_lifetime&#x27;: {&#x27;prey&#x27;: 4809429, &#x27;predator&#x27;: 2034480}, &#x27;episode_len_min&#x27;: 85, &#x27;episode_videos_best&#x27;: [], &#x27;num_module_steps_sampled&#x27;: {&#x27;prey&#x27;: 4818, &#x27;predator&#x27;: 2804}, &#x27;num_episodes&#x27;: 1, &#x27;module_episode_returns_mean&#x27;: {&#x27;prey&#x27;: -1.24432, &#x27;predator&#x27;: 9.23}, &#x27;num_env_steps_sampled_lifetime&#x27;: 508336, &#x27;episode_return_min&#x27;: -0.6019999999999968, &#x27;time_between_sampling&#x27;: 1.443436662608461, &#x27;num_env_steps_sampled_lifetime_throughput&#x27;: 139.5585100141166}</td><td>{&#x27;num_healthy_workers&#x27;: 1, &#x27;num_remote_worker_restarts&#x27;: 5}</td><td>MacBook-Pro-de-Tanguy.fritz.box</td><td style=\"text-align: right;\">                      1500</td><td>{&#x27;__all_modules__&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;AddColumnsFromEpisodesToTrainBatch&#x27;: 0.03617516712056098, &#x27;AddOneTsToEpisodesAndTruncate&#x27;: 0.0030597222971253737, &#x27;AddStatesFromEpisodesToBatch&#x27;: 1.3661796671831872e-05, &#x27;GeneralAdvantageEstimation&#x27;: 0.0068145446802137565, &#x27;AddObservationsFromEpisodesToBatch&#x27;: 0.00015022291679985772, &#x27;AgentToModuleMapping&#x27;: 0.0022745045488960546, &#x27;NumpyToTensor&#x27;: 0.0002468781685097811, &#x27;BatchIndividualItems&#x27;: 0.039348483496475795}}, &#x27;num_env_steps_trained&#x27;: 701, &#x27;learner_connector_sum_episodes_length_out&#x27;: 326.32219255761856, &#x27;num_module_steps_trained_lifetime&#x27;: 6929328, &#x27;num_non_trainable_parameters&#x27;: 0.0, &#x27;learner_connector_sum_episodes_length_in&#x27;: 326.32219255761856, &#x27;num_env_steps_trained_lifetime&#x27;: 508620, &#x27;num_module_steps_trained&#x27;: 7685, &#x27;num_trainable_parameters&#x27;: 159498.0, &#x27;num_env_steps_trained_lifetime_throughput&#x27;: 139.5582599308655}, &#x27;prey&#x27;: {&#x27;curr_kl_coeff&#x27;: 1.1546030044555664, &#x27;policy_loss&#x27;: 0.04992106929421425, &#x27;vf_loss_unclipped&#x27;: 0.2210424691438675, &#x27;num_module_steps_trained_lifetime&#x27;: 4888908, &#x27;module_train_batch_size_mean&#x27;: 3069.6878863544484, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.0, &#x27;default_optimizer_learning_rate&#x27;: 5e-05, &#x27;num_non_trainable_parameters&#x27;: 0.0, &#x27;gradients_default_optimizer_global_norm&#x27;: 4.412717819213867, &#x27;total_loss&#x27;: 0.28007662296295166, &#x27;mean_kl_loss&#x27;: 0.007892820052802563, &#x27;entropy&#x27;: 16.667478561401367, &#x27;vf_explained_var&#x27;: 0.30255717039108276, &#x27;vf_loss&#x27;: 0.2210424691438675, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;num_module_steps_trained&#x27;: 4877, &#x27;num_trainable_parameters&#x27;: 79749.0, &#x27;weights_seq_no&#x27;: 1485.0}, &#x27;predator&#x27;: {&#x27;vf_loss_unclipped&#x27;: 0.5194647312164307, &#x27;num_module_steps_trained_lifetime&#x27;: 2040420, &#x27;module_train_batch_size_mean&#x27;: 1309.288770230473, &#x27;num_non_trainable_parameters&#x27;: 0.0, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.0, &#x27;gradients_default_optimizer_global_norm&#x27;: 72.67535400390625, &#x27;total_loss&#x27;: 0.5664339661598206, &#x27;default_optimizer_learning_rate&#x27;: 5e-05, &#x27;mean_kl_loss&#x27;: 0.01416424848139286, &#x27;entropy&#x27;: -5.4502105712890625, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;num_module_steps_trained&#x27;: 2808, &#x27;vf_explained_var&#x27;: 0.7355270385742188, &#x27;vf_loss&#x27;: 0.5194647312164307, &#x27;num_trainable_parameters&#x27;: 79749.0, &#x27;weights_seq_no&#x27;: 1485.0, &#x27;curr_kl_coeff&#x27;: 3.97477388381958, &#x27;policy_loss&#x27;: -0.009330473840236664}}</td><td>127.0.0.1</td><td style=\"text-align: right;\">                          508336</td><td style=\"text-align: right;\">                                    139.559</td><td style=\"text-align: right;\">                                      1</td><td>{&#x27;cpu_util_percent&#x27;: np.float64(23.84285714285715), &#x27;ram_util_percent&#x27;: np.float64(47.471428571428575)}</td><td style=\"text-align: right;\"> 9602</td><td style=\"text-align: right;\">             4573.27</td><td style=\"text-align: right;\">           3.69886</td><td style=\"text-align: right;\">       4573.27</td><td>{&#x27;training_iteration&#x27;: 2.9793158648683744, &#x27;restore_workers&#x27;: 0.015002854687042579, &#x27;training_step&#x27;: 2.9634760071230857, &#x27;env_runner_sampling_timer&#x27;: 1.8965182139333392, &#x27;learner_update_timer&#x27;: 1.0727236388052572, &#x27;synch_weights&#x27;: 0.0035421026736569042, &#x27;synch_env_connectors&#x27;: 0.010433492102718683}</td><td style=\"text-align: right;\"> 1738417791</td><td style=\"text-align: right;\">                1500</td><td>7c04b_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 13:32:05,061\tWARNING tensorboardx.py:265 -- You are trying to log an invalid value (ray/tune/env_runners/episode_videos_best=[<wandb.sdk.data_types.video.Video object at 0x3e4697b10>]) via TBXLoggerCallback!\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000000)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m 2025-02-01 13:38:10,367\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: Agent 24 acted and then got truncated, but did NOT receive a last (truncation) observation, required for e.g. value function bootstrapping!\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     return func(self, *args, **kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     return method(self, *_args, **_kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 202, in sample\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     samples = self._sample_episodes(\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     return method(self, *_args, **_kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 579, in _sample_episodes\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     _episode.add_env_step(\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/env/multi_agent_episode.py\", line 544, in add_env_step\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m     raise MultiAgentEnvError(\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9612)\u001B[0m ray.rllib.utils.error.MultiAgentEnvError: Agent 24 acted and then got truncated, but did NOT receive a last (truncation) observation, required for e.g. value function bootstrapping!\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:39:09,076\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m(raylet)\u001B[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff24d3dc8ef8d4f6a7cfacb44401000000 Worker ID: 1519c1157d56a9d86884dc5de82064a6f919b895c29cabdb887383d4 Node ID: c2fed0cd03b5a3c90434566716fc84270f2a54145d53f9c7eeeddc76 Worker IP address: 127.0.0.1 Worker port: 50345 Worker PID: 9612 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:39:11,467\tERROR actor_manager.py:815 -- Ray error (The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:39:11,467\tERROR actor_manager.py:646 -- The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m NoneType: None\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:39:13,468\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m 2025-02-01 13:39:14,607\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m 2025-02-01 13:39:14,607\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:39:15,470\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:39:15,542\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m Error importing optional module moviepy.editor\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 215, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     return sys.modules[name]\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m            ~~~~~~~~~~~^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m KeyError: 'moviepy.editor'\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m During handling of the above exception, another exception occurred:\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 244, in get_module\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     return import_module_lazy(name)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 219, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     raise ModuleNotFoundError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m ModuleNotFoundError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m 2025-02-01 13:39:21,501\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: There was an error while reducing the Stats object under key=('num_env_steps_sampled_lifetime',)! Check, whether you logged invalid or incompatible values into this key over time in your custom code.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m The values under this key are: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m The original error was \n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/metrics_logger.py\", line 933, in reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     tree.map_structure_with_path(_reduce, stats_to_return)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/tree/__init__.py\", line 474, in map_structure_with_path\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     return map_structure_with_path_up_to(structures[0], func, *structures,\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/tree/__init__.py\", line 778, in map_structure_with_path_up_to\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     results.append(func(*path_and_values))\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m                    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/metrics_logger.py\", line 916, in _reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     return stats.reduce()\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m            ^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/stats.py\", line 321, in reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     assert delta_sum >= 0\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m            ^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m AssertionError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m During handling of the above exception, another exception occurred:\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     return func(self, *args, **kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m                                                        ^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     return method(self, *_args, **_kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 726, in get_metrics\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     return self.metrics.reduce()\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/metrics_logger.py\", line 941, in reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m     raise ValueError(\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m ValueError: There was an error while reducing the Stats object under key=('num_env_steps_sampled_lifetime',)! Check, whether you logged invalid or incompatible values into this key over time in your custom code.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m The values under this key are: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].\n",
      "\u001B[36m(MultiAgentEnvRunner pid=9984)\u001B[0m The original error was \n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:40:21,114\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m(raylet)\u001B[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff24d3dc8ef8d4f6a7cfacb44401000000 Worker ID: f0ec47967d13070bb638c2d81c60b3ddf22a1542e61353d28fdb9c0f Node ID: c2fed0cd03b5a3c90434566716fc84270f2a54145d53f9c7eeeddc76 Worker IP address: 127.0.0.1 Worker port: 52684 Worker PID: 9984 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:40:22,524\tERROR actor_manager.py:815 -- Ray error (The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:40:22,524\tERROR actor_manager.py:646 -- The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m NoneType: None\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:40:24,526\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m 2025-02-01 13:40:25,864\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m 2025-02-01 13:40:25,864\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:40:26,529\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:40:26,605\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m Error importing optional module moviepy.editor\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 215, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     return sys.modules[name]\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m            ~~~~~~~~~~~^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m KeyError: 'moviepy.editor'\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m During handling of the above exception, another exception occurred:\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 244, in get_module\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     return import_module_lazy(name)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 219, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     raise ModuleNotFoundError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m ModuleNotFoundError\n",
      "\u001B[36m(raylet)\u001B[0m Spilled 2590 MiB, 12 objects, write throughput 2127 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m 2025-02-01 13:40:42,416\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: There was an error while reducing the Stats object under key=('num_env_steps_sampled_lifetime',)! Check, whether you logged invalid or incompatible values into this key over time in your custom code.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m The values under this key are: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m The original error was \n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/metrics_logger.py\", line 933, in reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     tree.map_structure_with_path(_reduce, stats_to_return)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/tree/__init__.py\", line 474, in map_structure_with_path\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     return map_structure_with_path_up_to(structures[0], func, *structures,\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/tree/__init__.py\", line 778, in map_structure_with_path_up_to\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     results.append(func(*path_and_values))\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m                    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/metrics_logger.py\", line 916, in _reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     return stats.reduce()\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m            ^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/stats.py\", line 321, in reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     assert delta_sum >= 0\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m            ^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m AssertionError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m During handling of the above exception, another exception occurred:\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     return func(self, *args, **kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m                                                        ^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     return method(self, *_args, **_kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 726, in get_metrics\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     return self.metrics.reduce()\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/metrics_logger.py\", line 941, in reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m     raise ValueError(\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m ValueError: There was an error while reducing the Stats object under key=('num_env_steps_sampled_lifetime',)! Check, whether you logged invalid or incompatible values into this key over time in your custom code.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m The values under this key are: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10044)\u001B[0m The original error was \n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:41:41,979\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m(raylet)\u001B[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff24d3dc8ef8d4f6a7cfacb44401000000 Worker ID: 377b8b52b4bb361ebe45ddcb5b143aee06be2bd4f3c36ee84760585a Node ID: c2fed0cd03b5a3c90434566716fc84270f2a54145d53f9c7eeeddc76 Worker IP address: 127.0.0.1 Worker port: 52715 Worker PID: 10044 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:41:43,593\tERROR actor_manager.py:815 -- Ray error (The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:41:43,593\tERROR actor_manager.py:646 -- The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m NoneType: None\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:41:45,594\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m 2025-02-01 13:41:46,755\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m 2025-02-01 13:41:46,755\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:41:47,597\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 13:41:47,680\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m Error importing optional module moviepy.editor\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 215, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     return sys.modules[name]\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m            ~~~~~~~~~~~^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m KeyError: 'moviepy.editor'\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m During handling of the above exception, another exception occurred:\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 244, in get_module\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     return import_module_lazy(name)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 219, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     raise ModuleNotFoundError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m ModuleNotFoundError\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000001)\n",
      "\u001B[36m(raylet)\u001B[0m Spilled 4501 MiB, 24 objects, write throughput 2350 MiB/s.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000002)\n",
      "\u001B[36m(raylet)\u001B[0m Spilled 8221 MiB, 38 objects, write throughput 2532 MiB/s.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000003)\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000004)\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000005)\n",
      "\u001B[36m(raylet)\u001B[0m Spilled 16650 MiB, 68 objects, write throughput 2661 MiB/s.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000006)\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000007)\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000008)\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000009)\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000010)\n",
      "\u001B[36m(raylet)\u001B[0m Spilled 33005 MiB, 123 objects, write throughput 2686 MiB/s.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000011)\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000012)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m 2025-02-01 14:40:08,172\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: Agent 15 acted and then got truncated, but did NOT receive a last (truncation) observation, required for e.g. value function bootstrapping!\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     return func(self, *args, **kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     return method(self, *_args, **_kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 202, in sample\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     samples = self._sample_episodes(\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     return method(self, *_args, **_kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 579, in _sample_episodes\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     _episode.add_env_step(\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/env/multi_agent_episode.py\", line 544, in add_env_step\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m     raise MultiAgentEnvError(\n",
      "\u001B[36m(MultiAgentEnvRunner pid=10070)\u001B[0m ray.rllib.utils.error.MultiAgentEnvError: Agent 15 acted and then got truncated, but did NOT receive a last (truncation) observation, required for e.g. value function bootstrapping!\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:41:06,981\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m(raylet)\u001B[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff24d3dc8ef8d4f6a7cfacb44401000000 Worker ID: db89c6211c37cac60791bc61ee4a96efc12ce2039d263f375144d062 Node ID: c2fed0cd03b5a3c90434566716fc84270f2a54145d53f9c7eeeddc76 Worker IP address: 127.0.0.1 Worker port: 52752 Worker PID: 10070 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:41:09,657\tERROR actor_manager.py:815 -- Ray error (The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:41:09,657\tERROR actor_manager.py:646 -- The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m NoneType: None\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:41:11,659\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m 2025-02-01 14:41:12,681\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m 2025-02-01 14:41:12,681\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:41:13,663\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:41:13,738\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m Error importing optional module moviepy.editor\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 215, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     return sys.modules[name]\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m            ~~~~~~~~~~~^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m KeyError: 'moviepy.editor'\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m During handling of the above exception, another exception occurred:\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 244, in get_module\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     return import_module_lazy(name)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 219, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     raise ModuleNotFoundError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m ModuleNotFoundError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m 2025-02-01 14:41:19,550\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: There was an error while reducing the Stats object under key=('num_env_steps_sampled_lifetime',)! Check, whether you logged invalid or incompatible values into this key over time in your custom code.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m The values under this key are: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m The original error was \n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/metrics_logger.py\", line 933, in reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     tree.map_structure_with_path(_reduce, stats_to_return)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/tree/__init__.py\", line 474, in map_structure_with_path\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     return map_structure_with_path_up_to(structures[0], func, *structures,\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/tree/__init__.py\", line 778, in map_structure_with_path_up_to\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     results.append(func(*path_and_values))\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m                    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/metrics_logger.py\", line 916, in _reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     return stats.reduce()\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m            ^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/stats.py\", line 321, in reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     assert delta_sum >= 0\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m            ^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m AssertionError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m During handling of the above exception, another exception occurred:\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     return func(self, *args, **kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m                                                        ^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     return method(self, *_args, **_kwargs)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 726, in get_metrics\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     return self.metrics.reduce()\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/metrics/metrics_logger.py\", line 941, in reduce\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m     raise ValueError(\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m ValueError: There was an error while reducing the Stats object under key=('num_env_steps_sampled_lifetime',)! Check, whether you logged invalid or incompatible values into this key over time in your custom code.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m The values under this key are: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1].\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12389)\u001B[0m The original error was \n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:42:19,153\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m(raylet)\u001B[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff24d3dc8ef8d4f6a7cfacb44401000000 Worker ID: 29fa4c24429b55684b80186f813947a96fe5e51c45569e6f6248d9cc Node ID: c2fed0cd03b5a3c90434566716fc84270f2a54145d53f9c7eeeddc76 Worker IP address: 127.0.0.1 Worker port: 53510 Worker PID: 12389 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:42:20,772\tERROR actor_manager.py:815 -- Ray error (The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:42:20,772\tERROR actor_manager.py:646 -- The actor 24d3dc8ef8d4f6a7cfacb44401000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m NoneType: None\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:42:22,776\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m 2025-02-01 14:42:23,647\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m 2025-02-01 14:42:23,647\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:42:24,781\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m 2025-02-01 14:42:24,855\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m Error importing optional module moviepy.editor\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 215, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m     return sys.modules[name]\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m            ~~~~~~~~~~~^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m KeyError: 'moviepy.editor'\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m During handling of the above exception, another exception occurred:\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m \n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 244, in get_module\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m     return import_module_lazy(name)\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/wandb/util.py\", line 219, in import_module_lazy\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m     raise ModuleNotFoundError\n",
      "\u001B[36m(MultiAgentEnvRunner pid=12438)\u001B[0m ModuleNotFoundError\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000013)\n",
      "\u001B[36m(PPO(env=<class 'particle_2d_env.Particle2dEnvironment'>; env-runners=1; learners=1; multi-agent=True) pid=9602)\u001B[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39/PPO_Particle2dEnvironment_7c04b_00000_0_prey_consumed=True_2025-02-01_13-31-39/checkpoint_000014)\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                                \n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: \n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Run history:\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/_disable_execution_plan_api ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/_enable_rl_module_api ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                          config/_train_batch_size_per_learner ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/always_attach_evaluation_results ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 config/auto_wrap_old_gym_envs ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                             config/clip_param ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/custom_async_evaluation_function ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    config/delay_between_env_runner_restarts_s ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     config/eager_max_retraces ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/enable_async_evaluation ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      config/enable_connectors ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/entropy_coeff ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/env_config/agent_density ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                           config/env_config/collective_death_penalty_for_prey ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                       config/env_config/collective_eating_reward_for_predator ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/env_config/contact_force_coefficient ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/env_config/contact_margin ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/env_config/death_penalty_for_prey ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/env_config/dragging_force_coefficient ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/env_config/eating_reward_for_predator ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/env_config/edge_hit_penalty ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    config/env_config/energy_cost_penalty_coef ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/env_config/episode_length ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                           config/env_config/food_patch_radius ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/env_config/food_patch_regen_time ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 config/env_config/food_radius ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 config/env_config/food_reward ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/env_config/max_acceleration_predator ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/env_config/max_acceleration_prey ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                          config/env_config/max_number_of_food ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/env_config/max_seeing_angle ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         config/env_config/max_seeing_distance ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/env_config/max_turn ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/env_config/num_food_patch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/env_config/num_other_agents_observed ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/env_config/num_predators ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/env_config/num_preys ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/env_config/predator_radius ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 config/env_config/prey_radius ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/env_config/stage_size ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               config/env_config/starving_penalty_for_predator ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/env_config/step_per_time_increment ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/env_config/surviving_reward_for_prey ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                              config/env_config/wall_contact_force_coefficient ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/env_runner_health_probe_timeout_s ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                           config/env_runner_restore_timeout_s ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                            config/env_task_fn ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/episode_lookback_horizon ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/evaluation_duration ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/evaluation_num_env_runners ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/evaluation_sample_timeout_s ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                  config/gamma ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                               config/kl_coeff ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                              config/kl_target ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                 config/lambda ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/local_gpu_idx ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     config/local_tf_session_args/inter_op_parallelism_threads ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     config/local_tf_session_args/intra_op_parallelism_threads ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                     config/lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/max_num_env_runner_restarts ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                            config/max_requests_in_flight_per_aggregator_actor ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/max_requests_in_flight_per_env_runner ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/max_requests_in_flight_per_learner ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/metrics_episode_collection_timeout_s ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/metrics_num_episodes_for_smoothing ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/min_sample_timesteps_per_iteration ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/min_train_timesteps_per_iteration ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         config/minibatch_size ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/model/_use_default_native_models ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/model/attention_dim ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/model/attention_head_dim ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/model/attention_init_gru_gate_bias ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/model/attention_memory_inference ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        config/model/attention_memory_training ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/model/attention_num_heads ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/model/attention_num_transformer_units ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/model/attention_position_wise_mlp_dim ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/model/attention_use_n_prev_actions ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/model/attention_use_n_prev_rewards ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                              config/model/dim ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/model/log_std_clip_param ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/model/lstm_cell_size ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/model/lstm_use_prev_action_reward ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      config/model/max_seq_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/num_aggregator_actors_per_learner ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                          config/num_consecutive_env_runner_failures_tolerance ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/num_cpus_for_main_process ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/num_cpus_per_env_runner ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/num_cpus_per_learner ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                        config/num_env_runners ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/num_envs_per_env_runner ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                             config/num_epochs ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                               config/num_gpus ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/num_gpus_per_env_runner ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/num_gpus_per_learner ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                           config/num_learners ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/output_max_file_size ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/policy_map_cache ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/policy_map_capacity ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         config/prelearner_module_synch_period ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/remote_env_batch_wait_ms ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/sample_timeout_s ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     config/sgd_minibatch_size ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/shuffle_buffer_size ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                              config/sync_filters_on_rollout_workers_timeout_s ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/synchronize_filters ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/tf_session_args/device_count/CPU ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                           config/tf_session_args/inter_op_parallelism_threads ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                           config/tf_session_args/intra_op_parallelism_threads ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/train_batch_size ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/vf_clip_param ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/vf_loss_coeff ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                        config/vf_share_layers ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                             config/worker_cls ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     env_runner_group/actor_manager_num_outstanding_async_reqs ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/0 █▆▆▆▅▄▄▃▄▃▃▂▄▄▄▄▃▂▂▃▃▄▄▃▅▃▃▃▁▂▃▃▃▅▂▃▃▃▃▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/1 ▃▄▃▄▃▃▂▃▃▃▅▃▃▃▃▂▂▂▂▂▃▃▃▂▁▂▂▃▃▃▂▃▃▄▄█▃▂▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/10 █▇▅▅▄▃▂▁▃▃▅▄▃▄▃▅▅▄▃▃▆▄▂▃▄▆▄▃▃▃▄▅▃▂▂▄▄▅█▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/11 ▃▆▆▅▅▄█▆▅▄▅▄▄▁▃▇█▇▇▆▅▅▅▅▃▃▁▂▃▅▅▆▇▄▅▃▁▆█▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/12 ▄▃▃▆▄▂▃▃▃▃▄▄▄▃▂▂▂▂▂▂▃▂▁▁▁▂▂▂▂▃▃▂▂█▃▃▃▃▃▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/13 █▅▄▄▄▅▅███▁▁▁▃▂▄▅▄▆▆▄▅▄▃▂▁▂▂▃▄▅▆▆▄▆▆▆▆▇▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/14 ▄▃▅▄▅▄▃▃▃▁▁▂▂▃▃▁▁▃▃▂▃▃▃▃▃▄▄▄▃▃▄▃▄▄▃█▅▅▄▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/15 ▆▅▄▂▃▂▃▃▃▄▄▄▆▇▃▁▁▃▆█▆▄▃▂▄▄▄▅▄▃▆▅▅▅▆▄▂▃▃▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/16 █▃▃▄▄▃▆▃▄▃▄▅▄▃▂▃▄▄▅▆▅▆▇▆▂▃▄▄▂▁▁▁▃▂▃▇▅▅▅▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/17 ▃▃▅▃▃▃▄▄▄▃▅▄▄▅▄▁▁▁▂▂▃▃▄▄▅▄▃▃▃▃▄▄▄▄▂█▆▆▅▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/18 ▁▂▆▆█▆▄▅▄▅▄▅▆▆▇▆▅▅▄▃▄▅▆▆▆▇▇▇▇▇▅▅▅▄▅▆▆▆▄▇\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/19 ▃▃█▆▆▄▃▅▅▄▃▂▂▃▄▃▃▃▃▄▆▆▇▇▅▅▄▄▅▅▅▄▄▄▄▅▁▄▆▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/2 ▅▄▄▄▄█▄▄▅▄▄▄▄▃▁▂▄▄▄▆▅▅▆▅▆▄▄▄▃▄▅▄▄▄▄▅▇▇▅▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/20 ▅▅▅▆▁▁▂▃▂▃▆▆▅▅▅▃▅▅▅▄▅▅▅▆▅█▇▄▄▅▆▆▆▅▃▃▄▆▅▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/21 ▁▄▇█▇▆▆▆▇██▇▇▇▇▇▇▇▇██▇▇▇█▇▇▆▆▆▆▆▆▆▆▇▇▇▇▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/22 ▅▅▆▅▄▄▄▆▄▄▄▄▄▃▄▃▃▂▂▂▃▃▃▄▄▄▃▃▂▁▁▂▅▅▃██▇▇▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/23 ▇▄▆▆▂▂▃▃▂▄▄▃▃▂▄▁▁▄▆▃▅▄▄▃▄▃▂▂▄▆▅▆▄▅▅▁▁▁█▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/24 ▆█▇▄▅▄▄▃▃▃▂▁▁▃▄▄▅▅▄▄▂▄▄▄▁▃▅▅▇▅▄▄▄▂▃▄▄▄▄▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/25 █▄▄▄▅▃▂▁▂▂▂▁▂▂▃▃▄▃▂▂▂▂▁▁▂▂▅▃▂▂▂▂▄▄▄▁▁▁▃▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/26 ▁▅█▆▆▃▂▂▃▄▁▂▂▂▁▂▃▂▁▁▁▂▃▂▂▁▂▁▃▄▅▆▆▆▆▆▆█▅▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/27 ▁▆█▇▇▆▇▆▇▆▆▆▇▆▆▆▆▆▇▇▆▆▆▆▆▇▆▆▇▇▆▆▆▅▇▆▆▆▆▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/28 ▆▅▇▆▆▆█▇▄▅▄▅▆▆▇▃▂▃▂▂▄▅▃▂▆▅▅▄▂▃▄▅▁▁▁▅▆█▆▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/29 ▁▅▇▆▇▆▆▅▄▆▄▄▃▄▃▇▇▆▆▅▅▄▄▆▅▆▅▇▇██▄▄▄▅▆▆▆▅▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/3 ▆▆▅▂▃▁▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▄▃▃▃▃▂▂▂▄▄▃▃▄▅▄▃▃█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/30 ▆▁▅▅▅▃▇▃▃▄▅▅▆▅▄▅▄████▅▄▅▅▄▄▃▅▄▅▄▄▄▅▆▆▄▄▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/31 ▅▅▇▇▆▁▂▄▅▇██▇▅▇▆▇▇▆▇▇▇▇▇▇▅▆▅▅▆▆▇▇▇▃▅▅▅▆▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/32 ▁▁▂▃▂█▅▆▆▆▄▄▆▅▅▆▄▄▄▄▄▅▆▅▄▂▂▅▇▇▅▂▂▂▃▄▃▂▁▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/33 ▁▁▅▄▄▁▃▅▇▆▄▅▃▃▂▄▄▄▆▅▅▅▃▄█▇▇▅▅▅▃▇▇▆▆▆▆▄▅▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/4 ▃▃▅▄▆▆▇▄▃▅███▆▆▅▅▃▃▃▃▂▅▆█▇▆▄▄▁▃▃▅▅▆▂▄▂▁▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/5 ▆▇▇▅▄▅▅▅▃▃▅▁▂▃▄▄▄▄▅▅▅▄▄▅▅▃▃▃▂▆▄▃▄▄██▆▆▆▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/6 █▇▇▆▅▄▅▄▂▂▃▃▄▄▄▄▄▂▃▃▂▅▅▅▄▃▃▄▄▆▆▁▄▄▄▄▃▃▂▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/7 ▃▄▅▅▃▁▂▁▂▂▃▄▂▃▄▄▄▄▃▃▂▃▃▃▃▃▄▂▁▃▆▅▅▄█▂▄▅▅▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/8 ▃▄▂▃▂▆▅▅▆▆▄▃▇▇▇▄▃▂▂▃▃▁▄▅██▅▆▇▆▅▆▄▄▄▅▃▃▇▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/9 ▁▄▅▃▄▃▄▅▄▄▄▄▆▆▇▇▇▆▆▆▇▆▆▂▄█▆▆▅▆▇▅▅▅▄█▇█▆▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/0 ▆▆▆▇▇▄▅▆▇█▇▆▆▅▄▄▄▄▄▄▃▃▃▂▁▂▂▃▃▃▄▄▂▁▂▁▁▁▁▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/1 ▄▇█▃▅▅▆▅▅▅▄▃▃▃▃▃▄▄▄▂▁▂▃▃▅▅▃▄▄▄▃▃▃▅▅▆▄▃▁▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/10 ▇▆▆▆▂▃▅▄▆▆▆▇▇▆▅▆▇█▇▇▆▄▅▅▅▃▃▃▃▂▅▅▆▅▄▄▄▁▂▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/11 ▁▃▃▃▄█▇▆▆▇▃▄▄▄▃▅▅▅▄▄▃▄▄▄▄▄▅▅▅▅▄▄▄▃▄▄▁▁▁▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/12 ▇███▆▃▃▄▅▅▃▃▅▆▆▅▂▂▃▃▅▅▃▂▃▂▂▂▂▂▃▃▂▃▂▃▃▃▃▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/13 ▆▆█▇▃▅▅▆▇▆▄▅▅▆▆▆▄▄▄▃▅▅▅▄▄▄▄▄▄▄▆▄▅▄▃▂▄▄▃▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/14 ▆█▅█▆▇▆▅▅▄▂▄▃▃▂▂▂▁▁▁▂▃▃▃▃▄▄▂▂▁▁▁▃▃▂▃▂▃▃▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/15 ▆▅▅▅█▇▄▅▅▇▆▄▅▆▆▄▆▆▅▆▅▄▃▄▄▅▃▃▃▃▃▃▁▁▄▁▃▄▄▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/16 ▄▆▅▅▃▅▆▆▆▆▅▅▅▅▅▅▅▄▄▆█▆▅▄▅▃▂▂▁▁▁▂▂▂▂▂▂▃▃▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/17 ▄▄█▇▃▂▄▄▄▄▃▃▄▄▃▄▃▂▂▁▁▂▂▂▂▂▂▁▁▂▂▂▂▂▂▁▂▁▂▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/18 ▄▇▆▆▇▅▅▇▇█▇▆▆▆▆▇▇█▇▇▆▆▅▅▅▆▇▇▇▇▄▂▄▄▄▄▁▂▁▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/19 █▆▆▅▆▃▃▃▅▄▅▅▄▄▄▄▂▂▁▁▄▄▄▄▄▃▁▁▂▃▄▄▁▂▂▃▃▃▃▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/2 ▆▅▅▅███▅▅▅▄▄▄▄▄▅▅▄▄▄▃▃▅▅▅▁▂▂▃▄▄▄▅▄▁▂▃▂▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/20 ▇▇▃▄▅▃▅▆█▇███▇▇▃▃▂▄▅▃▃▃▃▂▃▂▂▂▃▂▂▃▃▃▃▂▂▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/21 ▄▄█▃▆▆▆▆▇▇▅▆▅█▅▃▃▃▂▂▂▂▂▁▂▂▄▅▄▅▄▄▃▁▃▆▃▂▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/22 ▇██▇▆▄▄▅▅▅▃▂▁▁▂▄▃▁▁▃▃▃▃▂▂▁▄▄▄▄▃▃▃▃▃▃▃▃▃▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/23 █▆▅▂▅▅▅▅▅▅▄▃▃▄▃▄▄▄▃▄▃▁▃▃▃▃▃▂▂▃▃▃▂▅▄▃▂▃▂▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/24 ▇█▁▃▄▇▇▆▃▃▃▆▇▅▄▅▅▅▅▅▅▅▅▅▅▅▄▄▃▂▂▂▂▅▅▂▃▁▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/25 █▇▆▆▂▃▃▃▃▄▃▃▃▃▃▄▄▅▄▃▄▄▆▂▂▂▄▄▄▂▃▃▁▃▂▄▂▄▃▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/26 █▇▃▃▂▃▂▃▂▂▂▂▃▃▃▂▂▂▂▂▂▂▁▁▁▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/27 ▅█▇▇▇▆▂▃▃▃▆▅▆▇██▇▅▄▃▂▃▄▄▃▄▃▄▄▁▂▂▁▁▃▂▁▂▁▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/28 ▅▆▇▇█▅▄▆▄▄▃▄▄▅▆▅▅▆▇▆▆▆▄▅▆▅▅▅▅▅▃▃▂▁▂▄▅▄▃▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/29 █▇██▄▆▆▄▅▅▅▆▇▆▆▇▇▅▄▄▅▅▄▄▂▁▁▂▂▂▃▂▂▂▄▄▅▅▄▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/3 ▇▇▇▇▇▅▅▅▅█▆▆▅▇█▄▁▂▂▃▃▃▃▂▂▂▃▄▄▄▄▃▃▃▁▃▃▃▄▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/30 ▂▃▄▅▆▄▇▇▆▅▆▇▇▇▅▆▇▇█▇▄▄▅▅▆▅▅▅▅█▅▅▃▂▄▇▆▁▄▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/31 ▄▅▆▇▇▃▇▆██▆▇▇▇▇▆▅▆▇█▅▅▅▅▇▇▆▅▆▅▄▆▇▆▅▇▇▂▁▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/32 ▄▄▅▆▅▆▇█▇▆▇▇▆▆▅▇██▅▅▅▅▅▆▆▅▅▄▆▇██▆▄▄▆▆▆▁▇\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/33 ▄▇▃▃▇█▇▇▇█▇▆███▇▇▆▆▆▆▄▅▇▇▇▇▆▆▅▄▅▄▅▆▃▃▃▁█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/4 █▅▄▄▅▅▄▄▂▁▁▂▂▂▂▃▃▃▃▅▆▅▅▄▂▂▂▂▂▂▁▁▂▂▃▃▃▄▁▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/5 ▇▄▄█▇▆▅▄▄▃▃▃▄▅▅▅▅▅▅▅▄▃▂▂▂▁▁▁▃▄▃▂▃▄▃▄▃▃▄▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/6 █▇█▇▂▂▃▅▄▄▄▅▄▃▃▂▆▅▅▆▇▇▆▄▃▁▁▁▂▃▅▃▂▃▄▃▂▂▂▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/7 █▆▂▄▅▅▄▆▆▅▆▅▆▅▄▄▃▃▁▂▂▂▂▃▃▄▄▅▅▅▅▅▅▅▄▂▄▄▆▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/8 ▄▄██▇▇▇▅▄▃▄▄▅▆▆▅▅▃▃▃▂▂▂▄▅▇▄▄▃▃▃▄▄▂▃▃▃▃▁▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/9 █▇▆▆▆▂▅▆▆▆▇███▆▅▅▆▅▆▇▄▃▄▄▃▁▂▃▃▃▃▂▁▁▂▆▆▃▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                              env_runners/env_to_module_sum_episodes_length_in ▁▁▃▃▄▄▄▇█▇▇▆▇▇▇▇▇█▇▇▇██▇▇▇▆▆▇█▆▅▆▆▆▇▇▇▇▇\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                             env_runners/env_to_module_sum_episodes_length_out ▁▁▁▃▄▅▅▅▆▇▇▇▇▇▆▇███▇█▇███▇▇▇██▇▆▆▆▆█▇▇▅▇\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/episode_duration_sec_mean █▅▆▇▇▇▂▄▅▅▆▅▆▅▆▆▅▅▃▃▃▃▄▄▄▃▂▃▄▄▁▄▄▄▄▄▄▁▁▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   env_runners/episode_len_max ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  env_runners/episode_len_mean ▃▃▄▆▇▄▆▆▇██▆▇▆▆█▇▇▅▅▇▆▆▅▅▇▆██▇▄▅▅▆▇▁▃▁▃▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   env_runners/episode_len_min ██▃▃▃▃▃▆▇▇▄▄▄▄▄▅▅▄▄▄▄▁▁▁▄▄▃▃▃▃▃▃▃▃▄▄▄▄▄▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                env_runners/episode_return_max ▂▂▂▂▂▁▂▂▂▄▆▆▆▆▆▇▂▂▂▂▃▄▄▄▄▅▃▃▃▃▁▂▂▂█▃▃▂▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               env_runners/episode_return_mean ▂▃▁▁▁▃▄▄▅▅▆███▇▇▆▆▆▆▆▆▆▅▅▆▅▄▄▄▅▅▅▄▄▃▃▇▄▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                env_runners/episode_return_min ██▁▁▆▅▅▅▅▅▆▆▆▆▇▄▄▄▆▆▆▆▃▃▃▄▅▅▅▅▅▅▅▅▆▆▄▄▆▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                           env_runners/max_doa ▁▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                           env_runners/max_dos ▁▃▃▃▃▃▄▄▄▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆███████████████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          env_runners/mean_doa ▆▆▆▆▆▅▅▅▄▄▃▃▂▂▂▁▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▆██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          env_runners/mean_dos ▃▁▁▁▂▂▂▃▃▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█▇▇██▇▇▇█████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                              env_runners/module_episode_returns_mean/predator ▄▇▁▅▅▅█▆▆▅▃▃▃▃▄▅▃▃▃▅▆▆▅▄▃█▅▅▅▄▄▆▆▆▆▄▆▆▃▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  env_runners/module_episode_returns_mean/prey ▇▅▅▁▃█▇▇▅▃▄▄▄▄▅▃▆▆▄▃▄▃▄▄▄▆▅▅▅▅█▆▅▄▄▅▅▅▅▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/0 ▂▁▁▃▁▁▂▃▂█▅▁▃▂▁▂▃▁▂▃▁▂▂▂▃▂▂▂▃▂▂▁▂▂▃▁▂▃▃▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/1 ▁▂▂▇▃▂▂▃▃▃▁▂▂▃▄▂▁▁▂▂▃▂▁▂▁▁▄▁▂▁▂▃▁▁▁▂▁▂▃█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/10 ▁▁▃▃▃▂▂▂▃▂▂█▂▂▃▁▃▁▁▁▂▁▂▁▂▁▅▄▃▁▂▂▄▅▁▂▁▃▁▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/11 ▂█▆▃▃▃▂▅▁▅▄▅▄▃▄▃▁▂▄▄▂▄▃▅▂▂▅▃▇▆▆▄▃▁▂▆▂▂▂▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/12 ▄▇▄▁▁▃▁▃▂▄▃▁▃▄▁▂▄▂▄▂▃▃▃▃▁▂▃▁▃▂▂▁▂▂▁▂▂▂▃█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/13 ▅▁▂▁▂▂▄▃▄▂▁▁▄▃▁▁▁▁▂▃▃▂█▂▄▂▁▂▂▁▃▂▂▁▂▂▃▃▃▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/14 ▃▃▃▄▃▁▃▂▃▄▃▃▄▁▁▁▄▂▁▄▂▄▁▂▄▂▃▃▃▂▁▂▃▆▁▄█▁▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/15 ▁▆▁▂▂▂▃▄▂▃▆▂▂▂▄█▁▂▄▃▄▄▄▃▄▃▁▁▅▃▂▅▃▂▂▃█▄▃▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/16 ▄▃▃▃▃▂▃▃█▁▃▂▃▁▁▃▂▃▁▆▆▃▃▂▂▂▂▃▃▃▂▃▃█▂▂▁▂▄▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/17 ▂▂▁▃▁▄▄▂▂▃█▂▂▁▁▂▂▁▁▂▂▂▂▂▁▂▂▂▁▂▂▂▁▃▂▂▁▂▃▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/18 ▄▄▄▁▇▄▂▄▄▄▂▃▄▄▁▃▃▁▃▃▃▄▄▃▄▃▅▂▃▃▁▃▄▄█▅▂▃▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/19 ▂▂▃▁▃▄▁▁▆▁▁▂▂▂▅▂▃▂▃▂▃▂▄▃▂▄▂▂▂▄▄▃▄▃▃█▂▃▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/2 ▂▄▄▁▆▁▁▂▃▂▅▂▃▃▁▂▅▆▁▁▅▃▁▂▂▂▁▂▄▃▇▃█▁▁▁▂▁▆▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/20 ▇▁▇▃▂▆▁▁▅▅▂▅▁▆▃▁▂▇▂▂▃▅▆▃▃█▃▄▁▄▂▂▅▄▅▁▂▂▅▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/21 ▆▄▅▁▆▁▅▅▁▅▃▆▃▂▃▁▁▆▃▂▃▂▇▄▇▂▅▆▆▅▅█▅▂▅▅▅▄▂▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/22 ▂▂▁▂▄▁▄▄▂▂▃▂▄▁▃▂▃█▁▆▂▂▂▂▁▁▂▂▃▁▂▃▁▃▃▂▂▃▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/23 ▁▆▄▃▄▄▂▄▂▁▃▂▂▂▅▅▂▃▁▁▂▃▃█▃▄▂▃▄▃▂▂▃▃▂▅▄▅▁▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/24 ▄█▂▁▂▃▁▄▃▄▃▂▇▂▁▄▃▄▁▄▃▃▅▄▃▃▄▂▂▄▂▁▁▅▃▂▂▃▄▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/25 ▁▃▃▁▂▃▄▃▃▁▄▄▅▃▄▄▃▁▃▂█▁▂▄▃▂▄▄▂▅▃▂▄▃▄▄▅▁▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/26 ▁▃▁▂▂▁▃▃▁▂▁▄▃▂▂▃▁▃▂▃▂▃█▄▁▂▁▁▂▇▃▃▃▁▂▁▂▃▃▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/27 ▃▁▂▂▂▂▂▃▂▂█▂▃▂▂▂▂▄▁▂▃▂▁▂▃▂▂▂▂▂▃▁▃▂▂▁▃▃▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/28 ▂▃▄▃▄▃▄▁▃▃▁▂▄▃▄▅▂▃▂▂▂▄▂▃▂▂▂▁▂▁▄▂▁▁▁█▄▂▂▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/29 ▄▄▂▅▆▄█▁▃▁▁▃▅▂▃▂▄▂▂▂▅▅▂▂▂▂█▁▂▃▄▁▁▃▄▂▄▃▂▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/3 ▂▂▃▃▂▁▂▄▄▁▁▂▁█▁▁▁▃▂▄▂▁▃▄▁▃▁▃▁▅▃▃▂▃▂▃▁▁▃▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/30 ▄▃█▁▂▂▂██▂▂▂▁▁▂▁██▂█▁▂▂▂▂▂▂█▁██▂▁██▁▂▁▁█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/31 ▂▃█▂▃█▂█▂█▂▂▂███▃▂█▃▂█▃▂▂██▂█▂███▁▁█▂█▃█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/32 █▂██▂▁▂█▂▂█▂█▁▂▂▁▁▂▁█▂█▂▂▁███▁▂█▂█▂██▁▁█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/33 ▃█▁█▂▂▄▃▃▂█▂██▃▃▃▂▂█▂█▂▂██▂█▂██▃▂▂▂▃▁▁▃█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/4 ▁▃▃▃▄▃▄▂▂▁▁▃▄▁▃▄▃▃▁▂▄▃▁▃▃▃▃▁▂█▃▂▃▂▂▂▂▇▃▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/5 ▄▃▂▁▂▂▁▁▂▁▂▄▇▂▂█▃▅▅▁▂▃▂▂▂▂▁▂▁▃▂▁▁▂▂▁▃▁▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/6 ▂▄▂▆▂▄▂▄▃▂▂▅▅▃▅▂▁▃▃▄▃▄▄▂▂▆█▃▂▂▂▁▂▁▂▃▂▂▃▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/7 ▁▃▄▂▂▂▁▁▁█▃▂▁▂▁▂▂▂▁▁▁▂▃▃▂▁▁▁▂▂▂▂▃▂▂▂▂▂▁▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/8 ▅▅▅▃█▁▅▁▁▄▂▄▁▁▅▂▃█▂▁▃▃▂▄▄▄▄▃▃▃▁▄▃▆▆▅▄▆▆▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/9 ▂▆▂▂▂▃▃▃▃█▃▂▂▃▁▃▂▂▁▃▂▂▃▃▁▁▁▂▂▂▃▂▁▁▂▁▂▂▁▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/0 ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇█████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/1 ▁▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/10 ▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/11 ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/12 ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/13 ▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/14 ▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/15 ▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/16 ▁▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/17 ▁▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/18 ▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/19 ▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/2 ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/20 ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/21 ▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/22 ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/23 ▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/24 ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/25 ▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/26 ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/27 ▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/28 ▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇███\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/29 ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇█████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/3 ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/30 ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/31 ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/32 ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/33 ▁▁▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/4 ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/5 ▁▁▁▁▂▂▂▂▂▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/6 ▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/7 ▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/8 ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/9 ▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             env_runners/num_env_steps_sampled ▃▂▂▃███▂▁▂▂▂██▁█▂▁▁▁▂█▂█▁▂▁▂▁▁▁▁▁▂████▂█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    env_runners/num_env_steps_sampled_lifetime ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         env_runners/num_env_steps_sampled_lifetime_throughput ▃▃▆▃▁▃▂▃▃▂▇▆█▃▃▂█▃▃▂▁▆▃▆▆▃▅▃▃▇▇▇▃▆▂▂▃▆▃▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      env_runners/num_episodes ████▁███████████████████████████████████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             env_runners/num_episodes_lifetime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                 env_runners/num_module_steps_sampled/predator █▃▁▂█▂███▃▃▃▃████▂▃▂█▂▂▂▂▃▂▃▂▄▂█▂█▃▃▃██▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/num_module_steps_sampled/prey ▃▃▇▅▅▅▄▃▅█▅▅▇▅▄▂▄▆▁▃▃▂▄▃▄▅▃▃▅▅▃▆▃▃▄▂▅▅▅▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                        env_runners/num_module_steps_sampled_lifetime/predator ▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                            env_runners/num_module_steps_sampled_lifetime/prey ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             env_runners/time_between_sampling ███▇▆▅▅▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:              env_runners/timers/connectors/AddObservationsFromEpisodesToBatch █▇▆▆▆▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▁▁▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                    env_runners/timers/connectors/AddStatesFromEpisodesToBatch ██▅▅▅▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                            env_runners/timers/connectors/AgentToModuleMapping █▆▅▅▅▃▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▂▂▁▁▁▂▂▂▂▂▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                            env_runners/timers/connectors/BatchIndividualItems ██▆▆▅▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▁▁▁▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/timers/connectors/GetActions █▇▇▆▆▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         env_runners/timers/connectors/ListifyDataForVectorEnv █▇▆▅▅▄▃▂▂▂▂▂▂▂▃▂▁▁▂▂▁▁▁▁▂▂▂▂▁▁▁▂▂▁▁▂▂▂▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                          env_runners/timers/connectors/ModuleToAgentUnmapping ██▇▇▆▅▅▄▃▃▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▂▂▂▂▂▁▁▁▁▁▂▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         env_runners/timers/connectors/NormalizeAndClipActions █▇▇▇▆▄▄▄▃▃▂▂▂▂▂▂▂▁▁▂▁▂▂▂▁▂▂▂▂▃▂▂▂▂▁▁▂▂▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   env_runners/timers/connectors/NumpyToTensor █▇▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                 env_runners/timers/connectors/RemoveSingleTsTimeRankFromBatch ██▆▆▅▅▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   env_runners/timers/connectors/TensorToNumpy ██▇▆▆▅▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                        env_runners/timers/connectors/UnBatchToIndividualItems █▇▅▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                           fault_tolerance/num_healthy_workers ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    fault_tolerance/num_remote_worker_restarts ▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      iterations_since_restore ▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇███\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:             learners/__all_modules__/learner_connector_sum_episodes_length_in ▁▁▂▃▄▅▆▇▇▇▇▇▇▇▇█████▇▇▇▇▇▇▇▇▇▇▆▆▇▇▇▆▆▆▆▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:            learners/__all_modules__/learner_connector_sum_episodes_length_out ▁▂▄▄▅▇▇▇▇▇█████████▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▆▆▆▆▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                learners/__all_modules__/num_env_steps_trained ▂██▂▁▂▂▂██▂▂▂▂█▂▁▂█▂█▂▂▂▁▂█▂▂▂▁▁▁▂█▁▂██▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                       learners/__all_modules__/num_env_steps_trained_lifetime ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:            learners/__all_modules__/num_env_steps_trained_lifetime_throughput ▃▃▆▆▁▃▃▃▃▆▃▆▆▃▅▃▃▆▃▂▃▃▂█▃▃▆▃▂▃▂▆▆▂▆▂▆▇▃▇\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                             learners/__all_modules__/num_module_steps_trained ▄▃▅▂▂▇█▅▃▆▆▁▄▅▅▃▂▂▇▁▃▃▁▆▅▂▁▆▂▁▆▂▆▂▂▂▂▆▇▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                    learners/__all_modules__/num_module_steps_trained_lifetime ▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         learners/__all_modules__/num_non_trainable_parameters ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                             learners/__all_modules__/num_trainable_parameters ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: learners/__all_modules__/timers/connectors/AddColumnsFromEpisodesToTrainBatch ▇███▇▇▇▇▇███▇▇▇▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▁▁▁▁▁▁▁▂▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: learners/__all_modules__/timers/connectors/AddObservationsFromEpisodesToBatch █▆▆▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:      learners/__all_modules__/timers/connectors/AddOneTsToEpisodesAndTruncate ████▇▆▆▅▆▇▇▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:       learners/__all_modules__/timers/connectors/AddStatesFromEpisodesToBatch ████▆▄▄▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:               learners/__all_modules__/timers/connectors/AgentToModuleMapping ▇███████▇▇▇▇▇▇▇▇▇▇▇▆▅▅▅▅▅▃▃▃▂▃▂▂▁▁▁▁▂▂▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:               learners/__all_modules__/timers/connectors/BatchIndividualItems ▁▁▅▅▇██████████▇▇▇▇▇▇▇▇▆▅▅▅▅▅▅▅▅▄▄▄▅▅▅▅▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:         learners/__all_modules__/timers/connectors/GeneralAdvantageEstimation █▆▆▆▆▆▅▅▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                      learners/__all_modules__/timers/connectors/NumpyToTensor ██▇▇▆▆▆▆▆▆▅▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                          learners/predator/curr_entropy_coeff ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               learners/predator/curr_kl_coeff ▁▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▃▄▃▃▄▃▃▃▃▃▂▃▅▃▃▃▄█▃▃▄▂▂▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                             learners/predator/default_optimizer_learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     learners/predator/diff_num_grad_updates_vs_sampler_policy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     learners/predator/entropy ████▆▆▆▆▆▅▄▅▅▃▃▄▄▄▂▄▂▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     learners/predator/gradients_default_optimizer_global_norm ▁▁▁▁▁▁▂▁▂▂▁▁▃▂▃▃▃▅▃▃▃▇▂▄▄▅▅▃▆▄▃▆▄▃▂▄▃█▄▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                learners/predator/mean_kl_loss ▃▂▃▂▁▂▄▂▁▃▁▄▃▃▃▇▂▁▄▄▃▅▄▁▅▄▅▁▄█▂▆▃▃▂▃▃▂▂▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                learners/predator/module_train_batch_size_mean ▁▁▄▄▅▆▆▇▇▇▇█████▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▇▇▆▆▆▆▆▆▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    learners/predator/num_module_steps_trained █▃████▂▂█▂▂▂▂▁▃███▂█▂▁▂▁██▁▁▁▂▁█▂██▁▁▁█▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                           learners/predator/num_module_steps_trained_lifetime ▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                learners/predator/num_non_trainable_parameters ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    learners/predator/num_trainable_parameters ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 learners/predator/policy_loss ▆▆▄▅▅█▅▄▇▄▂▆▄▃▅▄▄▇▁▅▆▄▇▇▆▄▅▅▆▄▃▃▃▇▆▄▄▆▅▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  learners/predator/total_loss ▅▇▄▆▇▂▅▆▅▃▃▂▆▄▄▅▆▃▂▂▄▂▂▄▃▂▄█▁▂▅▃▂▃▅▂▇▄▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            learners/predator/vf_explained_var ▅▄▅▆▃▁▂▇▅▇▇▇▁▆█▇▇█▇█▇▇▇▆▇▄█▄▇▇▇█▇▇▇▇▅▆██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     learners/predator/vf_loss ▅▂▃▅▄▃█▃▂▄▃▆▅▅▃▂▆▅▂▃▂▃▂▅▂▆▂▂▆▂▂▁▁▃▁▁▁▁▆▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                           learners/predator/vf_loss_unclipped ▂▂▃▂▂▂▄▂▂▂▃▁▂▂▁▂▂▂▂▂▂▁▁█▁▁▁▁▁▁▂▂▁▁▂▁▁▁▁▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              learners/predator/weights_seq_no ▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              learners/prey/curr_entropy_coeff ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   learners/prey/curr_kl_coeff ▁▂▄▄▄▆▆▆▆▆▆▆▄▄▄▄▆▆▆█▄▆▃▄▄▇▇▇▇▅▅▅▅██▆▆▆▆▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                 learners/prey/default_optimizer_learning_rate ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         learners/prey/diff_num_grad_updates_vs_sampler_policy ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         learners/prey/entropy ▁▁▁▁▁▁▁▂▂▂▂▂▂▃▄▄▄▅▅▄▅▅▅▅▅▆▅▆▆▆▇▆▇▇▆▇▆▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         learners/prey/gradients_default_optimizer_global_norm ▁▂▁▁▂▄▄▂▄▃▅▄▆▃▆▅▄▅▅▅▃▄▆▃▄▅▇▇▅▆▆▆▄▄▆▆▄▆█▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    learners/prey/mean_kl_loss ▃▄█▄▃▄▄▃▂▁▂▂▄▂▂▁▃▂▂▄▄▁▃▆▄▂▂▂▁▁▂▂▂▂▃▃▂▂▃▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    learners/prey/module_train_batch_size_mean ██████▇▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        learners/prey/num_module_steps_trained ▅▆█▅▆▄▅▅▄▄▆▄▅▄▄▅▆▃▅▄▄▅▅▄▄▃▃▅▅▄▆▅▄▄▇▅▁▃▆█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               learners/prey/num_module_steps_trained_lifetime ▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    learners/prey/num_non_trainable_parameters ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        learners/prey/num_trainable_parameters ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     learners/prey/policy_loss █▅▆▂▅▅▄▄▅█▁▇▆▅▃▃▆▂▃▅▅▄▆▅▅▃▄▅▁▇▅▂▆▆▆▆▆█▇▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      learners/prey/total_loss ▄▅▃▂█▆▃▅▆▂▃▄▇▃▄▃▂▃▄▄▄▄▄▃▄▅█▅▄▆▃▄▇▅▁▅▆▄▇▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                learners/prey/vf_explained_var ▂█▅█▂█▆▄▆▆▅▆▆▅▃▄▃▆▄▃▅▅▄▄▅▅▄▇▄█▄▅▄▄▄▇▃▅▃▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         learners/prey/vf_loss ▅▁▅▃▅▃▄▂▃▄▃▄▃▅▃▃▃▆▄▄▄▅▃▄▅▄▃▃▂▃▃▄▃▄▄▂▅▄▄█\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               learners/prey/vf_loss_unclipped ▂▁█▂▁▃▃▂▁▃▁▃▂▂▅▅▄▄▃▅▆▇▆▅▄▅▄▅▂▆▄▄▄▅▄▄▂▄▄▆\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  learners/prey/weights_seq_no ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                num_env_steps_sampled_lifetime ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     num_env_steps_sampled_lifetime_throughput ▂▇▂▃▂▇▂▇▂▇▃▅▂▁▂▂▂▂▇▇▃█▃▃▂▂▂▇▂▆▁▂▂▃█▁▆▁▂▇\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         num_training_step_calls_per_iteration ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         perf/cpu_util_percent ▂▃▂▂▂▃▂▃▃▂▄█▃▂▆▂▃▃▃▆▃▃▁▄▆▁▃▃▂▁▁▁▁▂▂▁▁▁▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         perf/ram_util_percent ▁▁▂▃▃▄▆█▇▅▅▇▆▇▅▆▇▅▅▆▆▆▆▆▅▅█▆▆█▇▆▆▆▆█▂▂▄▅\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                            time_since_restore ▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                              time_this_iter_s ▃▃▅▄▂▅▃▄▃▃▇▂▄▁▁▂▃▃▃█▄▄▂▂▃▃▂▁▅▆▂▃▂▂▂▃▄▃▃▄\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                  time_total_s ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              timers/env_runner_sampling_timer ██▇▆▅▆▅▄▄▃▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▄▂▂▂▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   timers/learner_update_timer ██████▇▇▆▆▅▅▅▅▅▅▅▅▄▄▅▄▄▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                        timers/restore_workers ▁▁▁██▆▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▃▃▃▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   timers/synch_env_connectors ▁▁▁▁▁▁▁▆██▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▃\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          timers/synch_weights ▄▅▅▅▅▄▃▃▃▃▄▃▃▃▄▃▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁█▄▃▂▁▁\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     timers/training_iteration ▇██▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▃▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          timers/training_step █▆▅▅▄▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                     timestamp ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▇▇▇▇▇▇▇███\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                            training_iteration ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: \n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Run summary:\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/_disable_action_flattening False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/_disable_execution_plan_api -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                              config/_disable_initialize_loss_from_dummy_batch False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/_disable_preprocessor_api False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/_dont_auto_sync_env_runner_states False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/_enable_rl_module_api -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                             config/_fake_gpus False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/_model_config/use_attention True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/_tf_policy_handles_more_than_one_loss False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                          config/_train_batch_size_per_learner 512\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/_use_msgpack_checkpoints False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/_validate_config True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/actions_in_input_normalized False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                       config/add_default_connectors_to_env_to_module_pipeline True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                             config/add_default_connectors_to_learner_pipeline True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                       config/add_default_connectors_to_module_to_env_pipeline True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/always_attach_evaluation_results -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 config/auto_wrap_old_gym_envs -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/checkpoint_trainable_policies_only False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                           config/clip_actions False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                             config/clip_param 0.3\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/compress_observations False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/create_env_on_driver False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/custom_async_evaluation_function -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    config/delay_between_env_runner_restarts_s 60\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/disable_env_checking False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     config/eager_max_retraces 20\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/eager_tracing True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/enable_async_evaluation -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      config/enable_connectors -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/enable_env_runner_and_connector_v2 True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                           config/enable_rl_module_and_learner True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/enable_tf1_exec_eagerly False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/entropy_coeff 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/env_config/agent_density 1000\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                           config/env_config/collective_death_penalty_for_prey -0.001\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                       config/env_config/collective_eating_reward_for_predator 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/env_config/contact_force_coefficient 5\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/env_config/contact_margin 0.2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/env_config/death_penalty_for_prey -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/env_config/dragging_force_coefficient 2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/env_config/eating_reward_for_predator 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/env_config/edge_hit_penalty 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    config/env_config/energy_cost_penalty_coef 0.001\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/env_config/episode_length 700\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                           config/env_config/food_patch_radius 4\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/env_config/food_patch_regen_time 10\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 config/env_config/food_radius 0.1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 config/env_config/food_reward 0.01\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     config/env_config/inertia True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/env_config/max_acceleration_predator 2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/env_config/max_acceleration_prey 0.2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                          config/env_config/max_number_of_food 8\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/env_config/max_seeing_angle 2.35619\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         config/env_config/max_seeing_distance 20\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/env_config/max_turn 0.7854\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/env_config/num_food_patch 2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/env_config/num_other_agents_observed 10\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/env_config/num_predators 4\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/env_config/num_preys 30\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         config/env_config/periodical_boundary False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/env_config/predator_radius 0.4\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/env_config/prey_consumed True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 config/env_config/prey_radius 0.1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/env_config/stage_size 10\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               config/env_config/starving_penalty_for_predator 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/env_config/step_per_time_increment 2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/env_config/surviving_reward_for_prey 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        config/env_config/use_polar_coordinate True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/env_config/use_speed_observation False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/env_config/use_vectorized False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                              config/env_config/wall_contact_force_coefficient 5\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/env_runner_health_probe_timeout_s 30\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                           config/env_runner_restore_timeout_s 1800\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                            config/env_task_fn -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/episode_lookback_horizon 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      config/episodes_to_numpy True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/evaluation_duration 10\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                           config/evaluation_force_reset_envs_before_iteration True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/evaluation_num_env_runners 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        config/evaluation_parallel_to_training False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/evaluation_sample_timeout_s 120\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                config/explore True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/export_native_model_files True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                           config/fake_sampler False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                  config/gamma 0.99\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/ignore_env_runner_failures False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/in_evaluation False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/input_read_episodes False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/input_read_sample_batches False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/input_spaces_jsonable True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        config/keep_per_episode_custom_metrics False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                               config/kl_coeff 0.2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                              config/kl_target 0.01\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                 config/lambda 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/local_gpu_idx 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     config/local_tf_session_args/inter_op_parallelism_threads 8\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     config/local_tf_session_args/intra_op_parallelism_threads 8\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/log_gradients True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/log_sys_usage True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                     config/lr 5e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/materialize_data False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/materialize_mapped_data True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/max_num_env_runner_restarts 1000\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                            config/max_requests_in_flight_per_aggregator_actor 100\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/max_requests_in_flight_per_env_runner 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/max_requests_in_flight_per_learner 3\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/metrics_episode_collection_timeout_s 60\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/metrics_num_episodes_for_smoothing 100\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/min_sample_timesteps_per_iteration 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/min_train_timesteps_per_iteration 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         config/minibatch_size 128\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/model/_disable_action_flattening False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        config/model/_disable_preprocessor_api False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      config/model/_time_major False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/model/_use_default_native_models -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/model/always_check_shapes False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/model/attention_dim 64\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/model/attention_head_dim 32\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/model/attention_init_gru_gate_bias 2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/model/attention_memory_inference 50\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        config/model/attention_memory_training 50\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/model/attention_num_heads 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/model/attention_num_transformer_units 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  config/model/attention_position_wise_mlp_dim 32\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/model/attention_use_n_prev_actions 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     config/model/attention_use_n_prev_rewards 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                              config/model/dim 84\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/model/framestack True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     config/model/free_log_std False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                        config/model/grayscale False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/model/log_std_clip_param 20\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/model/lstm_cell_size 256\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/model/lstm_use_prev_action False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/model/lstm_use_prev_action_reward -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/model/lstm_use_prev_reward False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      config/model/max_seq_len 20\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/model/no_final_linear False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/model/use_attention False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         config/model/use_lstm False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/model/vf_share_layers False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                        config/model/zero_mean True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      config/normalize_actions True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      config/num_aggregator_actors_per_learner 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                          config/num_consecutive_env_runner_failures_tolerance 100\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              config/num_cpus_for_main_process 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/num_cpus_per_env_runner 2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/num_cpus_per_learner 3\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                        config/num_env_runners 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/num_envs_per_env_runner 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                             config/num_epochs 10\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                               config/num_gpus 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/num_gpus_per_env_runner 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/num_gpus_per_learner 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                           config/num_learners 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/offline_sampling False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/ope_split_batch_by_episode True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/output_max_file_size 67108864\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/output_write_episodes True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/output_write_remaining_data False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/policy_map_cache -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/policy_map_capacity 100\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            config/policy_states_are_swappable False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     config/postprocess_inputs False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         config/prelearner_module_synch_period 10\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/remote_env_batch_wait_ms 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     config/remote_worker_envs False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                             config/render_env False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/restart_failed_env_runners True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        config/restart_failed_sub_environments False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/sample_timeout_s 60\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     config/sgd_minibatch_size -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/shuffle_batch_per_epoch True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/shuffle_buffer_size 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/simple_optimizer True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                              config/sync_filters_on_rollout_workers_timeout_s 10\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    config/synchronize_filters -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/tf_session_args/allow_soft_placement True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                       config/tf_session_args/device_count/CPU 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               config/tf_session_args/gpu_options/allow_growth True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                           config/tf_session_args/inter_op_parallelism_threads 2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                           config/tf_session_args/intra_op_parallelism_threads 2\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   config/tf_session_args/log_device_placement False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  config/torch_compile_learner False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   config/torch_compile_worker False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               config/torch_skip_nan_gradients False\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                       config/train_batch_size 4000\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             config/update_worker_filter_stats True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                             config/use_critic True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                config/use_gae True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                            config/use_kl_loss True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                config/use_worker_filter_stats True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                config/validate_env_runners_after_construction True\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/vf_clip_param 10\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          config/vf_loss_coeff 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                        config/vf_share_layers -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                             config/worker_cls -1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     env_runner_group/actor_manager_num_outstanding_async_reqs 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/0 -1.22372\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/1 -1.27369\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/10 -1.15618\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/11 -1.32357\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/12 -1.27475\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/13 -1.21499\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/14 -1.20283\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/15 -1.18568\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/16 -1.23377\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/17 -1.13689\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/18 -1.18561\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/19 -1.25403\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/2 -1.29348\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/20 -1.28413\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/21 -1.24573\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/22 -1.21552\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/23 -1.26359\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/24 -1.15357\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/25 -1.17421\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/26 -1.20716\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/27 -1.28338\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/28 -1.22666\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/29 -1.24432\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/3 -1.17594\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/30 8.83\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/31 9.39\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/32 8.91\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/agent_episode_returns_mean/33 9.23\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/4 -1.25315\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/5 -1.14546\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/6 -1.27632\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/7 -1.25256\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/8 -1.1857\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/agent_episode_returns_mean/9 -1.25321\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/0 124.34\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/1 106.01\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/10 122.86\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/11 103.92\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/12 98.51\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/13 115.01\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/14 107.87\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/15 99.65\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/16 88.78\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/17 114.66\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/18 112.69\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/19 97.29\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/2 88.93\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/20 113.6\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/21 98.74\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/22 108.01\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/23 111.36\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/24 117.37\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/25 103.84\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/26 107.5\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/27 96.35\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/28 107.55\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/29 122.56\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/3 105.53\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/30 383.43\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/31 383.43\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/32 383.43\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    env_runners/agent_steps/33 383.43\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/4 111.05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/5 122.3\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/6 101.98\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/7 97.21\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/8 113\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     env_runners/agent_steps/9 94.19\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                              env_runners/env_to_module_sum_episodes_length_in 327.15858\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                             env_runners/env_to_module_sum_episodes_length_out 327.15858\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/episode_duration_sec_mean 0.94251\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   env_runners/episode_len_max 701\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  env_runners/episode_len_mean 383.43\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   env_runners/episode_len_min 85\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                env_runners/episode_return_max -0.227\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               env_runners/episode_return_mean -0.4398\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                env_runners/episode_return_min -0.602\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                           env_runners/max_doa 0.24684\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                           env_runners/max_dos 0.83635\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          env_runners/mean_doa 0.09104\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          env_runners/mean_dos 0.26487\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                              env_runners/module_episode_returns_mean/predator 9.23\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                  env_runners/module_episode_returns_mean/prey -1.24432\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/0 48\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/1 132\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/10 73\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/11 184\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/12 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/13 39\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/14 262\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/15 42\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/16 22\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/17 105\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/18 302\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/19 14\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/2 33\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/20 701\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/21 123\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/22 220\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/23 92\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/24 393\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/25 506\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/26 250\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/27 83\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/28 34\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/29 208\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/3 118\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/30 701\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/31 701\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/32 701\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        env_runners/num_agent_steps_sampled/33 701\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/4 415\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/5 45\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/6 127\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/7 56\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/8 158\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         env_runners/num_agent_steps_sampled/9 33\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/0 161484\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/1 159983\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/10 162014\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/11 155368\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/12 156066\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/13 157473\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/14 159171\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/15 162997\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/16 165869\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/17 159416\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/18 166504\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/19 159588\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/2 161535\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/20 163776\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/21 161100\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/22 154471\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/23 157918\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/24 163958\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/25 157906\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/26 159559\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/27 159705\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/28 156992\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/29 158343\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/3 158719\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/30 508620\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/31 508620\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/32 508620\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               env_runners/num_agent_steps_sampled_lifetime/33 508620\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/4 162540\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/5 164158\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/6 160842\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/7 155292\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/8 162398\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                env_runners/num_agent_steps_sampled_lifetime/9 164284\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             env_runners/num_env_steps_sampled 701\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    env_runners/num_env_steps_sampled_lifetime 508336\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         env_runners/num_env_steps_sampled_lifetime_throughput 139.55851\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      env_runners/num_episodes 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             env_runners/num_episodes_lifetime 1485\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                 env_runners/num_module_steps_sampled/predator 2804\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     env_runners/num_module_steps_sampled/prey 4818\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                        env_runners/num_module_steps_sampled_lifetime/predator 2034480\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                            env_runners/num_module_steps_sampled_lifetime/prey 4809429\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                             env_runners/time_between_sampling 1.44344\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:              env_runners/timers/connectors/AddObservationsFromEpisodesToBatch 5e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                    env_runners/timers/connectors/AddStatesFromEpisodesToBatch 1e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                            env_runners/timers/connectors/AgentToModuleMapping 0.0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                            env_runners/timers/connectors/BatchIndividualItems 2e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                      env_runners/timers/connectors/GetActions 9e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         env_runners/timers/connectors/ListifyDataForVectorEnv 1e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                          env_runners/timers/connectors/ModuleToAgentUnmapping 0.0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         env_runners/timers/connectors/NormalizeAndClipActions 0.00015\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   env_runners/timers/connectors/NumpyToTensor 2e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                 env_runners/timers/connectors/RemoveSingleTsTimeRankFromBatch 0.0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                   env_runners/timers/connectors/TensorToNumpy 3e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                        env_runners/timers/connectors/UnBatchToIndividualItems 4e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                           fault_tolerance/num_healthy_workers 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    fault_tolerance/num_remote_worker_restarts 5\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      iterations_since_restore 1500\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:             learners/__all_modules__/learner_connector_sum_episodes_length_in 326.32219\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:            learners/__all_modules__/learner_connector_sum_episodes_length_out 326.32219\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                learners/__all_modules__/num_env_steps_trained 701\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                       learners/__all_modules__/num_env_steps_trained_lifetime 508620\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:            learners/__all_modules__/num_env_steps_trained_lifetime_throughput 139.55826\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                             learners/__all_modules__/num_module_steps_trained 7685\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                    learners/__all_modules__/num_module_steps_trained_lifetime 6929328\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         learners/__all_modules__/num_non_trainable_parameters 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                             learners/__all_modules__/num_trainable_parameters 159498\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: learners/__all_modules__/timers/connectors/AddColumnsFromEpisodesToTrainBatch 0.03618\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: learners/__all_modules__/timers/connectors/AddObservationsFromEpisodesToBatch 0.00015\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:      learners/__all_modules__/timers/connectors/AddOneTsToEpisodesAndTruncate 0.00306\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:       learners/__all_modules__/timers/connectors/AddStatesFromEpisodesToBatch 1e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:               learners/__all_modules__/timers/connectors/AgentToModuleMapping 0.00227\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:               learners/__all_modules__/timers/connectors/BatchIndividualItems 0.03935\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:         learners/__all_modules__/timers/connectors/GeneralAdvantageEstimation 0.00681\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                      learners/__all_modules__/timers/connectors/NumpyToTensor 0.00025\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                          learners/predator/curr_entropy_coeff 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               learners/predator/curr_kl_coeff 3.97477\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                             learners/predator/default_optimizer_learning_rate 5e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     learners/predator/diff_num_grad_updates_vs_sampler_policy 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     learners/predator/entropy -5.45021\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                     learners/predator/gradients_default_optimizer_global_norm 72.67535\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                learners/predator/mean_kl_loss 0.01416\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                learners/predator/module_train_batch_size_mean 1309.28877\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    learners/predator/num_module_steps_trained 2808\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                           learners/predator/num_module_steps_trained_lifetime 2040420\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                learners/predator/num_non_trainable_parameters 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    learners/predator/num_trainable_parameters 79749\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                 learners/predator/policy_loss -0.00933\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  learners/predator/total_loss 0.56643\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                            learners/predator/vf_explained_var 0.73553\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     learners/predator/vf_loss 0.51946\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                           learners/predator/vf_loss_unclipped 0.51946\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              learners/predator/weights_seq_no 1485\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              learners/prey/curr_entropy_coeff 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   learners/prey/curr_kl_coeff 1.1546\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                 learners/prey/default_optimizer_learning_rate 5e-05\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         learners/prey/diff_num_grad_updates_vs_sampler_policy 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         learners/prey/entropy 16.66748\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                         learners/prey/gradients_default_optimizer_global_norm 4.41272\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                    learners/prey/mean_kl_loss 0.00789\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    learners/prey/module_train_batch_size_mean 3069.68789\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        learners/prey/num_module_steps_trained 4877\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                               learners/prey/num_module_steps_trained_lifetime 4888908\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                    learners/prey/num_non_trainable_parameters 0\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                        learners/prey/num_trainable_parameters 79749\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     learners/prey/policy_loss 0.04992\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                      learners/prey/total_loss 0.28008\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                learners/prey/vf_explained_var 0.30256\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         learners/prey/vf_loss 0.22104\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                               learners/prey/vf_loss_unclipped 0.22104\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                  learners/prey/weights_seq_no 1485\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                num_env_steps_sampled_lifetime 508336\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                     num_env_steps_sampled_lifetime_throughput 139.55851\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                         num_training_step_calls_per_iteration 1\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         perf/cpu_util_percent 23.84286\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                         perf/ram_util_percent 47.47143\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                            time_since_restore 4573.26985\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                              time_this_iter_s 3.69886\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                  time_total_s 4573.26985\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                              timers/env_runner_sampling_timer 1.89652\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   timers/learner_update_timer 1.07272\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                        timers/restore_workers 0.015\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                   timers/synch_env_connectors 0.01043\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          timers/synch_weights 0.00354\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                     timers/training_iteration 2.97932\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                          timers/training_step 2.96348\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                                     timestamp 1738417791\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb:                                                            training_iteration 1500\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: \n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: 🚀 View run PPO_Particle2dEnvironment_7c04b_00000 at: https://wandb.ai/tcazalet_airo/marl-rllib/runs/7c04b_00000\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: ⭐️ View project at: https://wandb.ai/tcazalet_airo/marl-rllib\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Synced 5 W&B file(s), 153 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001B[36m(_WandbLoggingActor pid=9631)\u001B[0m wandb: Find logs at: ./wandb/run-20250201_133157-7c04b_00000/logs\n",
      "2025-02-02 10:32:18,617\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-02-02 10:32:18,733\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/tanguy/Code/Finebouche/collective_behavior/project/ray_results/PPO_2025-02-01_13-31-39' in 0.1146s.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 65\u001B[0m\n\u001B[1;32m     55\u001B[0m     tuner \u001B[38;5;241m=\u001B[39m tune\u001B[38;5;241m.\u001B[39mTuner\u001B[38;5;241m.\u001B[39mrestore(\n\u001B[1;32m     56\u001B[0m         trainable \u001B[38;5;241m=\u001B[39m ALGO,\n\u001B[1;32m     57\u001B[0m         path \u001B[38;5;241m=\u001B[39m path, \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     60\u001B[0m         restart_errored\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     61\u001B[0m     )\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Run the experiment \u001B[39;00m\n\u001B[0;32m---> 65\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m ray\u001B[38;5;241m.\u001B[39mshutdown()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/tune/tuner.py:377\u001B[0m, in \u001B[0;36mTuner.fit\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Executes hyperparameter tuning job as configured and returns result.\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \n\u001B[1;32m    347\u001B[0m \u001B[38;5;124;03mFailure handling:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;124;03m    RayTaskError: If user-provided trainable raises an exception\u001B[39;00m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_ray_client:\n\u001B[0;32m--> 377\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_local_tuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    379\u001B[0m     (\n\u001B[1;32m    380\u001B[0m         progress_reporter,\n\u001B[1;32m    381\u001B[0m         string_queue,\n\u001B[1;32m    382\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_remote_tuner_for_jupyter_progress_reporting()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:501\u001B[0m, in \u001B[0;36mTunerInternal.fit\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    499\u001B[0m param_space \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_space)\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_restored:\n\u001B[0;32m--> 501\u001B[0m     analysis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_space\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    503\u001B[0m     analysis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_resume(trainable, param_space)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:617\u001B[0m, in \u001B[0;36mTunerInternal._fit_internal\u001B[0;34m(self, trainable, param_space)\u001B[0m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001B[39;00m\n\u001B[1;32m    605\u001B[0m args \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    606\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tune_run_arguments(trainable),\n\u001B[1;32m    607\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mdict\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    615\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tuner_kwargs,\n\u001B[1;32m    616\u001B[0m }\n\u001B[0;32m--> 617\u001B[0m analysis \u001B[38;5;241m=\u001B[39m \u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    619\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclear_remote_string_queue()\n\u001B[1;32m    621\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m analysis\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/tune/tune.py:1026\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     _report_air_progress(runner, air_progress_reporter, force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   1024\u001B[0m all_trials \u001B[38;5;241m=\u001B[39m runner\u001B[38;5;241m.\u001B[39mget_trials()\n\u001B[0;32m-> 1026\u001B[0m \u001B[43mrunner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcleanup\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1028\u001B[0m incomplete_trials \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m trial \u001B[38;5;129;01min\u001B[39;00m all_trials:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:1975\u001B[0m, in \u001B[0;36mTuneController.cleanup\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1973\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcleanup\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1974\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Cleanup trials and callbacks.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1975\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cleanup_trials\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1976\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mend_experiment_callbacks()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:792\u001B[0m, in \u001B[0;36mTuneController._cleanup_trials\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    787\u001B[0m     trial \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actor_to_trial[tracked_actor]\n\u001B[1;32m    788\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m    789\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScheduling trial stop at end of experiment (trial \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrial\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m): \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    790\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtracked_actor\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    791\u001B[0m     )\n\u001B[0;32m--> 792\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_schedule_trial_stop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[38;5;66;03m# Clean up cached actors now\u001B[39;00m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cleanup_cached_actors(force_all\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:1403\u001B[0m, in \u001B[0;36mTuneController._schedule_trial_stop\u001B[0;34m(self, trial, exception)\u001B[0m\n\u001B[1;32m   1399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actor_to_trial\u001B[38;5;241m.\u001B[39mpop(tracked_actor)\n\u001B[1;32m   1401\u001B[0m trial\u001B[38;5;241m.\u001B[39mset_ray_actor(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1403\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_remove_actor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtracked_actor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtracked_actor\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:811\u001B[0m, in \u001B[0;36mTuneController._remove_actor\u001B[0;34m(self, tracked_actor)\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_remove_actor\u001B[39m(\u001B[38;5;28mself\u001B[39m, tracked_actor: TrackedActor):\n\u001B[0;32m--> 811\u001B[0m     stop_future \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_actor_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mschedule_actor_task\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtracked_actor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_return_future\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m    813\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    814\u001B[0m     now \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[1;32m    816\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actor_manager\u001B[38;5;241m.\u001B[39mremove_actor(\n\u001B[1;32m    817\u001B[0m         tracked_actor, kill\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, stop_future\u001B[38;5;241m=\u001B[39mstop_future\n\u001B[1;32m    818\u001B[0m     ):\n\u001B[1;32m    819\u001B[0m         \u001B[38;5;66;03m# If the actor was previously alive, track\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/air/execution/_internal/actor_manager.py:736\u001B[0m, in \u001B[0;36mRayActorManager.schedule_actor_task\u001B[0;34m(self, tracked_actor, method_name, args, kwargs, on_result, on_error, _return_future)\u001B[0m\n\u001B[1;32m    732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pending_actors_to_enqueued_actor_tasks[tracked_actor]\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m    733\u001B[0m         (tracked_actor_task, method_name, args, kwargs)\n\u001B[1;32m    734\u001B[0m     )\n\u001B[1;32m    735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 736\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_schedule_tracked_actor_task\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    737\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtracked_actor_task\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtracked_actor_task\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    738\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    739\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    740\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    741\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_return_future\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_return_future\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    742\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    743\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _return_future:\n\u001B[1;32m    744\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m res[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/air/execution/_internal/actor_manager.py:775\u001B[0m, in \u001B[0;36mRayActorManager._schedule_tracked_actor_task\u001B[0;34m(self, tracked_actor_task, method_name, args, kwargs, _return_future)\u001B[0m\n\u001B[1;32m    770\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_error\u001B[39m(exception: \u001B[38;5;167;01mException\u001B[39;00m):\n\u001B[1;32m    771\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actor_task_failed(\n\u001B[1;32m    772\u001B[0m         tracked_actor_task\u001B[38;5;241m=\u001B[39mtracked_actor_task, exception\u001B[38;5;241m=\u001B[39mexception\n\u001B[1;32m    773\u001B[0m     )\n\u001B[0;32m--> 775\u001B[0m future \u001B[38;5;241m=\u001B[39m \u001B[43mremote_fn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mremote\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    777\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actor_task_events\u001B[38;5;241m.\u001B[39mtrack_future(\n\u001B[1;32m    778\u001B[0m     future\u001B[38;5;241m=\u001B[39mfuture, on_result\u001B[38;5;241m=\u001B[39mon_result, on_error\u001B[38;5;241m=\u001B[39mon_error\n\u001B[1;32m    779\u001B[0m )\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tracked_actors_to_task_futures[tracked_actor]\u001B[38;5;241m.\u001B[39madd(future)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/actor.py:206\u001B[0m, in \u001B[0;36mActorMethod.remote\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mremote\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 206\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_remote\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:21\u001B[0m, in \u001B[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(fn)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mauto_init_wrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     20\u001B[0m     auto_init_ray()\n\u001B[0;32m---> 21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/util/tracing/tracing_helper.py:422\u001B[0m, in \u001B[0;36m_tracing_actor_method_invocation.<locals>._start_span\u001B[0;34m(self, args, kwargs, *_args, **_kwargs)\u001B[0m\n\u001B[1;32m    420\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    421\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_ray_trace_ctx\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m kwargs\n\u001B[0;32m--> 422\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    424\u001B[0m class_name \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actor_ref()\u001B[38;5;241m.\u001B[39m_ray_actor_creation_function_descriptor\u001B[38;5;241m.\u001B[39mclass_name\n\u001B[1;32m    426\u001B[0m )\n\u001B[1;32m    427\u001B[0m method_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_method_name\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/actor.py:366\u001B[0m, in \u001B[0;36mActorMethod._remote\u001B[0;34m(self, args, kwargs, name, num_returns, max_task_retries, retry_exceptions, concurrency_group, _generator_backpressure_num_objects, enable_task_events)\u001B[0m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decorator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    364\u001B[0m     invocation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decorator(invocation)\n\u001B[0;32m--> 366\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minvocation\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/actor.py:347\u001B[0m, in \u001B[0;36mActorMethod._remote.<locals>.invocation\u001B[0;34m(args, kwargs)\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m actor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLost reference to actor\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 347\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mactor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_actor_method_call\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_method_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_returns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_returns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_task_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_task_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretry_exceptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_exceptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconcurrency_group_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcurrency_group\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgenerator_backpressure_num_objects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_generator_backpressure_num_objects\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_task_events\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_task_events\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/collective_env/lib/python3.11/site-packages/ray/actor.py:1503\u001B[0m, in \u001B[0;36mActorHandle._actor_method_call\u001B[0;34m(self, method_name, args, kwargs, name, num_returns, max_task_retries, retry_exceptions, concurrency_group_name, generator_backpressure_num_objects, enable_task_events)\u001B[0m\n\u001B[1;32m   1500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generator_backpressure_num_objects \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1501\u001B[0m     generator_backpressure_num_objects \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1503\u001B[0m object_refs \u001B[38;5;241m=\u001B[39m \u001B[43mworker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcore_worker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubmit_actor_task\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1504\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ray_actor_language\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1505\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ray_actor_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1506\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunction_descriptor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1507\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlist_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_returns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1510\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_task_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretry_exceptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1512\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretry_exception_allowlist\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1513\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ray_actor_method_cpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1514\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconcurrency_group_name\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconcurrency_group_name\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1515\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgenerator_backpressure_num_objects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1516\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_task_events\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1517\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_returns \u001B[38;5;241m==\u001B[39m STREAMING_GENERATOR_RETURN:\n\u001B[1;32m   1520\u001B[0m     \u001B[38;5;66;03m# Streaming generator will return a single ref\u001B[39;00m\n\u001B[1;32m   1521\u001B[0m     \u001B[38;5;66;03m# that is for the generator task.\u001B[39;00m\n\u001B[1;32m   1522\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(object_refs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "import os\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=num_cpus, \n",
    "    num_gpus=num_gpus,\n",
    "    ignore_reinit_error = True,\n",
    ")\n",
    "\n",
    "############################################\n",
    "# Where to save \n",
    "############################################\n",
    "# absolute path + ray_results directory\n",
    "storage_path=os.getcwd() + \"/ray_results\"\n",
    "checkpoint_folder = None # is something like \"PPO_2024-12-19_01-09-51\"\n",
    "\n",
    "\n",
    "############################################\n",
    "# Config\n",
    "############################################\n",
    "config_dict = ppo_config.to_dict()\n",
    "\n",
    "# Environment parameters\n",
    "#config_dict[\"env_config\"][\"friction_regime\"] = tune.grid_search([\"linear\", \"quadratic\"])\n",
    "#config_dict[\"env_config\"][\"periodical_boundary\"] = tune.grid_search([True, False])\n",
    "config_dict[\"env_config\"][\"prey_consumed\"] = tune.grid_search([True, False])\n",
    "\n",
    "#config_dict[\"env_config\"][\"num_food_patch\"] = tune.grid_search([0, 2])\n",
    "\n",
    "# RLlib parameters\n",
    "#config_dict[\"train_batch_size_per_learner\"] = tune.grid_search([256, 512, 1024])\n",
    "\n",
    "\n",
    "############################################\n",
    "# Build the Tuner\n",
    "############################################\n",
    "if checkpoint_folder is None : \n",
    "    tuner = tune.Tuner(\n",
    "        trainable = ALGO,                                     # Defined before\n",
    "        param_space=config_dict,                              # Defined before\n",
    "        run_config=train.RunConfig(    \n",
    "            storage_path=storage_path,\n",
    "            stop={\"training_iteration\": 1500},\n",
    "            verbose=3,\n",
    "            callbacks=tune_callbacks,\n",
    "            checkpoint_config=train.CheckpointConfig(         \n",
    "                checkpoint_at_end=True,\n",
    "                checkpoint_frequency=100,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "else:  # If we start a training that failed\n",
    "    path = storage_path + \"/\" + checkpoint_folder\n",
    "    # Restore the training\n",
    "    tuner = tune.Tuner.restore(\n",
    "        trainable = ALGO,\n",
    "        path = path, \n",
    "        resume_unfinished=True, \n",
    "        resume_errored=True,\n",
    "        restart_errored=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "# Run the experiment \n",
    "results = tuner.fit()\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d90a6-5207-4d0c-86bb-c1bef3ae8ca6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T09:32:19.327520Z",
     "iopub.status.idle": "2025-02-02T09:32:19.327628Z",
     "shell.execute_reply": "2025-02-02T09:32:19.327572Z",
     "shell.execute_reply.started": "2025-02-02T09:32:19.327566Z"
    }
   },
   "outputs": [],
   "source": [
    "ray.nodes()[0]\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5f306-eb46-410a-80a3-6050faaa92ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6296a38e-6484-492c-a49e-61772669e63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c62942-4501-4c52-9848-6250d456d4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_behavior",
   "language": "python",
   "name": "collective_behavior"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
