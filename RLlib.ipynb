{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b7cc4-c1da-49fb-85a5-29eee064dd0d",
   "metadata": {},
   "source": [
    "# Some preliminary checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14976ee1-cfb2-4052-a40b-0f316a54d317",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import ray \n",
    "\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "print(\"onnx Available:\", torch.onnx.is_onnxrt_backend_supported())\n",
    "torch._dynamo.list_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367a277-f21e-4ccb-9fc1-c5c1c1dd292b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# print number of gpus / CPUs\n",
    "print(\"Number of CPUs: \", psutil.cpu_count())\n",
    "\n",
    "num_cpus = 34\n",
    "num_gpus = 0\n",
    "num_learner = 0\n",
    "\n",
    "assert num_cpus <= psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421e6f2-e64c-4f7c-8ef2-cd969f9888fe",
   "metadata": {},
   "source": [
    "# Environement and algorithm configuration\n",
    "\n",
    "Some of the commented lines are preparation work to use a futur feature of RLLib\n",
    "\n",
    "Note: In multi-agent environments, `rollout_fragment_lenght` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec\n",
    "#from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\n",
    "#from ray.rllib.core.rl_module.marl_module import MultiAgentRLModuleSpec\n",
    "\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "ALGO = \"PPO\"        \n",
    "FRAMEWORK= \"torch\" # \"tf2\" or \"torch\"\n",
    "env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "config = (\n",
    "    get_trainable_cls(ALGO)\n",
    "    .get_default_config()\n",
    "    .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(\n",
    "        FRAMEWORK,\n",
    "    )\n",
    "    .training(\n",
    "        num_sgd_iter=5, \n",
    "        sgd_minibatch_size=256,             # the batch size\n",
    "        train_batch_size=524288,             # the number of step collected\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [64, 64, 64], \n",
    "            #\"use_attention\": True,\n",
    "            #\"use_lstm\": False,\n",
    "            #\"max_seq_len\": 5,\n",
    "            #\"lstm_cell_size\": 16,\n",
    "        },\n",
    "        #lr=tune.grid_search([0.01, 0.001, 0.0001])\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"],\n",
    "        count_steps_by=\"agent_steps\",\n",
    "    )\n",
    "    #rl_module_api\n",
    "    .experimental(_enable_new_api_stack=True)\n",
    "    .rl_module(\n",
    "#        rl_module_spec=MultiAgentRLModuleSpec(\n",
    "#            module_specs={\n",
    "#                \"prey\": SingleAgentRLModuleSpec(\n",
    "#                    module_class=PPOTorchRLModule,\n",
    "#                    observation_space=env.observation_space,\n",
    "#                    action_space=env.action_space,\n",
    "#                    model_config_dict={\"fcnet_hiddens\": [64, 64, 64]},\n",
    "#                    catalog_class=PPOCatalog\n",
    "#                ),\n",
    "#                \"predator\": SingleAgentRLModuleSpec(\n",
    "#                    module_class=PPOTorchRLModule,\n",
    "#                    observation_space=env.observation_space,\n",
    "#                    action_space=env.action_space,\n",
    "#                    model_config_dict={\"fcnet_hiddens\": [64, 64, 64]},\n",
    "#                    catalog_class=PPOCatalog\n",
    "#                ),\n",
    "#            }\n",
    "#        ),\n",
    "    )\n",
    "    .rollouts(\n",
    "        rollout_fragment_length=\"auto\", # explained here : https://docs.ray.io/en/latest/rllib/rllib-sample-collection.html\n",
    "        batch_mode= 'truncate_episodes',\n",
    "        num_rollout_workers=num_cpus-num_learner-1,\n",
    "        num_envs_per_worker=2,\n",
    "        #create_env_on_local_worker=False,\n",
    "    )\n",
    "    .resources(\n",
    "        #num_gpus = num_gpus,\n",
    "        #num_gpus_per_worker=0,\n",
    "        #num_cpus_per_worker=2,\n",
    "        # learner workers when using learner api - doesn't work on arm (mac) yet\n",
    "        #num_learner_workers=num_learner,\n",
    "        #num_gpus_per_learner_worker=1, # always 1 for PPO\n",
    "        #num_cpus_per_learner_worker=1,\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd95c1-c2d5-439c-9478-cf1f15c10244",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cfa99f-260e-4407-814c-b2f6d715797e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "path_to_checkpoint = None #\"/Users/tanguy/ray_results/PPO_2023-12-10_17-58-05/PPO_CustomEnvironment_49b3e_00000_0_2023-12-10_17-58-05/checkpoint_000134\"\n",
    "\n",
    "def restore_weights(path, policy_type):\n",
    "    checkpoint_path = os.path.join(path, f\"policies/{policy_type}\")\n",
    "    restored_policy = Policy.from_checkpoint(checkpoint_path)\n",
    "    return restored_policy.get_weights()\n",
    "\n",
    "if path_to_checkpoint is not None: \n",
    "    class RestoreWeightsCallback(DefaultCallbacks):\n",
    "        def on_algorithm_init(self, *, algorithm: \"Algorithm\", **kwargs) -> None:\n",
    "            algorithm.set_weights({\"predator\": restore_weights(path_to_checkpoint, \"predator\")})\n",
    "            algorithm.set_weights({\"prey\": restore_weights(path_to_checkpoint, \"prey\")})\n",
    "\n",
    "    config.callbacks(RestoreWeightsCallback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5385022-05b2-42ae-a406-87b46736f257",
   "metadata": {},
   "source": [
    "## Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056eb565-f91d-4c3c-bced-93a56886dca0",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "from ray.tune import Tuner\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=num_cpus, \n",
    "    num_gpus=num_gpus\n",
    ")\n",
    "\n",
    "# Stop criterium\n",
    "stop = {  \n",
    "    \"training_iteration\": 2000,\n",
    "    #\"timesteps_total\": 200000000,\n",
    "}\n",
    "\n",
    "# To use Wanddb\n",
    "callbacks = [WandbLoggerCallback(                   \n",
    "    project=\"marl-rllib\", \n",
    "    group=\"PPO\",\n",
    "    api_key=\"90dc2cefddde123eaac0caae90161981ed969abe\",\n",
    "    log_config=True,\n",
    ")]\n",
    "\n",
    "# When to save the models \n",
    "checkpoint_config = train.CheckpointConfig(         \n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_frequency=10,\n",
    ")\n",
    "\n",
    "if path_to_checkpoint is None : \n",
    "    tuner = tune.Tuner(\n",
    "        ALGO,                                                 # Defined before\n",
    "        param_space=config,                                   # Defined before\n",
    "        run_config=train.RunConfig(                          \n",
    "            stop=stop,\n",
    "            verbose=3,\n",
    "            callbacks=callbacks,\n",
    "            checkpoint_config=checkpoint_config,\n",
    "        ),\n",
    "    )\n",
    "    # Run the experiment \n",
    "    results = tuner.fit()\n",
    "    \n",
    "else: \n",
    "    callbacks.append(RestoreWeightsCallback)\n",
    "\n",
    "    results = tune.run(\n",
    "        ALGO,\n",
    "        config=config.to_dict(),\n",
    "        stop=stop,\n",
    "        verbose=3,\n",
    "        callbacks=callbacks,\n",
    "        checkpoint_config=checkpoint_config,\n",
    "    )\n",
    "\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43c80fe3-9e29-46f5-95b4-38e1f582eb27",
   "metadata": {},
   "source": [
    "from ray import tune\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=num_cpus, \n",
    "    num_gpus=num_gpus\n",
    ")\n",
    "\n",
    "experiment_dir = \"/Users/tanguy/ray_results/PPO_2023-12-10_17-58-05/\"\n",
    "# Restore the training\n",
    "tuner = tune.Tuner.restore(\n",
    "    experiment_dir, \n",
    "    trainable=tune.with_resources(\n",
    "        tune.with_parameters(self.model),\n",
    "        resources={\"cpu\": self.cpuFrac, \"gpu\": self.gpuFrac}\n",
    "    ),\n",
    "    resume_unfinished=True, \n",
    "    resume_errored=True,\n",
    "    restart_errored=False,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc839667-1769-4da3-b819-e8ef65a37e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_env",
   "language": "python",
   "name": "collective_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
