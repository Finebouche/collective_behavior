{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b7cc4-c1da-49fb-85a5-29eee064dd0d",
   "metadata": {},
   "source": [
    "# Some preliminary checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14976ee1-cfb2-4052-a40b-0f316a54d317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T07:39:50.407194Z",
     "iopub.status.busy": "2023-09-26T07:39:50.407037Z",
     "iopub.status.idle": "2023-09-26T07:39:53.005173Z",
     "shell.execute_reply": "2023-09-26T07:39:53.004893Z",
     "shell.execute_reply.started": "2023-09-26T07:39:50.407171Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c3bb7-053c-498c-a15a-8d42031fb8d0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T07:39:53.006293Z",
     "iopub.status.busy": "2023-09-26T07:39:53.006150Z",
     "iopub.status.idle": "2023-09-26T07:39:56.018108Z",
     "shell.execute_reply": "2023-09-26T07:39:56.017755Z",
     "shell.execute_reply.started": "2023-09-26T07:39:53.006285Z"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "\n",
    "## The RLlib configuration\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.run = \"PPO\"\n",
    "        self.framework = \"torch\" # \"tf2\" or \"torch\"\n",
    "        self.stop_iters = 5\n",
    "        self.stop_timesteps = 20000\n",
    "        self.stop_reward = 0.1\n",
    "        self.as_test = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "## Generate the configuration\n",
    "ray.init()\n",
    "env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(args.framework)\n",
    "    .training(num_sgd_iter=10, sgd_minibatch_size=256, train_batch_size=4000)\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"]\n",
    "    )\n",
    "    .rl_module(_enable_rl_module_api=True)\n",
    "    .training(_enable_learner_api=True)\n",
    "    .rollouts(\n",
    "        rollout_fragment_length= 200,\n",
    "        batch_mode= 'truncate_episodes',\n",
    "        num_rollout_workers=3\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus = ray.cluster_resources().get('GPU', 0),\n",
    "        num_gpus_per_worker=0,\n",
    "        num_cpus_per_worker=2,\n",
    "        # learner workers\n",
    "        num_learner_workers=2,\n",
    "        num_gpus_per_learner_worker=0,\n",
    "        num_cpus_per_learner_worker=2,\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056eb565-f91d-4c3c-bced-93a56886dca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T07:39:56.018806Z",
     "iopub.status.busy": "2023-09-26T07:39:56.018713Z",
     "iopub.status.idle": "2023-09-26T07:41:25.263142Z",
     "shell.execute_reply": "2023-09-26T07:41:25.262652Z",
     "shell.execute_reply.started": "2023-09-26T07:39:56.018796Z"
    }
   },
   "outputs": [],
   "source": [
    "## Run the experiemnt    \n",
    "tuner = tune.Tuner(\n",
    "    args.run,\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        stop={\n",
    "            \"training_iteration\": args.stop_iters,\n",
    "            \"timesteps_total\": args.stop_timesteps,\n",
    "            \"episode_reward_mean\": args.stop_reward,\n",
    "        },\n",
    "        verbose=3,\n",
    "        callbacks=[WandbLoggerCallback(\n",
    "            project=\"marl-rllib\", \n",
    "            api_key=\"90dc2cefddde123eaac0caae90161981ed969abe\"\n",
    "        )],\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "if args.as_test:\n",
    "    print(\"Checking if learning goals were achieved\")\n",
    "    check_learning_achieved(results, args.stop_reward)\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53c262-da4a-41ca-bc49-b97821f17639",
   "metadata": {},
   "source": [
    "# Render episode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67626d-946b-45ea-b59d-c6ed65ab1772",
   "metadata": {},
   "source": [
    "### Retrieve checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edfb36-fe9d-47f1-a07d-cbaa2b709b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T07:41:25.263812Z",
     "iopub.status.busy": "2023-09-26T07:41:25.263703Z",
     "iopub.status.idle": "2023-09-26T07:41:25.267513Z",
     "shell.execute_reply": "2023-09-26T07:41:25.267208Z",
     "shell.execute_reply.started": "2023-09-26T07:41:25.263799Z"
    }
   },
   "outputs": [],
   "source": [
    "best_checkpoint = results.get_best_result().checkpoint\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96925a71-e8c6-4d9b-97b5-593a15214c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T07:41:25.268135Z",
     "iopub.status.busy": "2023-09-26T07:41:25.268036Z",
     "iopub.status.idle": "2023-09-26T07:41:33.525957Z",
     "shell.execute_reply": "2023-09-26T07:41:33.525638Z",
     "shell.execute_reply.started": "2023-09-26T07:41:25.268126Z"
    }
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "algo = Algorithm.from_checkpoint(best_checkpoint)\n",
    "\n",
    "# After loading the algorithm\n",
    "local_worker = algo.workers.local_worker()\n",
    "available_policy_ids = list(local_worker.policy_map.keys())\n",
    "print(\"Available Policy IDs:\", available_policy_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18e7bb-3a95-4978-82ce-03d4c8bc9b6c",
   "metadata": {},
   "source": [
    "### Run and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8874e06-f306-4ab1-be42-084d479af642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T07:41:33.528531Z",
     "iopub.status.busy": "2023-09-26T07:41:33.528434Z",
     "iopub.status.idle": "2023-09-26T07:41:36.308642Z",
     "shell.execute_reply": "2023-09-26T07:41:36.308098Z",
     "shell.execute_reply.started": "2023-09-26T07:41:33.528520Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_observations(observation, agent_ids, truncation=None):\n",
    "    loc_x = [observation[key][4] if key in observation else 0 for key in agent_ids]\n",
    "    loc_y = [observation[key][5] if key in observation else 0 for key in agent_ids]\n",
    "    if truncation:\n",
    "        still_in_the_game = [1 if not truncation[key] else 0 for key in agent_ids]\n",
    "    else:\n",
    "        still_in_the_game = [1 for _ in agent_ids]\n",
    "    observations[\"loc_x\"].append(np.array(loc_x))\n",
    "    observations[\"loc_y\"].append(np.array(loc_y))\n",
    "    observations[\"still_in_the_game\"].append(np.array(still_in_the_game))\n",
    "    \n",
    "    return observations\n",
    "\n",
    "# Use the first available policy ID\n",
    "policy_id = available_policy_ids[0]\n",
    "\n",
    "step_count = 0\n",
    "observations = {\"loc_x\": [], \"loc_y\": [], \"still_in_the_game\": []}\n",
    "\n",
    "observation, _ = env.reset()\n",
    "agent_ids = env._agent_ids\n",
    "loc_x, loc_y, still_in_the_game = process_observations(observation, agent_ids)\n",
    "\n",
    "\n",
    "while step_count < 500:\n",
    "    actions = {\n",
    "        key: algo.compute_single_action(\n",
    "            value, policy_id=\"prey\" if env.agents[key].agent_type == 0 else \"predator\"\n",
    "        ) for key, value in observation.items()\n",
    "    }\n",
    "    \n",
    "    observation, _, termination, truncation, _ = env.step(actions)\n",
    "    \n",
    "    observations = process_observations(observation, agent_ids, truncation)\n",
    "    \n",
    "    step_count += 1\n",
    "\n",
    "stage_size = env.stage_size\n",
    "observations[\"loc_x\"] = np.array(observations[\"loc_x\"]) * stage_size\n",
    "observations[\"loc_y\"] = np.array(observations[\"loc_y\"]) * stage_size\n",
    "observations[\"still_in_the_game\"] = np.array(observations[\"still_in_the_game\"])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe696df-26b6-4a1b-8692-8965b6798da0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T07:41:36.309388Z",
     "iopub.status.busy": "2023-09-26T07:41:36.309289Z",
     "iopub.status.idle": "2023-09-26T07:41:36.489387Z",
     "shell.execute_reply": "2023-09-26T07:41:36.488639Z",
     "shell.execute_reply.started": "2023-09-26T07:41:36.309378Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import animation\n",
    "\n",
    "importlib.reload(animation)\n",
    "from animation import generate_animation\n",
    "\n",
    "ani = generate_animation(observations, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695eb08-dd06-49f2-8cb0-c7285ca37570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-26T07:41:36.500348Z",
     "iopub.status.busy": "2023-09-26T07:41:36.499166Z",
     "iopub.status.idle": "2023-09-26T07:41:40.891192Z",
     "shell.execute_reply": "2023-09-26T07:41:40.890253Z",
     "shell.execute_reply.started": "2023-09-26T07:41:36.500317Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de9596-3117-4b09-ba52-c7e05c8acf7a",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801af95-5ed3-4128-9c15-44f465cae96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "def restore_policy_and_weights(policy_type):\n",
    "    checkpoint_path = os.path.join(best_checkpoint.to_directory(), f\"policies/{policy_type}\")\n",
    "    restored_policy = Policy.from_checkpoint(checkpoint_path)\n",
    "    return restored_policy.get_weights()\n",
    "\n",
    "restored_policy_predator_weights = restore_policy_and_weights(\"predator\")\n",
    "restored_policy_prey_weights = restore_policy_and_weights(\"prey\")\n",
    "\n",
    "print(\"Starting new tune.Tuner().fit()\")\n",
    "\n",
    "ray.init()\n",
    "\n",
    "# Start our actual experiment.\n",
    "stop = {\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "}\n",
    "\n",
    "class RestoreWeightsCallback(DefaultCallbacks):\n",
    "    def on_algorithm_init(self, *, algorithm: \"Algorithm\", **kwargs) -> None:\n",
    "        algorithm.set_weights({\"predator\": restored_policy_predator_weights})\n",
    "        algorithm.set_weights({\"prey\": restored_policy_prey_weights})\n",
    "\n",
    "config.callbacks(RestoreWeightsCallback)\n",
    "\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    stop=stop,\n",
    "    config=config.to_dict(),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "if args.as_test:\n",
    "    check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dc324-7316-4f4d-945d-55a81d1bdb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2f440ff6-9d75-4466-8b96-19421a01be6f",
   "metadata": {},
   "source": [
    "1GPU V100  2CPU\n",
    "426 for 20000 with         num_gpus_per_learner_worker=1, num_learner_workers=1\n",
    "\n",
    "2CPU\n",
    "378.389 for 20000 without\n",
    "\n",
    "4GPU, 2CPU\n",
    "295.219 for 20000 with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85972d-e671-410b-a555-73e52fde2c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_behavior_env",
   "language": "python",
   "name": "collective_behavior_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
