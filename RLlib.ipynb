{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6dda5f-a9b9-4d09-8891-5672135f526a",
   "metadata": {},
   "source": [
    "# Our Environmeent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.run = \"PPO\"\n",
    "        self.framework = \"torch\" # \"tf2\" or \"torch\"\n",
    "        self.stop_iters = 5\n",
    "        self.stop_timesteps = 100000\n",
    "        self.stop_reward = 0.1\n",
    "        self.as_test = False\n",
    "\n",
    "args = Args()\n",
    "print(\"num GPUs:\", os.environ.get(\"RLLIB_NUM_GPUS\", \"0\"))\n",
    "ray.init()\n",
    "env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .rollouts(rollout_fragment_length=\"auto\", num_rollout_workers=2)\n",
    "    .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(args.framework)\n",
    "    .training(num_sgd_iter=10, sgd_minibatch_size=256, train_batch_size=4000)\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"]\n",
    "    )\n",
    "    .rl_module(_enable_rl_module_api=True)\n",
    "    .training(_enable_learner_api=True)\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    ")\n",
    "\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    args.run,\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(stop=stop, verbose=3),\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "if args.as_test:\n",
    "    print(\"Checking if learning goals were achieved\")\n",
    "    check_learning_achieved(results, args.stop_reward)\n",
    "ray.shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7f85c-c1f8-411f-9550-0338f3b1765f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_env",
   "language": "python",
   "name": "collective_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
