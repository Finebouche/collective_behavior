{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb21529-5a8b-4411-9693-b69a90f8ba0d",
   "metadata": {},
   "source": [
    "# Example from RLLib"
   ]
  },
  {
   "cell_type": "raw",
   "id": "099bcca9-9d01-447f-b0c4-7ecb54da2dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T12:39:51.963927Z",
     "iopub.status.busy": "2023-08-08T12:39:51.963307Z",
     "iopub.status.idle": "2023-08-08T12:43:03.637428Z",
     "shell.execute_reply": "2023-08-08T12:43:03.635623Z",
     "shell.execute_reply.started": "2023-08-08T12:39:51.963863Z"
    }
   },
   "source": [
    "\"\"\"Simple example of setting up a multi-agent policy mapping.\n",
    "\n",
    "Control the number of agents and policies via --num-agents and --num-policies.\n",
    "\n",
    "This works with hundreds of agents and policies, but note that initializing\n",
    "many TF policies will take some time.\n",
    "\n",
    "Also, TF evals might slow down with large numbers of policies. To debug TF\n",
    "execution, set the TF_TIMELINE_DIR environment variable.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray import tune, air\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.num_agents = 4\n",
    "            self.num_policies = 2\n",
    "            self.framework = \"torch\"\n",
    "            self.num_gpus = int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\"))\n",
    "            self.as_test = False\n",
    "            self.stop_iters = 20\n",
    "            self.stop_timesteps = 50000\n",
    "            self.stop_reward_per_agent = 150.0\n",
    "    \n",
    "    args = Args()\n",
    "    print(f\"Running with following options: {args.__dict__}\")\n",
    "\n",
    "    \n",
    "    ray.init()\n",
    "\n",
    "    # Each policy can have a different configuration (including custom model).\n",
    "    def gen_policy(i):\n",
    "        gammas = [0.95, 0.99]\n",
    "        # just change the gammas between the two policies.\n",
    "        # changing the module is not a critical part of this example.\n",
    "        # the important part is that the policies are different.\n",
    "        config = {\n",
    "            \"gamma\": gammas[i % len(gammas)],\n",
    "        }\n",
    "\n",
    "        return PolicySpec(config=config)\n",
    "\n",
    "    # Setup PPO with an ensemble of `num_policies` different policies.\n",
    "    policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(args.num_policies)}\n",
    "    policy_ids = list(policies.keys())\n",
    "\n",
    "    def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "        pol_id = random.choice(policy_ids)\n",
    "        return pol_id\n",
    "\n",
    "    scaling_config = {\n",
    "        \"num_learner_workers\": args.num_gpus,\n",
    "        \"num_gpus_per_learner_worker\": int(args.num_gpus > 0),\n",
    "    }\n",
    "\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .rollouts(rollout_fragment_length=\"auto\", num_rollout_workers=3)\n",
    "        .environment(MultiAgentCartPole, env_config={\"num_agents\": args.num_agents})\n",
    "        .framework(args.framework)\n",
    "        .training(num_sgd_iter=10, sgd_minibatch_size=2**9, train_batch_size=2**12)\n",
    "        .multi_agent(policies=policies, policy_mapping_fn=policy_mapping_fn)\n",
    "        .rl_module(_enable_rl_module_api=True)\n",
    "        .training(_enable_learner_api=True)\n",
    "        .resources(**scaling_config)\n",
    "    )\n",
    "\n",
    "    stop_reward = args.stop_reward_per_agent * args.num_agents\n",
    "    stop = {\n",
    "        \"episode_reward_mean\": stop_reward,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "    }\n",
    "\n",
    "    results = tune.Tuner(\n",
    "        \"PPO\",\n",
    "        param_space=config.to_dict(),\n",
    "        run_config=air.RunConfig(stop=stop, verbose=3),\n",
    "    ).fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        check_learning_achieved(results, stop_reward)\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6dda5f-a9b9-4d09-8891-5672135f526a",
   "metadata": {},
   "source": [
    "# Our Environmeent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.run = \"PPO\"\n",
    "        self.framework = \"torch\" # \"tf2\" or \"torch\"\n",
    "        self.stop_iters = 50\n",
    "        self.stop_timesteps = 100000\n",
    "        self.stop_reward = 0.1\n",
    "        self.as_test = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "ray.init()\n",
    "env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .rollouts(rollout_fragment_length=\"auto\", num_rollout_workers=0)\n",
    "    .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(args.framework)\n",
    "    .training(num_sgd_iter=10, sgd_minibatch_size=256, train_batch_size=4000)\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"]\n",
    "    )\n",
    "    .rl_module(_enable_rl_module_api=True)\n",
    "    .training(_enable_learner_api=True)\n",
    "    .resources(num_gpus=0)\n",
    ")\n",
    "\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    args.run,\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(stop=stop, verbose=3),\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "if args.as_test:\n",
    "    print(\"Checking if learning goals were achieved\")\n",
    "    check_learning_achieved(results, args.stop_reward)\n",
    "ray.shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916ed4d-12f9-4d18-af43-c227ee249520",
   "metadata": {},
   "source": [
    "# A mockup minimal environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f36c39-bc7e-426d-8f39-8761fc1a1e0b",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from env import Environment\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "\n",
    "env = Environment({})\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .rollouts(rollout_fragment_length=\"auto\", num_rollout_workers=0)\n",
    "    .environment(Environment)\n",
    "    .framework(\"torch\")\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"0\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space,  # infer automatically from env\n",
    "                action_space=env.action_space,  # infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"1\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space,  # infer automatically from env\n",
    "                action_space=env.action_space,  # infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda agent_id, episode, worker: \"0\" if agent_id == \"0\" else \"1\",\n",
    "    )\n",
    "    .resources(num_gpus=0)\n",
    ")\n",
    "\n",
    "my_ma_algo = config.build()\n",
    "my_ma_algo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7f85c-c1f8-411f-9550-0338f3b1765f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_env",
   "language": "python",
   "name": "collective_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
