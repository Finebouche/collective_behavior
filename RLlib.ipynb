{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e0e5c-cee0-4040-9380-b39c9163e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "\n",
    "## The RLlib configuration\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.run = \"PPO\"\n",
    "        self.framework = \"torch\" # \"tf2\" or \"torch\"\n",
    "        self.stop_iters = 5\n",
    "        self.stop_timesteps = 20000\n",
    "        self.stop_reward = 0.1\n",
    "        self.as_test = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "## Generate the configuration\n",
    "ray.init()\n",
    "env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(args.framework)\n",
    "    .training(num_sgd_iter=10, sgd_minibatch_size=256, train_batch_size=4000)\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"]\n",
    "    )\n",
    "    .rl_module(_enable_rl_module_api=True)\n",
    "    .training(_enable_learner_api=True)\n",
    "    .rollouts(\n",
    "        rollout_fragment_length= 200,\n",
    "        batch_mode= 'truncate_episodes',\n",
    "        num_rollout_workers=3\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus = ray.cluster_resources().get('GPU', 0),\n",
    "        num_gpus_per_worker=0,\n",
    "        num_cpus_per_worker=3,\n",
    "        # learner workers\n",
    "        num_learner_workers=3,\n",
    "        num_gpus_per_learner_worker=0,\n",
    "        num_cpus_per_learner_worker=3,\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056eb565-f91d-4c3c-bced-93a56886dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the experiemnt    \n",
    "tuner = tune.Tuner(\n",
    "    args.run,\n",
    "    param_space=config.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        stop={\n",
    "            \"training_iteration\": args.stop_iters,\n",
    "            \"timesteps_total\": args.stop_timesteps,\n",
    "            \"episode_reward_mean\": args.stop_reward,\n",
    "        },\n",
    "        verbose=3,\n",
    "        callbacks=[WandbLoggerCallback(\n",
    "            project=\"marl-rllib\", \n",
    "            api_key=\"90dc2cefddde123eaac0caae90161981ed969abe\"\n",
    "        )],\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "if args.as_test:\n",
    "    print(\"Checking if learning goals were achieved\")\n",
    "    check_learning_achieved(results, args.stop_reward)\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edfb36-fe9d-47f1-a07d-cbaa2b709b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = results.get_best_result().checkpoint\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801af95-5ed3-4128-9c15-44f465cae96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "ray.init()\n",
    "\n",
    "policy_0_checkpoint = os.path.join(\n",
    "    best_checkpoint.to_directory(), \"policies/policy_0\"\n",
    ")\n",
    "restored_policy_0 = Policy.from_checkpoint(policy_0_checkpoint)\n",
    "restored_policy_0_weights = restored_policy_0.get_weights()\n",
    "print(\"Starting new tune.Tuner().fit()\")\n",
    "\n",
    "# Start our actual experiment.\n",
    "stop = {\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "}\n",
    "\n",
    "class RestoreWeightsCallback(DefaultCallbacks):\n",
    "    def on_algorithm_init(self, *, algorithm: \"Algorithm\", **kwargs) -> None:\n",
    "        algorithm.set_weights({\"policy_0\": restored_policy_0_weights})\n",
    "\n",
    "# Make sure, the non-1st policies are not updated anymore.\n",
    "config.policies_to_train = [pid for pid in policy_ids if pid != \"policy_0\"]\n",
    "config.callbacks(RestoreWeightsCallback)\n",
    "\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    stop=stop,\n",
    "    config=config.to_dict(),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "if args.as_test:\n",
    "    check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f440ff6-9d75-4466-8b96-19421a01be6f",
   "metadata": {},
   "source": [
    "1GPU V100  2CPU\n",
    "426 for 20000 with         num_gpus_per_learner_worker=1, num_learner_workers=1\n",
    "\n",
    "2CPU\n",
    "378.389 for 20000 without\n",
    "\n",
    "4GPU, 2CPU\n",
    "295.219 for 20000 with \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
