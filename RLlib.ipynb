{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b7cc4-c1da-49fb-85a5-29eee064dd0d",
   "metadata": {},
   "source": [
    "# Some preliminary checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14976ee1-cfb2-4052-a40b-0f316a54d317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T12:05:09.554928Z",
     "iopub.status.busy": "2023-10-09T12:05:09.554616Z",
     "iopub.status.idle": "2023-10-09T12:05:12.080636Z",
     "shell.execute_reply": "2023-10-09T12:05:12.080352Z",
     "shell.execute_reply.started": "2023-10-09T12:05:09.554898Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.1.0\n",
      "CUDA Available: False\n",
      "CUDA Version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9367a277-f21e-4ccb-9fc1-c5c1c1dd292b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T12:05:12.081236Z",
     "iopub.status.busy": "2023-10-09T12:05:12.081094Z",
     "iopub.status.idle": "2023-10-09T12:05:12.082936Z",
     "shell.execute_reply": "2023-10-09T12:05:12.082676Z",
     "shell.execute_reply.started": "2023-10-09T12:05:12.081229Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs:  12\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# print number of gpus / CPUs\n",
    "print(\"Number of CPUs: \", psutil.cpu_count())\n",
    "\n",
    "num_cpus = 12\n",
    "num_gpus = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c3bb7-053c-498c-a15a-8d42031fb8d0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T12:05:12.083315Z",
     "iopub.status.busy": "2023-10-09T12:05:12.083256Z",
     "iopub.status.idle": "2023-10-09T12:05:12.714946Z",
     "shell.execute_reply": "2023-10-09T12:05:12.714659Z",
     "shell.execute_reply.started": "2023-10-09T12:05:12.083309Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 14:05:12,245\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-10-09 14:05:12,296\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-10-09 14:05:12,362\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "2023-10-09 14:05:12,682\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-10-09 14:05:12,684\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "2023-10-09 14:05:12,695\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-10-09 14:05:12,708\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-09 14:05:12,713\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "## The RLlib configuration\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.run = \"PPO\"\n",
    "        self.framework = \"torch\" # \"tf2\" or \"torch\"\n",
    "args = Args()\n",
    "\n",
    "## Generate the configuration\n",
    "env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "config = (\n",
    "    get_trainable_cls(args.run)\n",
    "    .get_default_config()\n",
    "    .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(args.framework)\n",
    "    .training(_enable_learner_api=True, num_sgd_iter=10, sgd_minibatch_size=256, train_batch_size=20000)\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"]\n",
    "    )\n",
    "    .rl_module(_enable_rl_module_api=True)\n",
    "    .rollouts(\n",
    "        rollout_fragment_length=\"auto\",\n",
    "        batch_mode= 'truncate_episodes',\n",
    "        num_rollout_workers=num_cpus-1,\n",
    "        num_envs_per_worker=1,\n",
    "        #create_env_on_local_worker=False,\n",
    "    )\n",
    "    # This as to be specified everytime (don't know how to automatically ajust)\n",
    "    .resources(\n",
    "        #num_gpus = num_gpus,\n",
    "        #num_gpus_per_worker=0,\n",
    "        #num_cpus_per_worker=2,\n",
    "        # learner workers\n",
    "        #num_learner_workers=num_gpus,\n",
    "        #num_gpus_per_learner_worker=1,\n",
    "        #num_cpus_per_learner_worker=0,\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cb86d-9cfa-44ef-baa9-935ae4103759",
   "metadata": {},
   "source": [
    "## To load a previously trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1bece33-1f8a-4afa-bdf4-bd2b6f5eb934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T12:05:12.716203Z",
     "iopub.status.busy": "2023-10-09T12:05:12.716033Z",
     "iopub.status.idle": "2023-10-09T12:05:12.718666Z",
     "shell.execute_reply": "2023-10-09T12:05:12.718443Z",
     "shell.execute_reply.started": "2023-10-09T12:05:12.716193Z"
    }
   },
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "path_to_checkpoint = None\n",
    "def restore_weights(path_to_checkpoint, policy_type):\n",
    "    checkpoint_path = os.path.join(path_to_checkpoint, f\"policies/{policy_type}\")\n",
    "    restored_policy = Policy.from_checkpoint(checkpoint_path)\n",
    "    return restored_policy.get_weights()\n",
    "\n",
    "if path_to_checkpoint is not None: \n",
    "    class RestoreWeightsCallback(DefaultCallbacks):\n",
    "        def __init__(self):\n",
    "            self.restored_policy_predator_weights = restore_weights(path_to_checkpoint, \"predator\")\n",
    "            self.restored_policy_prey_weights = restore_weights(path_to_checkpoint,\"prey\")\n",
    "    \n",
    "        def on_algorithm_init(self, *, algorithm: \"Algorithm\", **kwargs) -> None:\n",
    "            algorithm.set_weights({\"predator\": self.restored_policy_predator_weights})\n",
    "            algorithm.set_weights({\"prey\": self.restored_policy_prey_weights})\n",
    "\n",
    "    config.callbacks(RestoreWeightsCallback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056eb565-f91d-4c3c-bced-93a56886dca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-09T12:05:12.719076Z",
     "iopub.status.busy": "2023-10-09T12:05:12.719011Z",
     "iopub.status.idle": "2023-10-09T12:05:18.344954Z",
     "shell.execute_reply": "2023-10-09T12:05:18.344419Z",
     "shell.execute_reply.started": "2023-10-09T12:05:12.719069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 14:05:14,368\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "2023-10-09 14:05:14,858\tINFO tune.py:645 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num CPUS rays sees : 12.0\n",
      "num GPUS rays sees : 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-10-09 14:05:14</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:00.12        </td></tr>\n",
       "<tr><td>Memory:      </td><td>22.5/64.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 12.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnvironment_1afa3_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 14:05:14,870\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=66283)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m 2023-10-09 14:05:17,751\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m 2023-10-09 14:05:17,751\tWARNING algorithm_config.py:672 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "2023-10-09 14:05:17,836\tERROR tune_controller.py:1502 -- Trial task failed for trial PPO_CustomEnvironment_1afa3_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/_private/worker.py\", line 2549, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=66283, ip=127.0.0.1, actor_id=d926aaa3e2665bcd33e76b5501000000, repr=PPO)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 185, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 575, in setup\n",
      "    self.callbacks = self.config.callbacks_class()\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/c9/p_q1xbbj7sv7nwvd_17c75lh0000gn/T/ipykernel_66255/4251594233.py\", line 13, in __init__\n",
      "  File \"/var/folders/c9/p_q1xbbj7sv7nwvd_17c75lh0000gn/T/ipykernel_66255/4251594233.py\", line 7, in restore_weights\n",
      "  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/policy/policy.py\", line 297, in from_checkpoint\n",
      "    checkpoint_info = get_checkpoint_info(checkpoint)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/checkpoints.py\", line 171, in get_checkpoint_info\n",
      "    raise ValueError(\n",
      "ValueError: Given checkpoint (/Users/tanguy/ray_results/PPO_2023-10-09_11-58-40/PPO_CustomEnvironment_6c9d5_00000_0_2023-10-09_11-58-40/policies/predator) not found! Must be a checkpoint directory (or a file for older checkpoint versions).\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=66283, ip=127.0.0.1, actor_id=d926aaa3e2665bcd33e76b5501000000, repr=PPO)\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m   File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m     super().__init__(\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m   File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 185, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m   File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 575, in setup\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m     self.callbacks = self.config.callbacks_class()\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m   File \"/var/folders/c9/p_q1xbbj7sv7nwvd_17c75lh0000gn/T/ipykernel_66255/4251594233.py\", line 13, in __init__\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m   File \"/var/folders/c9/p_q1xbbj7sv7nwvd_17c75lh0000gn/T/ipykernel_66255/4251594233.py\", line 7, in restore_weights\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m   File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/policy/policy.py\", line 297, in from_checkpoint\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m     checkpoint_info = get_checkpoint_info(checkpoint)\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m   File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/checkpoints.py\", line 171, in get_checkpoint_info\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(PPO pid=66283)\u001b[0m ValueError: Given checkpoint (/Users/tanguy/ray_results/PPO_2023-10-09_11-58-40/PPO_CustomEnvironment_6c9d5_00000_0_2023-10-09_11-58-40/policies/predator) not found! Must be a checkpoint directory (or a file for older checkpoint versions).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "PPO_CustomEnvironment_1afa3_00000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py:110\u001b[0m, in \u001b[0;36mRayEventManager.resolve_future\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/_private/auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m auto_init_ray()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/_private/worker.py:2549\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2548\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2549\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=66283, ip=127.0.0.1, actor_id=d926aaa3e2665bcd33e76b5501000000, repr=PPO)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n    super().__init__(\n  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/trainable/trainable.py\", line 185, in __init__\n    self.setup(copy.deepcopy(self.config))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py\", line 575, in setup\n    self.callbacks = self.config.callbacks_class()\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/c9/p_q1xbbj7sv7nwvd_17c75lh0000gn/T/ipykernel_66255/4251594233.py\", line 13, in __init__\n  File \"/var/folders/c9/p_q1xbbj7sv7nwvd_17c75lh0000gn/T/ipykernel_66255/4251594233.py\", line 7, in restore_weights\n  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/policy/policy.py\", line 297, in from_checkpoint\n    checkpoint_info = get_checkpoint_info(checkpoint)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/rllib/utils/checkpoints.py\", line 171, in get_checkpoint_info\n    raise ValueError(\nValueError: Given checkpoint (/Users/tanguy/ray_results/PPO_2023-10-09_11-58-40/PPO_CustomEnvironment_6c9d5_00000_0_2023-10-09_11-58-40/policies/predator) not found! Must be a checkpoint directory (or a file for older checkpoint versions).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m## Run the experiemnt    \u001b[39;00m\n\u001b[1;32m     19\u001b[0m tuner \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mTuner(\n\u001b[1;32m     20\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun,\n\u001b[1;32m     21\u001b[0m     param_space\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     ),\n\u001b[1;32m     40\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opti_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking if learning goals were achieved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/tuner.py:372\u001b[0m, in \u001b[0;36mTuner.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_ray_client:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TuneError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\n\u001b[1;32m    375\u001b[0m             _TUNER_FAILED_MSG\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    376\u001b[0m                 path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_tuner\u001b[38;5;241m.\u001b[39mget_experiment_checkpoint_dir()\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:579\u001b[0m, in \u001b[0;36mTunerInternal.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m param_space \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_space)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_restored:\n\u001b[0;32m--> 579\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resume(trainable, param_space)\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py:699\u001b[0m, in \u001b[0;36mTunerInternal._fit_internal\u001b[0;34m(self, trainable, param_space)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001b[39;00m\n\u001b[1;32m    687\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tune_run_arguments(trainable),\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuner_kwargs,\n\u001b[1;32m    698\u001b[0m }\n\u001b[0;32m--> 699\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_remote_string_queue()\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/tune.py:1103\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mis_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set()\n\u001b[1;32m   1102\u001b[0m     ):\n\u001b[0;32m-> 1103\u001b[0m         \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity\u001b[38;5;241m.\u001b[39mV1_EXPERIMENT):\n\u001b[1;32m   1105\u001b[0m             _report_progress(runner, progress_reporter)\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:850\u001b[0m, in \u001b[0;36mTuneController.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_add_actors()\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# Handle one event\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;66;03m# If there are no actors running, warn about potentially\u001b[39;00m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# insufficient resources\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_manager\u001b[38;5;241m.\u001b[39mnum_live_actors:\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_resources_manager\u001b[38;5;241m.\u001b[39mon_no_available_trials(\n\u001b[1;32m    855\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_trials()\n\u001b[1;32m    856\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/air/execution/_internal/actor_manager.py:222\u001b[0m, in \u001b[0;36mRayActorManager.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m [future] \u001b[38;5;241m=\u001b[39m ready\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m actor_state_futures:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_state_events\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve_future\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m actor_task_futures:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_task_events\u001b[38;5;241m.\u001b[39mresolve_future(future)\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py:113\u001b[0m, in \u001b[0;36mRayEventManager.resolve_future\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_error:\n\u001b[0;32m--> 113\u001b[0m         \u001b[43mon_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/air/execution/_internal/actor_manager.py:386\u001b[0m, in \u001b[0;36mRayActorManager._try_start_actors.<locals>.create_callbacks.<locals>.on_error\u001b[0;34m(exception)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_error\u001b[39m(exception: \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_start_failed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracked_actor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracked_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexception\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/air/execution/_internal/actor_manager.py:259\u001b[0m, in \u001b[0;36mRayActorManager._actor_start_failed\u001b[0;34m(self, tracked_actor, exception)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup_actor(tracked_actor\u001b[38;5;241m=\u001b[39mtracked_actor)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracked_actor\u001b[38;5;241m.\u001b[39m_on_error:\n\u001b[0;32m--> 259\u001b[0m     \u001b[43mtracked_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracked_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:1343\u001b[0m, in \u001b[0;36mTuneController._actor_failed\u001b[0;34m(self, tracked_actor, exception)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1338\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed in its creation task. Unstaging \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1339\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto allow it to be re-scheduled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1340\u001b[0m     )\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unstage_trial_with_resources(trial)\n\u001b[0;32m-> 1343\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trial_task_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_manager\u001b[38;5;241m.\u001b[39mclear_actor_task_futures(tracked_actor)\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;66;03m# Clean up actor\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:1503\u001b[0m, in \u001b[0;36mTuneController._trial_task_failure\u001b[0;34m(self, trial, exception)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_trial_errors:\n\u001b[1;32m   1502\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial task failed for trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39mexception)\n\u001b[0;32m-> 1503\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_trial_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/execution/tune_controller.py:1526\u001b[0m, in \u001b[0;36mTuneController._process_trial_failure\u001b[0;34m(self, trial, exception)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search_alg\u001b[38;5;241m.\u001b[39mon_trial_complete(trial\u001b[38;5;241m.\u001b[39mtrial_id, error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule_trial_stop(trial, exception\u001b[38;5;241m=\u001b[39mexception)\n\u001b[0;32m-> 1526\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/callback.py:424\u001b[0m, in \u001b[0;36mCallbackList.on_trial_error\u001b[0;34m(self, **info)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minfo):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callbacks:\n\u001b[0;32m--> 424\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_error\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/tune/logger/logger.py:168\u001b[0m, in \u001b[0;36mLoggerCallback.on_trial_error\u001b[0;34m(self, iteration, trials, trial, **info)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_error\u001b[39m(\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m, iteration: \u001b[38;5;28mint\u001b[39m, trials: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial\u001b[39m\u001b[38;5;124m\"\u001b[39m], trial: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minfo\n\u001b[1;32m    167\u001b[0m ):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfailed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/air/integrations/wandb.py:688\u001b[0m, in \u001b[0;36mWandbLoggerCallback.log_trial_end\u001b[0;34m(self, trial, failed)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial\u001b[39m\u001b[38;5;124m\"\u001b[39m, failed: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_signal_logging_actor_stop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup_logging_actors()\n",
      "File \u001b[0;32m~/miniforge3/envs/collective_env/lib/python3.11/site-packages/ray/air/integrations/wandb.py:666\u001b[0m, in \u001b[0;36mWandbLoggerCallback._signal_logging_actor_stop\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_signal_logging_actor_stop\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trial_queues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mput((_QueueItem\u001b[38;5;241m.\u001b[39mEND, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[0;31mKeyError\u001b[0m: PPO_CustomEnvironment_1afa3_00000"
     ]
    }
   ],
   "source": [
    "import ray \n",
    "from ray import train, tune\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "ray.init(num_cpus=num_cpus, num_gpus=num_gpus)\n",
    "\n",
    "print(\"num CPUS rays sees :\", ray.cluster_resources().get('CPU', 0))\n",
    "print(\"num GPUS rays sees :\", ray.cluster_resources().get('GPU', 0))\n",
    "\n",
    "opti_config = {\n",
    "    'stop_iters': 500,\n",
    "    'stop_timesteps': 2000000,\n",
    "    'stop_reward': 0.1,\n",
    "    'as_test': False\n",
    "}\n",
    "\n",
    "## Run the experiemnt    \n",
    "tuner = tune.Tuner(\n",
    "    args.run,\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\n",
    "            \"training_iteration\": opti_config[\"stop_iters\"],\n",
    "            \"timesteps_total\": opti_config[\"stop_timesteps\"],\n",
    "            \"episode_reward_mean\": opti_config[\"stop_reward\"],\n",
    "        },\n",
    "        verbose=3,\n",
    "        callbacks=[WandbLoggerCallback(\n",
    "            project=\"marl-rllib\", \n",
    "            group=\"PPO\",\n",
    "            api_key=\"90dc2cefddde123eaac0caae90161981ed969abe\",\n",
    "            log_config=True,\n",
    "        )],\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "            checkpoint_frequency=10\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "if opti_config[\"as_test\"]:\n",
    "    print(\"Checking if learning goals were achieved\")\n",
    "    check_learning_achieved(results, opti_config[\"stop_reward\"])\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53c262-da4a-41ca-bc49-b97821f17639",
   "metadata": {},
   "source": [
    "# Render episode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67626d-946b-45ea-b59d-c6ed65ab1772",
   "metadata": {},
   "source": [
    "### Retrieve checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edfb36-fe9d-47f1-a07d-cbaa2b709b29",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-09T12:05:18.345333Z",
     "iopub.status.idle": "2023-10-09T12:05:18.345442Z",
     "shell.execute_reply": "2023-10-09T12:05:18.345389Z",
     "shell.execute_reply.started": "2023-10-09T12:05:18.345384Z"
    }
   },
   "outputs": [],
   "source": [
    "best_checkpoint = results.get_best_result().checkpoint\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96925a71-e8c6-4d9b-97b5-593a15214c2f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-09T12:05:18.345769Z",
     "iopub.status.idle": "2023-10-09T12:05:18.345863Z",
     "shell.execute_reply": "2023-10-09T12:05:18.345816Z",
     "shell.execute_reply.started": "2023-10-09T12:05:18.345810Z"
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "path_to_checkpoint = \"/Users/tanguy/ray_results/PPO_2023-10-09_11-58-40/PPO_CustomEnvironment_6c9d5_00000_0_2023-10-09_11-58-40/checkpoint_000009\"\n",
    "\n",
    "ray.init()\n",
    "algo = Algorithm.from_checkpoint(path_to_checkpoint)\n",
    "\n",
    "# After loading the algorithm\n",
    "local_worker = algo.workers.local_worker()\n",
    "available_policy_ids = list(local_worker.policy_map.keys())\n",
    "print(\"Available Policy IDs:\", available_policy_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18e7bb-3a95-4978-82ce-03d4c8bc9b6c",
   "metadata": {},
   "source": [
    "### Run and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8874e06-f306-4ab1-be42-084d479af642",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-09T12:05:18.346318Z",
     "iopub.status.idle": "2023-10-09T12:05:18.346554Z",
     "shell.execute_reply": "2023-10-09T12:05:18.346466Z",
     "shell.execute_reply.started": "2023-10-09T12:05:18.346458Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_observations(observation, agent_ids, truncation=None):\n",
    "    loc_x = [observation[key][2] if key in observation else 0 for key in agent_ids]\n",
    "    loc_y = [observation[key][3] if key in observation else 0 for key in agent_ids]\n",
    "    heading = [observation[key][4] if key in observation else 0 for key in agent_ids]\n",
    "    if truncation:\n",
    "        still_in_the_game = [1 if not truncation[key] else 0 for key in agent_ids]\n",
    "    else:\n",
    "        still_in_the_game = [1 for _ in agent_ids]\n",
    "    observations[\"loc_x\"].append(np.array(loc_x))\n",
    "    observations[\"loc_y\"].append(np.array(loc_y))\n",
    "    observations[\"heading\"].append(np.array(loc_y))\n",
    "    observations[\"still_in_the_game\"].append(np.array(still_in_the_game))\n",
    "    \n",
    "    return observations\n",
    "\n",
    "# Use the first available policy ID\n",
    "policy_id = available_policy_ids[0]\n",
    "\n",
    "observations = {\"loc_x\": [], \"loc_y\": [], \"heading\": [], \"still_in_the_game\": []}\n",
    "\n",
    "observation, _ = env.reset()\n",
    "agent_ids = env._agent_ids\n",
    "loc_x, loc_y, heading, still_in_the_game = process_observations(observation, agent_ids)\n",
    "step_count = 1\n",
    "\n",
    "\n",
    "while step_count < 500:\n",
    "    actions = {\n",
    "        key: algo.compute_single_action(\n",
    "            value, policy_id=\"prey\" if env.agents[key].agent_type == 0 else \"predator\"\n",
    "        ) for key, value in observation.items()\n",
    "    }\n",
    "    \n",
    "    observation, _, termination, truncation, _ = env.step(actions)\n",
    "    \n",
    "    observations = process_observations(observation, agent_ids, truncation)\n",
    "    \n",
    "    step_count += 1\n",
    "\n",
    "stage_size = env.stage_size\n",
    "observations[\"loc_x\"] = np.array(observations[\"loc_x\"]) * stage_size\n",
    "observations[\"loc_y\"] = np.array(observations[\"loc_y\"]) * stage_size\n",
    "observations[\"still_in_the_game\"] = np.array(observations[\"still_in_the_game\"])\n",
    "\n",
    "env.close()\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe696df-26b6-4a1b-8692-8965b6798da0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-09T12:05:18.347052Z",
     "iopub.status.idle": "2023-10-09T12:05:18.347166Z",
     "shell.execute_reply": "2023-10-09T12:05:18.347111Z",
     "shell.execute_reply.started": "2023-10-09T12:05:18.347106Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import animation\n",
    "importlib.reload(animation)\n",
    "\n",
    "from animation import generate_animation\n",
    "\n",
    "ani = generate_animation(observations, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695eb08-dd06-49f2-8cb0-c7285ca37570",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-09T12:05:18.347839Z",
     "iopub.status.idle": "2023-10-09T12:05:18.347936Z",
     "shell.execute_reply": "2023-10-09T12:05:18.347886Z",
     "shell.execute_reply.started": "2023-10-09T12:05:18.347880Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de9596-3117-4b09-ba52-c7e05c8acf7a",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801af95-5ed3-4128-9c15-44f465cae96d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-10-09T12:05:18.348216Z",
     "iopub.status.idle": "2023-10-09T12:05:18.348305Z",
     "shell.execute_reply": "2023-10-09T12:05:18.348260Z",
     "shell.execute_reply.started": "2023-10-09T12:05:18.348256Z"
    }
   },
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "def restore_policy_and_weights(policy_type):\n",
    "    checkpoint_path = os.path.join(best_checkpoint.to_directory(), f\"policies/{policy_type}\")\n",
    "    restored_policy = Policy.from_checkpoint(checkpoint_path)\n",
    "    return restored_policy.get_weights()\n",
    "\n",
    "restored_policy_predator_weights = restore_policy_and_weights(\"predator\")\n",
    "restored_policy_prey_weights = restore_policy_and_weights(\"prey\")\n",
    "\n",
    "print(\"Starting new tune.Tuner().fit()\")\n",
    "\n",
    "ray.init()\n",
    "\n",
    "# Start our actual experiment.\n",
    "stop = {\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "}\n",
    "\n",
    "class RestoreWeightsCallback(DefaultCallbacks):\n",
    "    def on_algorithm_init(self, *, algorithm: \"Algorithm\", **kwargs) -> None:\n",
    "        algorithm.set_weights({\"predator\": restored_policy_predator_weights})\n",
    "        algorithm.set_weights({\"prey\": restored_policy_prey_weights})\n",
    "\n",
    "config.callbacks(RestoreWeightsCallback)\n",
    "\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    stop=stop,\n",
    "    config=config.to_dict(),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "if args.as_test:\n",
    "    check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dc324-7316-4f4d-945d-55a81d1bdb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2f440ff6-9d75-4466-8b96-19421a01be6f",
   "metadata": {},
   "source": [
    "1GPU V100  2CPU\n",
    "426 for 20000 with         num_gpus_per_learner_worker=1, num_learner_workers=1\n",
    "\n",
    "2CPU\n",
    "378.389 for 20000 without\n",
    "\n",
    "4GPU, 2CPU\n",
    "295.219 for 20000 with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85972d-e671-410b-a555-73e52fde2c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cd687-cfeb-4de4-b479-afc3daa27d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_env",
   "language": "python",
   "name": "collective_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
