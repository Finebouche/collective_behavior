{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b7cc4-c1da-49fb-85a5-29eee064dd0d",
   "metadata": {},
   "source": [
    "# Some preliminary checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14976ee1-cfb2-4052-a40b-0f316a54d317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T12:35:54.423550Z",
     "start_time": "2023-11-21T12:35:50.106622Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367a277-f21e-4ccb-9fc1-c5c1c1dd292b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T12:36:06.734680Z",
     "start_time": "2023-11-21T12:36:06.731185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# print number of gpus / CPUs\n",
    "print(\"Number of CPUs: \", psutil.cpu_count())\n",
    "\n",
    "\n",
    "num_cpus = 21\n",
    "num_gpus = 8\n",
    "num_learner_workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c3bb7-053c-498c-a15a-8d42031fb8d0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421e6f2-e64c-4f7c-8ef2-cd969f9888fe",
   "metadata": {},
   "source": [
    "## Environement and algorithm configuration\n",
    "\n",
    "Some of the commented lines are preparation work to use a futur feature of RLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T12:36:11.519772Z",
     "start_time": "2023-11-21T12:36:10.567325Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec\n",
    "#from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\n",
    "#from ray.rllib.core.rl_module.marl_module import MultiAgentRLModuleSpec\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "# Algorithm used and framework\n",
    "ALGO = \"PPO\"        \n",
    "FRAMEWORK= \"torch\" # \"tf2\" or \"torch\"\n",
    "\n",
    "# Generate the env with the configuration\n",
    "env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "# Generate the algorithm config for the tuner\n",
    "config = (\n",
    "    get_trainable_cls(ALGO)\n",
    "    .get_default_config()\n",
    "    .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(FRAMEWORK)\n",
    "    .training(\n",
    "        _enable_learner_api=False, \n",
    "        num_sgd_iter=5, \n",
    "        sgd_minibatch_size=256,             # the batch size\n",
    "        train_batch_size=32768,             # the number of step collected\n",
    "        model={\"fcnet_hiddens\": [64, 64, 64]},\n",
    "        #lr=tune.grid_search([0.01, 0.001, 0.0001])\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"]\n",
    "    )\n",
    "    #rl_module_api\n",
    "    .rl_module(\n",
    "        _enable_rl_module_api=True,\n",
    "#        rl_module_spec=MultiAgentRLModuleSpec(\n",
    "#            module_specs={\n",
    "#                \"prey\": SingleAgentRLModuleSpec(\n",
    "#                    module_class=PPOTorchRLModule,\n",
    "#                    observation_space=env.observation_space,\n",
    "#                    action_space=env.action_space,\n",
    "#                    model_config_dict={\"fcnet_hiddens\": [64, 64, 64]},\n",
    "#                    catalog_class=PPOCatalog\n",
    "#                ),\n",
    "#                \"predator\": SingleAgentRLModuleSpec(\n",
    "#                    module_class=PPOTorchRLModule,\n",
    "#                    observation_space=env.observation_space,\n",
    "#                    action_space=env.action_space,\n",
    "#                    model_config_dict={\"fcnet_hiddens\": [64, 64, 64]},\n",
    "#                    catalog_class=PPOCatalog\n",
    "#                ),\n",
    "#            }\n",
    "#        ),\n",
    "    )\n",
    "    .rollouts(\n",
    "        rollout_fragment_length=\"auto\", # explained here : https://docs.ray.io/en/latest/rllib/rllib-sample-collection.html\n",
    "        batch_mode= 'truncate_episodes',\n",
    "        num_rollout_workers=num_cpus-1,\n",
    "        num_envs_per_worker=1,\n",
    "        #create_env_on_local_worker=False,\n",
    "    )\n",
    "    # learner_api\n",
    "    .training(\n",
    "        _enable_learner_api=True, \n",
    "    )\n",
    "    .resources(\n",
    "        #num_gpus = num_gpus,\n",
    "        #num_gpus_per_worker=0,\n",
    "        #num_cpus_per_worker=2,\n",
    "        # learner workers when using learner api - doesn't work on arm (mac) yet\n",
    "        num_learner_workers=num_gpus,\n",
    "        num_gpus_per_learner_worker=1, # always 1 for PPO\n",
    "        #num_cpus_per_learner_worker=1,\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880cb86d-9cfa-44ef-baa9-935ae4103759",
   "metadata": {},
   "source": [
    "## To load a previously trained policy\n",
    "\n",
    "Possibility to load a previously saved checkpoint and start the training from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bece33-1f8a-4afa-bdf4-bd2b6f5eb934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T12:36:15.670453Z",
     "start_time": "2023-11-21T12:36:15.668163Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "path_to_checkpoint = None\n",
    "def restore_weights(path_to_checkpoint, policy_type):\n",
    "    checkpoint_path = os.path.join(path_to_checkpoint, f\"policies/{policy_type}\")\n",
    "    restored_policy = Policy.from_checkpoint(checkpoint_path)\n",
    "    return restored_policy.get_weights()\n",
    "\n",
    "if path_to_checkpoint is not None: \n",
    "    class RestoreWeightsCallback(DefaultCallbacks):\n",
    "        def __init__(self):\n",
    "            self.restored_policy_predator_weights = restore_weights(path_to_checkpoint, \"predator\")\n",
    "            self.restored_policy_prey_weights = restore_weights(path_to_checkpoint,\"prey\")\n",
    "    \n",
    "        def on_algorithm_init(self, *, algorithm: \"Algorithm\", **kwargs) -> None:\n",
    "            algorithm.set_weights({\"predator\": self.restored_policy_predator_weights})\n",
    "            algorithm.set_weights({\"prey\": self.restored_policy_prey_weights})\n",
    "\n",
    "    config.callbacks(RestoreWeightsCallback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5385022-05b2-42ae-a406-87b46736f257",
   "metadata": {},
   "source": [
    "## Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056eb565-f91d-4c3c-bced-93a56886dca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T12:44:08.386063Z",
     "start_time": "2023-11-21T12:36:18.583201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray \n",
    "from ray import train, tune\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=num_cpus, \n",
    "    num_gpus=num_gpus\n",
    ")\n",
    "\n",
    "print(\"num CPUS rays sees :\", ray.cluster_resources().get('CPU', 0))\n",
    "print(\"num GPUS rays sees :\", ray.cluster_resources().get('GPU', 0))\n",
    "\n",
    "# Define experiment    \n",
    "tuner = tune.Tuner(\n",
    "    ALGO,                                                 # Defined before\n",
    "    param_space=config,                                   # Defined before\n",
    "    run_config=train.RunConfig(                           ## RUN CONFIG ##\n",
    "        stop={                                            # Stop criterium\n",
    "            \"training_iteration\": 5000,\n",
    "            \"timesteps_total\": 200000,\n",
    "        },\n",
    "        verbose=3,\n",
    "        callbacks=[WandbLoggerCallback(                   # To use Wanddb\n",
    "            project=\"marl-rllib\", \n",
    "            group=\"PPO\",\n",
    "            api_key=\"90dc2cefddde123eaac0caae90161981ed969abe\",\n",
    "            log_config=True,\n",
    "        )],\n",
    "        checkpoint_config=train.CheckpointConfig(         # When to save the models \n",
    "            checkpoint_at_end=True,\n",
    "            checkpoint_frequency=10\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run the experiment \n",
    "results = tuner.fit()\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67626d-946b-45ea-b59d-c6ed65ab1772",
   "metadata": {},
   "source": [
    "### Retrieve checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edfb36-fe9d-47f1-a07d-cbaa2b709b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T13:04:45.381547Z",
     "start_time": "2023-11-21T13:04:45.367765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_checkpoint = results.get_best_result().checkpoint\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96925a71-e8c6-4d9b-97b5-593a15214c2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T13:04:54.323101Z",
     "start_time": "2023-11-21T13:04:45.898746Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "path_to_checkpoint = best_checkpoint\n",
    "\n",
    "# This does ray.init()\n",
    "algo = Algorithm.from_checkpoint(path_to_checkpoint)\n",
    "\n",
    "# After loading the algorithm\n",
    "available_policy_ids = list(algo.workers.local_worker().policy_map.keys())\n",
    "print(\"Available Policy IDs:\", available_policy_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18e7bb-3a95-4978-82ce-03d4c8bc9b6c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Episode animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8874e06-f306-4ab1-be42-084d479af642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T13:04:57.626324Z",
     "start_time": "2023-11-21T13:04:54.324426Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_observations(observation, agent_ids, truncation=None):\n",
    "    loc_x = [observation[key][2] if key in observation else 0 for key in agent_ids]\n",
    "    loc_y = [observation[key][3] if key in observation else 0 for key in agent_ids]\n",
    "    heading = [observation[key][4] if key in observation else 0 for key in agent_ids]\n",
    "    if truncation:\n",
    "        still_in_the_game = [1 if not truncation[key] else 0 for key in agent_ids]\n",
    "    else:\n",
    "        still_in_the_game = [1 for _ in agent_ids]\n",
    "    observations[\"loc_x\"].append(np.array(loc_x))\n",
    "    observations[\"loc_y\"].append(np.array(loc_y))\n",
    "    observations[\"heading\"].append(np.array(heading))\n",
    "    observations[\"still_in_the_game\"].append(np.array(still_in_the_game))\n",
    "    \n",
    "    return observations\n",
    "\n",
    "observations = {\"loc_x\": [], \"loc_y\": [], \"heading\": [], \"still_in_the_game\": []}\n",
    "\n",
    "observation, _ = env.reset()\n",
    "agent_ids = env._agent_ids\n",
    "loc_x, loc_y, heading, still_in_the_game = process_observations(observation, agent_ids)\n",
    "step_count = 1\n",
    "\n",
    "\n",
    "while step_count < 500:\n",
    "    actions = {\n",
    "        key: algo.compute_single_action(\n",
    "            value, policy_id=\"prey\" if env.agents[key].agent_type == 0 else \"predator\"\n",
    "        ) for key, value in observation.items()\n",
    "    }\n",
    "    \n",
    "    observation, _, termination, truncation, _ = env.step(actions)\n",
    "    \n",
    "    observations = process_observations(observation, agent_ids, truncation)\n",
    "    \n",
    "    step_count += 1\n",
    "\n",
    "stage_size = env.stage_size\n",
    "observations[\"loc_x\"] = np.array(observations[\"loc_x\"]) * stage_size\n",
    "observations[\"loc_y\"] = np.array(observations[\"loc_y\"]) * stage_size\n",
    "observations[\"heading\"] = np.array(observations[\"heading\"])\n",
    "observations[\"still_in_the_game\"] = np.array(observations[\"still_in_the_game\"])\n",
    "\n",
    "env.close()\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe696df-26b6-4a1b-8692-8965b6798da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T13:04:57.874333Z",
     "start_time": "2023-11-21T13:04:57.627413Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import animation\n",
    "importlib.reload(animation)\n",
    "\n",
    "from animation import generate_animation_3d\n",
    "\n",
    "ani = generate_animation_3d(observations, env, fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695eb08-dd06-49f2-8cb0-c7285ca37570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T13:05:02.088244Z",
     "start_time": "2023-11-21T13:04:57.875651Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ce39a-5d5f-4336-9f1e-22200180ac58",
   "metadata": {},
   "source": [
    "## Network visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6fc14d-fe4f-423c-950e-bdd8ca5a5846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-19T20:55:13.221803Z",
     "start_time": "2023-10-19T20:55:13.147387Z"
    }
   },
   "outputs": [],
   "source": [
    "algo.get_policy(available_policy_ids[0]).get_weights()['_hidden_layers.1._model.0.weight'].shape\n",
    "# from algo.get_policy(available_policy_ids[0]).get_weights() get the weights which have _value_branch in their key\n",
    "nn_weights = {}\n",
    "for key, value in algo.get_policy(available_policy_ids[0]).get_weights().items():\n",
    "    if \"_value_branch\" not in key:\n",
    "        nn_weights[key] = value\n",
    "\n",
    "nn_weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0770e41-04cd-4178-82d7-d9f98f5aba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from graph_tool.all import *\n",
    "\n",
    "def plot_mlp(neural_network):\n",
    "    g = Graph(directed=True)\n",
    "    \n",
    "    # Create property maps for vertex and edge labels and edge width\n",
    "    v_label = g.new_vertex_property(\"string\")\n",
    "    e_width = g.new_edge_property(\"double\")\n",
    "    pos = g.new_vertex_property(\"vector<double>\")\n",
    "    \n",
    "    max_neurons = max(len(neural_network[key]) for key in neural_network if 'weight' in key)\n",
    "    \n",
    "    ## VERTEX ##\n",
    "    # Add vertices for input layer and set their positions\n",
    "    input_neurons = [g.add_vertex() for _ in neural_network['_hidden_layers.0._model.0.weight'].T]\n",
    "    starting_y = (max_neurons - len(input_neurons)) / 2\n",
    "    for i, v in enumerate(input_neurons):\n",
    "        pos[v] = (0, starting_y + len(input_neurons) - 1 - i)\n",
    "    \n",
    "    # Get all the keys that contain \"_bias\"\n",
    "    biases_keys = [key for key in neural_network if \".bias\" in key and \"_hidden\" in key]\n",
    "    # Initial x position\n",
    "    x_position = 20\n",
    "    current_neurons = []\n",
    "    for i, biases_key in enumerate(biases_keys):\n",
    "        # Add vertices for the current layer and set their positions\n",
    "        current_neurons.append([g.add_vertex() for _ in neural_network[biases_key]])\n",
    "        starting_y = (max_neurons - len(current_neurons[i])) / 2\n",
    "        for j, v in enumerate(current_neurons[i]):\n",
    "            pos[v] = (x_position, starting_y + len(current_neurons[i]) - 1 - j)\n",
    "    \n",
    "        # Increment x position for the next layer\n",
    "        x_position += 20\n",
    "\n",
    "    # Add vertex for output layer and set its position\n",
    "    output_neurons = [g.add_vertex() for _ in neural_network['_logits._model.0.bias']]\n",
    "    starting_y = (max_neurons - len(output_neurons)) / 2\n",
    "    for i, v in enumerate(output_neurons):\n",
    "        pos[v] = (x_position, starting_y + len(output_neurons) - 1 - i)\n",
    "\n",
    "    ## LABELS ##\n",
    "    # Set labels and add edges for input-hidden layer\n",
    "    for i, input_neuron in enumerate(input_neurons):\n",
    "        for j, hidden_neuron in enumerate(current_neurons[0]):\n",
    "            e = g.add_edge(input_neuron, hidden_neuron)\n",
    "            weight = neural_network['_hidden_layers.0._model.0.weight'].T[i][j]\n",
    "            e_width[e] = weight\n",
    "            \n",
    "    # Set labels and add edges for hidden-hidden layer\n",
    "    weights_keys = [key for key in neural_network if \".weight\" in key and \"_hidden\" in key and not \"layers.0\" in key]\n",
    "    for k, weights_key in enumerate(weights_keys):\n",
    "        for i, hidden_neuron in enumerate(current_neurons[k]):\n",
    "            for j, next_hidden_neuron in enumerate(current_neurons[k+1]):\n",
    "                e = g.add_edge(hidden_neuron, next_hidden_neuron)\n",
    "                weight = neural_network[weights_key].T[i][j]\n",
    "                e_width[e] = weight\n",
    "\n",
    "    # Set labels and add edges for hidden-output layer\n",
    "    for j, output_neuron in enumerate(output_neurons):\n",
    "        for i, hidden_neuron in enumerate(current_neurons[-1]):\n",
    "            e = g.add_edge(hidden_neuron, output_neuron)\n",
    "            weight = neural_network['_logits._model.0.weight'].T[i][j]\n",
    "            e_width[e] = weight\n",
    "        \n",
    "    \n",
    "    # Set neuron labels (optional, for clarity)\n",
    "    for v in input_neurons:\n",
    "        v_label[v] = \"I\"\n",
    "    for k, hidden_neurons in enumerate(current_neurons):\n",
    "        for v in hidden_neurons:\n",
    "            v_label[v] = \"H\"\n",
    "    for v in output_neurons:\n",
    "        v_label[v] = \"O\"\n",
    "    \n",
    "    # Draw the graph\n",
    "    graph_draw(g, pos=pos, vertex_text=v_label, edge_text=None, edge_pen_width=e_width, vertex_size=15, vertex_font_size=10, edge_font_size=10, output_size=(800, 800))\n",
    "    \n",
    "# Example usage with the same nn_wandb\n",
    "plot_mlp(nn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cd687-cfeb-4de4-b479-afc3daa27d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_env",
   "language": "python",
   "name": "collective_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
