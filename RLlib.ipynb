{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615b7cc4-c1da-49fb-85a5-29eee064dd0d",
   "metadata": {},
   "source": [
    "# Some preliminary checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14976ee1-cfb2-4052-a40b-0f316a54d317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T21:04:58.404952Z",
     "iopub.status.busy": "2023-10-08T21:04:58.404548Z",
     "iopub.status.idle": "2023-10-08T21:05:01.064092Z",
     "shell.execute_reply": "2023-10-08T21:05:01.063754Z",
     "shell.execute_reply.started": "2023-10-08T21:04:58.404931Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.1.0\n",
      "CUDA Available: False\n",
      "CUDA Version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9367a277-f21e-4ccb-9fc1-c5c1c1dd292b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T21:05:01.064608Z",
     "iopub.status.busy": "2023-10-08T21:05:01.064462Z",
     "iopub.status.idle": "2023-10-08T21:05:01.066518Z",
     "shell.execute_reply": "2023-10-08T21:05:01.066302Z",
     "shell.execute_reply.started": "2023-10-08T21:05:01.064601Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs:  12\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# print number of gpus / CPUs\n",
    "print(\"Number of CPUs: \", psutil.cpu_count())\n",
    "\n",
    "num_cpus = 12\n",
    "num_gpus = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c3bb7-053c-498c-a15a-8d42031fb8d0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82dfc156-a283-4a91-a2e5-6e97568ea596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T21:05:01.066826Z",
     "iopub.status.busy": "2023-10-08T21:05:01.066755Z",
     "iopub.status.idle": "2023-10-08T21:05:01.714382Z",
     "shell.execute_reply": "2023-10-08T21:05:01.714122Z",
     "shell.execute_reply.started": "2023-10-08T21:05:01.066819Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 23:05:01,234\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-10-08 23:05:01,289\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-10-08 23:05:01,367\tWARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "2023-10-08 23:05:01,680\tWARNING deprecation.py:50 -- DeprecationWarning: `build_tf_policy` has been deprecated. This will raise an error in the future!\n",
      "2023-10-08 23:05:01,681\tWARNING deprecation.py:50 -- DeprecationWarning: `build_policy_class` has been deprecated. This will raise an error in the future!\n",
      "2023-10-08 23:05:01,694\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2023-10-08 23:05:01,707\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-10-08 23:05:01,712\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "## The RLlib configuration\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.run = \"PPO\"\n",
    "        self.framework = \"torch\" # \"tf2\" or \"torch\"\n",
    "args = Args()\n",
    "\n",
    "## Generate the configuration\n",
    "env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "config = (\n",
    "    get_trainable_cls(args.run)\n",
    "    .get_default_config()\n",
    "    .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "    .framework(args.framework)\n",
    "    .training(_enable_learner_api=True, num_sgd_iter=10, sgd_minibatch_size=256, train_batch_size=20000)\n",
    "    .multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": PolicySpec(\n",
    "                policy_class=None,  # infer automatically from Algorithm\n",
    "                observation_space=env.observation_space[0],  # if None infer automatically from env\n",
    "                action_space=env.action_space[0],  # if None infer automatically from env\n",
    "                config={\"gamma\": 0.85},  # use main config plus <- this override here\n",
    "            ),\n",
    "            \"predator\": PolicySpec(\n",
    "                policy_class=None,\n",
    "                observation_space=env.observation_space[0],\n",
    "                action_space=env.action_space[0],\n",
    "                config={\"gamma\": 0.85},\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda id, *arg, **karg: \"prey\" if env.agents[id].agent_type == 0 else \"predator\",\n",
    "        policies_to_train=[\"prey\", \"predator\"]\n",
    "    )\n",
    "    .rl_module(_enable_rl_module_api=True)\n",
    "    .rollouts(\n",
    "        rollout_fragment_length=\"auto\",\n",
    "        batch_mode= 'truncate_episodes',\n",
    "        num_rollout_workers=num_cpus-1,\n",
    "        num_envs_per_worker=1,\n",
    "        #create_env_on_local_worker=False,\n",
    "    )\n",
    "    # This as to be specified everytime (don't know how to automatically ajust)\n",
    "    .resources(\n",
    "        #num_gpus = num_gpus,\n",
    "        #num_gpus_per_worker=0,\n",
    "        #num_cpus_per_worker=2,\n",
    "        # learner workers\n",
    "        #num_learner_workers=num_gpus,\n",
    "        #num_gpus_per_learner_worker=1,\n",
    "        #num_cpus_per_learner_worker=0,\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    ")\n",
    "config.exploration_config = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056eb565-f91d-4c3c-bced-93a56886dca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-08T21:05:01.715627Z",
     "iopub.status.busy": "2023-10-08T21:05:01.715472Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 23:05:03,374\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "2023-10-08 23:05:03,817\tINFO tune.py:645 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num CPUS rays sees : 12.0\n",
      "num GPUS rays sees : 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-10-08 23:07:36</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:32.73        </td></tr>\n",
       "<tr><td>Memory:      </td><td>21.1/64.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 12.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnvironment_59e38_00000</td><td>RUNNING </td><td>127.0.0.1:44998</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         122.455</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-15.7471</td><td style=\"text-align: right;\">            -13.1752</td><td style=\"text-align: right;\">            -18.0806</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 23:05:03,831\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=44998)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=44998)\u001b[0m 2023-10-08 23:05:06,723\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=44998)\u001b[0m 2023-10-08 23:05:06,723\tWARNING algorithm_config.py:672 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=45001)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45001)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45001)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45001)\u001b[0m 2023-10-08 23:05:11,721\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45001)\u001b[0m 2023-10-08 23:05:11,728\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45001)\u001b[0m 2023-10-08 23:05:11,728\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45001)\u001b[0m 2023-10-08 23:05:11,728\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45001)\u001b[0m 2023-10-08 23:05:11,728\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45001)\u001b[0m 2023-10-08 23:05:11,731\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=45004)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=45005)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=45006)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=45008)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=45010)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=45009)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=45003)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=45002)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=45011)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=45007)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45004)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45004)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45004)\u001b[0m 2023-10-08 23:05:12,061\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45004)\u001b[0m 2023-10-08 23:05:12,068\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45005)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45005)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45005)\u001b[0m 2023-10-08 23:05:12,164\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45005)\u001b[0m 2023-10-08 23:05:12,170\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45010)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45010)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45006)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45006)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45006)\u001b[0m 2023-10-08 23:05:12,182\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45006)\u001b[0m 2023-10-08 23:05:12,189\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45002)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45002)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45002)\u001b[0m 2023-10-08 23:05:12,216\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45002)\u001b[0m 2023-10-08 23:05:12,223\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=44998)\u001b[0m 2023-10-08 23:05:12,266\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=44998)\u001b[0m 2023-10-08 23:05:12,270\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=44998)\u001b[0m 2023-10-08 23:05:12,270\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=44998)\u001b[0m 2023-10-08 23:05:12,270\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=44998)\u001b[0m 2023-10-08 23:05:12,270\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=44998)\u001b[0m 2023-10-08 23:05:12,273\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45008)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45008)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45008)\u001b[0m 2023-10-08 23:05:12,243\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45008)\u001b[0m 2023-10-08 23:05:12,249\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45010)\u001b[0m 2023-10-08 23:05:12,229\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45010)\u001b[0m 2023-10-08 23:05:12,236\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45009)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45009)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45009)\u001b[0m 2023-10-08 23:05:12,243\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45009)\u001b[0m 2023-10-08 23:05:12,250\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45007)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45007)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45007)\u001b[0m 2023-10-08 23:05:12,251\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45007)\u001b[0m 2023-10-08 23:05:12,257\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45003)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45003)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45003)\u001b[0m 2023-10-08 23:05:12,249\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45003)\u001b[0m 2023-10-08 23:05:12,255\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45011)\u001b[0m /Users/tanguy/miniforge3/envs/collective_env/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45011)\u001b[0m   gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45011)\u001b[0m 2023-10-08 23:05:12,241\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45011)\u001b[0m 2023-10-08 23:05:12,247\tWARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=44998)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=45024)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=45024)\u001b[0m wandb: Currently logged in as: tanguy-cazalets (tcazalet_airo). Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=45001)\u001b[0m 2023-10-08 23:05:20,458\tWARNING env_runner_v2.py:155 -- More than 27636 observations in 1819 env steps for episode 868775292833521196 are buffered in the sampler. If this is more than you expected, check that that you set a horizon on your environment correctly and that it terminates at some point. Note: In multi-agent environments, `rollout_fragment_length` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=45024)\u001b[0m wandb: Tracking run with wandb version 0.15.12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=45024)\u001b[0m wandb: Run data is saved locally in /Users/tanguy/ray_results/PPO_2023-10-08_23-05-03/PPO_CustomEnvironment_59e38_00000_0_2023-10-08_23-05-03/wandb/run-20231008_230517-59e38_00000\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=45024)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=45024)\u001b[0m wandb: Syncing run PPO_CustomEnvironment_59e38_00000\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=45024)\u001b[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/tcazalet_airo/marl-rllib\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=45024)\u001b[0m wandb: üöÄ View run at https://wandb.ai/tcazalet_airo/marl-rllib/runs/59e38_00000\n",
      "\u001b[2m\u001b[36m(_QueueActor pid=45023)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>checkpoint_dir_name  </th><th>connector_metrics                                                                                                                                             </th><th>counters                                                                                                                     </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>hostname                   </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                          </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max                                           </th><th>policy_reward_mean                                         </th><th>policy_reward_min                                             </th><th>sampler_perf                                                                                                                                                                                                   </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                               </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnvironment_59e38_00000</td><td style=\"text-align: right;\">                 606329</td><td>                     </td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.006701368274110736, &#x27;StateBufferConnector_ms&#x27;: 0.003612041473388672, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.4773656527201335}</td><td>{&#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 0, &#x27;num_agent_steps_sampled&#x27;: 606329, &#x27;num_agent_steps_trained&#x27;: 0}</td><td>{}              </td><td>2023-10-08_23-07-17</td><td>False </td><td style=\"text-align: right;\">              1000</td><td>{}             </td><td style=\"text-align: right;\">            -13.1752</td><td style=\"text-align: right;\">             -15.7471</td><td style=\"text-align: right;\">            -18.0806</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">              33</td><td>MacBook-Pro-de-Tanguy.local</td><td>{&#x27;learner&#x27;: {&#x27;__all__&#x27;: {&#x27;num_agent_steps_trained&#x27;: 512.0, &#x27;num_env_steps_trained&#x27;: 273597.0, &#x27;total_loss&#x27;: 0.27068477739164504}, &#x27;prey&#x27;: {&#x27;total_loss&#x27;: 0.27068477739164504, &#x27;policy_loss&#x27;: -0.0013974461996663658, &#x27;vf_loss&#x27;: 0.04968034114312784, &#x27;vf_loss_unclipped&#x27;: 0.16782034388466477, &#x27;vf_explained_var&#x27;: -0.8716005432993887, &#x27;entropy&#x27;: 2.8677008319534583, &#x27;mean_kl_loss&#x27;: 0.0067088654718796175, &#x27;default_optimizer_lr&#x27;: 5.000000000000002e-05, &#x27;curr_lr&#x27;: 5e-05, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;curr_kl_coeff&#x27;: 0.10000000149011612}, &#x27;predator&#x27;: {&#x27;total_loss&#x27;: 0.22173099595255882, &#x27;policy_loss&#x27;: -0.004570574517268409, &#x27;vf_loss&#x27;: 0.22484674687812167, &#x27;vf_loss_unclipped&#x27;: 0.7706875287265978, &#x27;vf_explained_var&#x27;: -0.546073189635432, &#x27;entropy&#x27;: 2.6696010577063003, &#x27;mean_kl_loss&#x27;: 0.007274116323971793, &#x27;default_optimizer_lr&#x27;: 5.000000000000002e-05, &#x27;curr_lr&#x27;: 5e-05, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;curr_kl_coeff&#x27;: 0.20000000298023224}}, &#x27;num_env_steps_sampled&#x27;: 40000, &#x27;num_env_steps_trained&#x27;: 0, &#x27;num_agent_steps_sampled&#x27;: 606329, &#x27;num_agent_steps_trained&#x27;: 0}</td><td style=\"text-align: right;\">                         2</td><td>127.0.0.1</td><td style=\"text-align: right;\">                   606329</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">                  40000</td><td style=\"text-align: right;\">                            20000</td><td style=\"text-align: right;\">                                   321.196</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                                0</td><td style=\"text-align: right;\">                                         0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                   11</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                            0</td><td>{&#x27;cpu_util_percent&#x27;: 25.52934782608696, &#x27;ram_util_percent&#x27;: 32.62173913043478}</td><td style=\"text-align: right;\">44998</td><td>{&#x27;prey&#x27;: -1.0251783437068376, &#x27;predator&#x27;: 68.89013974748939}</td><td>{&#x27;prey&#x27;: -4.093891080340487, &#x27;predator&#x27;: 22.83063343309827}</td><td>{&#x27;prey&#x27;: -11.080116384082547, &#x27;predator&#x27;: -1.1080296546613446}</td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 2.1939569612040453, &#x27;mean_inference_ms&#x27;: 1.2477843403823796, &#x27;mean_action_processing_ms&#x27;: 0.5297219811578259, &#x27;mean_env_wait_ms&#x27;: 0.5158033607569157, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: -13.175176681382728, &#x27;episode_reward_min&#x27;: -18.08055482624098, &#x27;episode_reward_mean&#x27;: -15.747099338910871, &#x27;episode_len_mean&#x27;: 1000.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 22, &#x27;policy_reward_min&#x27;: {&#x27;prey&#x27;: -11.080116384082547, &#x27;predator&#x27;: -1.1080296546613446}, &#x27;policy_reward_max&#x27;: {&#x27;prey&#x27;: -1.0251783437068376, &#x27;predator&#x27;: 68.89013974748939}, &#x27;policy_reward_mean&#x27;: {&#x27;prey&#x27;: -4.093891080340487, &#x27;predator&#x27;: 22.83063343309827}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-16.756132346618443, -14.501565454718891, -13.175176681382728, -18.08055482624098, -16.144062496855508, -13.824497821645368, -16.82879556655911, -16.255226058367786, -14.959426502814711, -14.573104679344599, -14.388639133580924, -15.547259474536718, -16.289217920126678, -15.34391587861511, -15.297621451297937, -17.190442560896557, -15.778501798514519, -14.406197921360242, -15.121161600087737, -16.990076999776697, -16.271901995584162, -15.08656900148258, -14.210986728809635, -16.935501941889058, -17.63501686421306, -16.751746859128286, -16.87354837408199, -14.110544765486335, -16.525795587064398, -16.096287366657762, -15.853084279926215, -15.27019188769467, -16.581525358699256], &#x27;episode_lengths&#x27;: [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000], &#x27;policy_prey_reward&#x27;: [-1.0331809450425662, -1.0860311765285002, -1.0789160309113015, -10.222585812305075, -1.0885190013877946, -1.0739105231401669, -1.0732489156569536, -1.0921225519214195, -10.368044143944816, -1.0671625879372197, -1.0743417182025696, -1.0650602115897558, -1.0663941795219765, -1.0770590675955622, -1.0607061164649316, -1.0821987554556747, -1.0848810905756954, -10.420628465479055, -1.0681116237797124, -10.425461987005278, -1.0757866610307094, -10.157271320075582, -1.0609468503549733, -1.0731122445915158, -1.0555006932373276, -1.0689417432039938, -10.002679324257274, -10.829902065476357, -10.881944503631733, -10.999749005103649, -10.534504407751927, -11.03015036433386, -1.088265576768338, -1.0952239511781974, -10.267192763511902, -10.490237850936511, -10.327538104389735, -10.946902892854187, -1.0461746198133122, -10.387415904345465, -1.0613581861701888, -10.039393239729876, -10.875296853356485, -1.051760158678675, -10.720347902505472, -1.0756334594062185, -1.0645100181970246, -1.0899318246859064, -10.835528933298074, -1.1052971028622822, -1.0935394734006152, -1.05324173418818, -1.0847921509260492, -1.075193977480347, -1.0813992174008693, -1.0633979132813636, -1.056373391940204, -1.0762870498672583, -1.0649791260724908, -1.0595737924450823, -1.094810772502805, -1.070551043421539, -1.0889233094808686, -1.0894892644213041, -1.0968734273495644, -1.08028321533418, -1.1011121648661713, -1.0893459529983396, -1.0636918780305766, -1.046717367581855, -10.001578636852912, -1.065097808501408, -10.435214924681675, -1.0382654871442476, -10.546708025052403, -1.0546830216455978, -1.0660283451351482, -10.421723676368654, -1.078780986362438, -10.525737337785818, -10.282051633589887, -10.495801967362983, -11.041343584654998, -10.540902876047618, -1.072167454529644, -10.594919937685368, -10.698722929322676, -1.0777409789408703, -10.59864107627411, -1.0704507058293986, -11.069523123008477, -1.0696898476332704, -1.0657603951898384, -1.0592689197045273, -10.246358951575024, -10.520070486708974, -1.084551716538982, -1.0752379471859184, -1.074385990102115, -11.018031971886245, -1.0744173002660773, -1.049011022667481, -1.0580235560426003, -1.105692210906728, -1.050753924840243, -1.0685528394060921, -1.0974548755489446, -1.0709307731568498, -1.067206933308821, -1.0745401224521294, -1.0964873965080473, -1.0688704030264784, -10.605201167113501, -11.038410598668671, -1.0871369979025678, -1.0593136256754514, -10.191100188901954, -11.036677773165614, -1.059840007403642, -10.410413122707675, -1.086456617850566, -1.0848153059093792, -10.238008443105928, -1.0967298865843054, -10.029639907701181, -1.0977946536028251, -1.04952792370286, -10.14885455101178, -1.0738451948389245, -1.096330897602225, -10.491248218191128, -11.038143249770254, -1.0531462287490492, -1.0727804920049868, -1.0915315413079392, -1.0705191992724175, -10.346475414439308, -1.0675691012650572, -1.0885210634597642, -1.0626541854905296, -10.260028689813842, -1.0911778973047912, -1.0955455639095235, -1.0772161096820287, -10.762526730696404, -1.1000030670951262, -10.056710141223254, -10.1136117261094, -1.0797200374329123, -1.0698569554556527, -1.0875865715681536, -1.0747585710234329, -10.708892946959644, -1.0696853480482797, -1.0538281273238694, -1.0798889888222356, -10.077134661005415, -1.056110884205031, -10.546677954866807, -1.0594030584564555, -1.0616993124425214, -10.93242470427781, -10.544810870029686, -10.32867728424329, -10.487663347988416, -11.032988880008473, -1.0532305309962444, -1.032304231928622, -1.0675332959210646, -1.0701143048528192, -1.0829052839801632, -10.125207151610613, -1.0732650076783383, -1.0816046692982013, -10.106750850453206, -10.908353891548918, -1.0762415930680718, -1.0807956563674659, -10.468791558694804, -1.0892709236147555, -1.071031633294851, -10.223798484888022, -10.781359694306623, -10.314183767491823, -10.759060875615104, -1.0904938120830452, -1.101493877525135, -1.0943390850332124, -1.0954513902724794, -1.0874638838981416, -1.0820925768288494, -1.1324423847928926, -1.0855930919846002, -1.0530611151282576, -1.0734353140123216, -1.0824154252071485, -10.156379174043112, -1.0631446653753724, -10.443814870394803, -1.0584384778256806, -1.056463558517698, -1.0932530087144305, -1.078616884204146, -1.0561677939284764, -1.0695846397663573, -10.115308941063704, -1.0656875226190943, -1.0921905952732047, -10.65260864577935, -1.043370558268691, -10.275200712145809, -1.1030842716582419, -1.1237186264107342, -1.1017016166736695, -10.895568645003584, -1.095275205719677, -10.2220516197573, -1.0815797631074242, -10.690317332937543, -10.954565928086895, -1.07430776302436, -10.244988928689487, -1.0620254802046767, -1.0889384907138109, -1.0849331413533418, -1.099542304851256, -1.0775055240924467, -10.816821402818642, -1.1139692262020477, -1.0758072580973295, -1.0859942505342628, -1.0638160618382515, -1.0717224846739772, -1.058588739171481, -1.034445930937675, -1.0438263378370192, -1.0701232140873527, -10.233039964132711, -1.0649232267328612, -1.055990050708078, -10.418955918670664, -1.071139071235715, -1.088018817559707, -11.06552623613043, -10.778600559642424, -1.1126688317547029, -10.105082457500899, -1.1202502688682165, -1.0994461030915788, -1.079490075100534, -10.899697289723072, -1.0942535602906753, -10.782162337073043, -10.79416768407992, -1.0522895344591814, -10.727284288621458, -1.0797150696275517, -1.064747055315219, -10.268011536774102, -1.0769981289728325, -10.07542744137072, -10.734900029493206, -1.084700598854851, -1.0868192608492488, -10.204039863428642, -1.0568488364650064, -1.0973648517565793, -1.0728077711182864, -10.489902778106178, -1.0653807236561825, -1.0893663575192816, -10.081559907742147, -1.097795501862053, -10.24162414226307, -1.1134425345350745, -1.0975926132401295, -1.1018975612727933, -1.103875634928367, -10.47700120138679, -1.117644688299702, -1.1027771728192113, -1.111529853132419, -10.203379354260182, -10.958365719193896, -10.990315680954216, -1.0678439173837893, -10.605509207802507, -1.0710954197335492, -1.0848326036300446, -1.056003209170052, -1.063675972076112, -1.0251783437068376, -1.0601557793960052, -1.067128819325811, -10.557563348548003, -10.832154117090113, -1.082364045906554, -1.089852597606391, -1.0544241197694946, -1.058258686592715, -1.0986305796519686, -11.030445948977993, -10.269730307033916, -10.785521939128925, -10.61987471596054, -1.0781846508987525, -1.0970568297995427, -1.0683428921405729, -1.0747155335060072, -1.0971013906278453, -1.0453802238968126, -10.81460180311684, -1.1220632464763642, -10.886156256261005, -10.954616130474184, -10.437778855647435, -1.0782178645986653, -10.31036740562049, -1.0422205486829084, -10.300392699516927, -1.0970732989710428, -1.0914356243308754, -1.0835491738929817, -10.460555234660893, -1.0583068998970806, -10.643434764014234, -1.0541899988003718, -1.0680066339242633, -1.0712837761645477, -1.0608347260771638, -11.080116384082547, -10.178144175702183, -1.0942055754941147, -1.1097166428732128, -1.0612465318179742, -1.1070964554444882, -1.0784736836104596, -10.518948397713206, -1.105649819577462, -10.194769264054672, -1.0681956155739556, -10.00448676015266, -10.196998033101181, -1.0904792372508165, -1.08460157769112, -1.0522642262963628, -10.250799331155555, -1.0768496918496917, -1.1028430096892006, -1.1124953973289073, -1.0866590115567276, -1.0764072353099479, -1.077892774471119, -1.0631948094227426, -1.0881285177813302, -1.0689045967546262, -10.770643950154495, -1.0900806402824859, -1.073184652126815, -10.725107310351612, -1.0896536809882025, -1.105865653861617, -1.0610572042265085, -1.0573719842025784, -10.261347240616423, -1.0954742319918571, -1.0627651853433815, -1.0642439081008852, -1.1208572130240173, -1.0860202818862466, -1.0919104205410075, -1.074644416493442, -1.074148818718204, -1.1086528095312784, -1.0465381447954814, -1.072173292630277, -1.0957140561838072, -1.0819561797230328, -1.047599800749802, -1.0598242689960942, -1.106086770014188, -1.0693152445555658, -1.049747197663337, -10.018408074352514, -1.0931741488770654, -1.058224307875032, -10.553282968293228, -1.0815606376486249, -1.0736422516293267, -1.0753129752076267, -10.542678271757229, -1.0744645227535807, -1.0760884340381736, -1.0877113431059178, -1.0822539159846, -1.0520669762561417, -11.071470682856416, -10.124150261358897, -1.084819956964208, -1.0805294636689489, -1.0950709786462736, -1.0855809759740778, -1.0626456023869664, -1.078049753131857, -1.0792649714837772, -1.057968325860867, -10.404959376308058, -1.056982699726126, -1.0967754544663486, -10.648941789269553, -10.25461019837555, -1.076753815543661, -1.0906039378574213, -1.0673579935241597, -10.50694669934502, -1.0521762509081305, -10.104995355880456, -10.377745402744035, -1.0478684755599577, -1.0947743051787946, -1.1017748907847629, -1.0644419112718693, -1.1101896617159093, -1.0812190396822912, -1.067605814285048, -10.529119514951821, -1.0825226799334773, -1.070467573519321, -1.085133736241524, -10.564836081843872, -1.0680683499808779, -1.1077525642883637, -10.184100517755843, -1.0787584836300497, -1.0984898781580335, -1.0602201104993965, -1.070217796841316, -1.0838192378267666, -1.1037669669110257, -10.686615023317847, -1.0613742024079371, -10.909715937169691, -1.0712382439537511, -1.1054196747916665, -10.387303128216809, -1.0822213154582763, -10.163430604144821, -1.0995192392664201, -1.0512325209544706, -10.980254566671388, -10.800669105186888, -1.1225541252609037, -10.317138548700388, -1.082434751424946, -1.1152824515216304, -1.0982077658477494, -1.0824508662337542, -1.0974416827478752, -10.088605572152103, -1.0760360847461707, -1.1104979727825082, -10.739215327151687, -1.0612861143050403, -1.1213684820135252, -10.67590401463751, -10.095235243943245, -10.851877676193856, -10.94656307888867, -1.070046876501675, -1.05828544502263, -1.0559387881790852, -1.0723341473376231, -10.387781380216355, -10.550830681193416, -1.0846760573699317, -1.0954202589620736, -10.611353406974729, -1.051444250260534, -1.0681857704205857, -1.0794349667848908, -10.743637259139097, -1.1253872456642624, -1.0934218217845364, -1.0652221285053343, -10.809323540216358, -1.0708240635979969, -1.1057796789764367, -1.115845529813247, -1.1228142050919772, -10.39094324706491, -1.0711494600264087, -10.72732546017665, -1.1135275793684372, -10.703327741956784, -1.10196379357755], &#x27;policy_predator_reward&#x27;: [-1.1007851384663407, 18.871935773998683, 28.89376661509801, 38.89178426344177, 28.89644634745207, 68.89013974748939, 8.899654699330442, -1.1005303601196093, 8.870033835753986, 18.894566945610507, 48.90514680767333, 38.890051882216625, 18.900563604227504, 18.891418193469878, 28.90572010590034, 18.87119066067836, 28.90157144799706, 18.887855161121767, 8.875849218106913, 38.89318198519854, 48.88512726223081, 18.89547623544951, 18.914888318781962, 28.88721003670337, 8.881436928630475, 28.874646138398266, 8.894550274449255, 28.88897860791711, 28.90632208498825, 28.894313989200477, 8.890647355619231, 8.885026060199605, 28.902797269932588, 38.880449676733825, 38.882233531547854, 18.89651678150211, 8.873765564168652, 48.89324075915313, 8.888858023738333, 18.89710516422296, 28.88927111735901, 38.881249335008214, 28.883584049905906, 18.887494453411584, 28.87119924045364, 28.890942184877005, 8.890537699783946, 18.88941551285859, -1.1080296546613446, 8.873563984769282, 18.88539706005956, -1.1011217447882673, -1.0922346314504443, 28.8955323677355, 38.925011095161366, 18.903904219900564, 8.882504041401841, 18.88618106957687, 28.904738380972702, 18.915322820801283, 28.876424012422714, 18.859584572363943, 28.89585882503507, 28.91335731551968, 38.86727688289295, 8.911690513367786]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 2.1939569612040453, &#x27;mean_inference_ms&#x27;: 1.2477843403823796, &#x27;mean_action_processing_ms&#x27;: 0.5297219811578259, &#x27;mean_env_wait_ms&#x27;: 0.5158033607569157, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.006701368274110736, &#x27;StateBufferConnector_ms&#x27;: 0.003612041473388672, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.4773656527201335}}</td><td style=\"text-align: right;\">             122.455</td><td style=\"text-align: right;\">           62.2698</td><td style=\"text-align: right;\">       122.455</td><td>{&#x27;training_iteration_time_ms&#x27;: 61225.002, &#x27;sample_time_ms&#x27;: 8838.516, &#x27;synch_weights_time_ms&#x27;: 7.191}</td><td style=\"text-align: right;\"> 1696799237</td><td style=\"text-align: right;\">            40000</td><td style=\"text-align: right;\">                   2</td><td>59e38_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 23:06:15,375\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.753 s, which may be a performance bottleneck.\n",
      "2023-10-08 23:06:15,376\tWARNING util.py:315 -- The `process_trial_result` operation took 2.754 s, which may be a performance bottleneck.\n",
      "2023-10-08 23:06:15,376\tWARNING util.py:315 -- Processing trial results took 2.754 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-10-08 23:06:15,376\tWARNING util.py:315 -- The `process_trial_result` operation took 2.754 s, which may be a performance bottleneck.\n"
     ]
    }
   ],
   "source": [
    "import ray \n",
    "from ray import air, tune\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "\n",
    "ray.init(num_cpus=num_cpus, num_gpus=num_gpus)\n",
    "\n",
    "print(\"num CPUS rays sees :\", ray.cluster_resources().get('CPU', 0))\n",
    "print(\"num GPUS rays sees :\", ray.cluster_resources().get('GPU', 0))\n",
    "\n",
    "opti_config = {\n",
    "    'stop_iters': 500,\n",
    "    'stop_timesteps': 10000000,\n",
    "    'stop_reward': 0.1,\n",
    "    'as_test': False\n",
    "}\n",
    "\n",
    "## Run the experiemnt    \n",
    "tuner = tune.Tuner(\n",
    "    args.run,\n",
    "    param_space=config,\n",
    "    run_config=air.RunConfig(\n",
    "        stop={\n",
    "            \"training_iteration\": opti_config[\"stop_iters\"],\n",
    "            \"timesteps_total\": opti_config[\"stop_timesteps\"],\n",
    "            \"episode_reward_mean\": opti_config[\"stop_reward\"],\n",
    "        },\n",
    "        verbose=3,\n",
    "        callbacks=[WandbLoggerCallback(\n",
    "            project=\"marl-rllib\", \n",
    "            group=\"PPO\",\n",
    "            api_key=\"90dc2cefddde123eaac0caae90161981ed969abe\",\n",
    "            log_config=True,\n",
    "        )],\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "            checkpoint_frequency=10\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "if opti_config[\"as_test\"]:\n",
    "    print(\"Checking if learning goals were achieved\")\n",
    "    check_learning_achieved(results, opti_config[\"stop_reward\"])\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53c262-da4a-41ca-bc49-b97821f17639",
   "metadata": {},
   "source": [
    "# Render episode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67626d-946b-45ea-b59d-c6ed65ab1772",
   "metadata": {},
   "source": [
    "### Retrieve checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edfb36-fe9d-47f1-a07d-cbaa2b709b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = results.get_best_result().checkpoint\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96925a71-e8c6-4d9b-97b5-593a15214c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "algo = Algorithm.from_checkpoint(best_checkpoint)\n",
    "\n",
    "# After loading the algorithm\n",
    "local_worker = algo.workers.local_worker()\n",
    "available_policy_ids = list(local_worker.policy_map.keys())\n",
    "print(\"Available Policy IDs:\", available_policy_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18e7bb-3a95-4978-82ce-03d4c8bc9b6c",
   "metadata": {},
   "source": [
    "### Run and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8874e06-f306-4ab1-be42-084d479af642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_observations(observation, agent_ids, truncation=None):\n",
    "    loc_x = [observation[key][4] if key in observation else 0 for key in agent_ids]\n",
    "    loc_y = [observation[key][5] if key in observation else 0 for key in agent_ids]\n",
    "    if truncation:\n",
    "        still_in_the_game = [1 if not truncation[key] else 0 for key in agent_ids]\n",
    "    else:\n",
    "        still_in_the_game = [1 for _ in agent_ids]\n",
    "    observations[\"loc_x\"].append(np.array(loc_x))\n",
    "    observations[\"loc_y\"].append(np.array(loc_y))\n",
    "    observations[\"still_in_the_game\"].append(np.array(still_in_the_game))\n",
    "    \n",
    "    return observations\n",
    "\n",
    "# Use the first available policy ID\n",
    "policy_id = available_policy_ids[0]\n",
    "\n",
    "step_count = 0\n",
    "observations = {\"loc_x\": [], \"loc_y\": [], \"still_in_the_game\": []}\n",
    "\n",
    "observation, _ = env.reset()\n",
    "agent_ids = env._agent_ids\n",
    "loc_x, loc_y, still_in_the_game = process_observations(observation, agent_ids)\n",
    "\n",
    "\n",
    "while step_count < 500:\n",
    "    actions = {\n",
    "        key: algo.compute_single_action(\n",
    "            value, policy_id=\"prey\" if env.agents[key].agent_type == 0 else \"predator\"\n",
    "        ) for key, value in observation.items()\n",
    "    }\n",
    "    \n",
    "    observation, _, termination, truncation, _ = env.step(actions)\n",
    "    \n",
    "    observations = process_observations(observation, agent_ids, truncation)\n",
    "    \n",
    "    step_count += 1\n",
    "\n",
    "stage_size = env.stage_size\n",
    "observations[\"loc_x\"] = np.array(observations[\"loc_x\"]) * stage_size\n",
    "observations[\"loc_y\"] = np.array(observations[\"loc_y\"]) * stage_size\n",
    "observations[\"still_in_the_game\"] = np.array(observations[\"still_in_the_game\"])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe696df-26b6-4a1b-8692-8965b6798da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import animation\n",
    "\n",
    "importlib.reload(animation)\n",
    "from animation import generate_animation\n",
    "\n",
    "ani = generate_animation(observations, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695eb08-dd06-49f2-8cb0-c7285ca37570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de9596-3117-4b09-ba52-c7e05c8acf7a",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801af95-5ed3-4128-9c15-44f465cae96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "def restore_policy_and_weights(policy_type):\n",
    "    checkpoint_path = os.path.join(best_checkpoint.to_directory(), f\"policies/{policy_type}\")\n",
    "    restored_policy = Policy.from_checkpoint(checkpoint_path)\n",
    "    return restored_policy.get_weights()\n",
    "\n",
    "restored_policy_predator_weights = restore_policy_and_weights(\"predator\")\n",
    "restored_policy_prey_weights = restore_policy_and_weights(\"prey\")\n",
    "\n",
    "print(\"Starting new tune.Tuner().fit()\")\n",
    "\n",
    "ray.init()\n",
    "\n",
    "# Start our actual experiment.\n",
    "stop = {\n",
    "    \"episode_reward_mean\": args.stop_reward,\n",
    "    \"timesteps_total\": args.stop_timesteps,\n",
    "    \"training_iteration\": args.stop_iters,\n",
    "}\n",
    "\n",
    "class RestoreWeightsCallback(DefaultCallbacks):\n",
    "    def on_algorithm_init(self, *, algorithm: \"Algorithm\", **kwargs) -> None:\n",
    "        algorithm.set_weights({\"predator\": restored_policy_predator_weights})\n",
    "        algorithm.set_weights({\"prey\": restored_policy_prey_weights})\n",
    "\n",
    "config.callbacks(RestoreWeightsCallback)\n",
    "\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    stop=stop,\n",
    "    config=config.to_dict(),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "if args.as_test:\n",
    "    check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dc324-7316-4f4d-945d-55a81d1bdb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2f440ff6-9d75-4466-8b96-19421a01be6f",
   "metadata": {},
   "source": [
    "1GPU V100  2CPU\n",
    "426 for 20000 with         num_gpus_per_learner_worker=1, num_learner_workers=1\n",
    "\n",
    "2CPU\n",
    "378.389 for 20000 without\n",
    "\n",
    "4GPU, 2CPU\n",
    "295.219 for 20000 with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85972d-e671-410b-a555-73e52fde2c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective_env",
   "language": "python",
   "name": "collective_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
