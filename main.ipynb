{
 "cells": [
  {
   "cell_type": "raw",
   "id": "35f7cf6e-8176-4d10-baa5-bf9d4249deee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T09:17:14.582582Z",
     "iopub.status.busy": "2023-08-07T09:17:14.582238Z",
     "iopub.status.idle": "2023-08-07T09:18:35.123919Z",
     "shell.execute_reply": "2023-08-07T09:18:35.123341Z",
     "shell.execute_reply.started": "2023-08-07T09:17:14.582559Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Example of a custom gym environment. Run this example for a demo.\n",
    "\n",
    "This example shows the usage of:\n",
    "  - a custom environment\n",
    "  - Ray Tune for grid search to try different learning rates\n",
    "\n",
    "You can visualize experiment results in ~/ray_results using TensorBoard.\n",
    "\n",
    "Run example with defaults:\n",
    "$ python custom_env.py\n",
    "For CLI options:\n",
    "$ python custom_env.py --help\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
    "\n",
    "    You can configure the length of the corridor via the env config.\"\"\"\n",
    "\n",
    "    def __init__(self, config: EnvContext):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(0.0, self.end_pos, shape=(1,), dtype=np.float32)\n",
    "        # Set the seed. This is only used for the final (reach goal) reward.\n",
    "        self.reset(seed=config.worker_index * config.num_workers)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        random.seed(seed)\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos], {}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        done = truncated = self.cur_pos >= self.end_pos\n",
    "        # Produce a random reward when we reach the goal.\n",
    "        return (\n",
    "            [self.cur_pos],\n",
    "            random.random() * 2 if done else -0.1,\n",
    "            done,\n",
    "            truncated,\n",
    "            {},\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.run = \"PPO\"\n",
    "            self.framework = \"torch\"\n",
    "            self.as_test = False\n",
    "            self.stop_iters = 50\n",
    "            self.stop_timesteps = 100000\n",
    "            self.stop_reward = 0.1\n",
    "            self.no_tune = False\n",
    "            self.local_mode = False\n",
    "    \n",
    "    args = Args()\n",
    "    print(f\"Running with following options: {args.__dict__}\")\n",
    "    print(f\"Running with following CLI options: {args}\")\n",
    "\n",
    "    ray.init(local_mode=args.local_mode)\n",
    "\n",
    "    # Can also register the env creator function explicitly with:\n",
    "    # register_env(\"corridor\", lambda config: SimpleCorridor(config))\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        # or \"corridor\" if registered above\n",
    "        .environment(SimpleCorridor, env_config={\"corridor_length\": 5})\n",
    "        .framework(args.framework)\n",
    "        .rollouts(num_rollout_workers=1)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "    }\n",
    "\n",
    "    if args.no_tune:\n",
    "        # manual training with train loop using PPO and fixed learning rate\n",
    "        if args.run != \"PPO\":\n",
    "            raise ValueError(\"Only support --run PPO with --no-tune.\")\n",
    "        print(\"Running manual train loop without Ray Tune.\")\n",
    "        # use fixed learning rate instead of grid search (needs tune)\n",
    "        config.lr = 1e-3\n",
    "        algo = config.build()\n",
    "        # run manual training loop and print results after each iteration\n",
    "        for _ in range(args.stop_iters):\n",
    "            result = algo.train()\n",
    "            print(pretty_print(result))\n",
    "            # stop training of the target train steps or reward are reached\n",
    "            if (\n",
    "                result[\"timesteps_total\"] >= args.stop_timesteps\n",
    "                or result[\"episode_reward_mean\"] >= args.stop_reward\n",
    "            ):\n",
    "                break\n",
    "        algo.stop()\n",
    "    else:\n",
    "        # automated run with Tune and grid search and TensorBoard\n",
    "        print(\"Training automatically with Ray Tune\")\n",
    "        tuner = tune.Tuner(\n",
    "            args.run,\n",
    "            param_space=config.to_dict(),\n",
    "            run_config=air.RunConfig(stop=stop),\n",
    "        )\n",
    "        results = tuner.fit()\n",
    "\n",
    "        if args.as_test:\n",
    "            print(\"Checking if learning goals were achieved\")\n",
    "            check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0c34bc-e6a9-4d82-a6ef-10429c1a45d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T13:54:07.214809Z",
     "iopub.status.busy": "2023-08-07T13:54:07.214524Z",
     "iopub.status.idle": "2023-08-07T13:54:37.455598Z",
     "shell.execute_reply": "2023-08-07T13:54:37.454984Z",
     "shell.execute_reply.started": "2023-08-07T13:54:07.214785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-07 13:54:35</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:13.63        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.9/31.3 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/12 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnvironment_e9642_00000</td><td style=\"text-align: right;\">           1</td><td>/home/tcazalet/ray_results/PPO/PPO_CustomEnvironment_e9642_00000_0_2023-08-07_13-54-22/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnvironment_e9642_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 13:54:22,103\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(pid=53220)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=53220)\u001b[0m 2023-08-07 13:54:28,829\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=53220)\u001b[0m 2023-08-07 13:54:28,829\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(pid=53274)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "2023-08-07 13:54:35,721\tERROR tune_controller.py:911 -- Trial task failed for trial PPO_CustomEnvironment_e9642_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/_private/worker.py\", line 2495, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=53220, ip=172.17.0.3, actor_id=109191e7d37e2b1c3d2c203401000000, repr=PPO)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 227, in _setup\n",
      "    self.add_workers(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 593, in add_workers\n",
      "    raise result.get()\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 481, in __fetch_result\n",
      "    result = ray.get(r)\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=53274, ip=172.17.0.3, actor_id=3918efa065e8de4e2145b86001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f55d226c3d0>)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 525, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1727, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1838, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 64, in __init__\n",
      "    self._initialize_loss_from_dummy_batch()\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/policy/policy.py\", line 1418, in _initialize_loss_from_dummy_batch\n",
      "    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 551, in compute_actions_from_input_dict\n",
      "    return self._compute_action_helper(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/utils/threading.py\", line 32, in wrapper\n",
      "    raise e\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n",
      "    return func(self, *a, **k)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1235, in _compute_action_helper\n",
      "    logp = action_dist.logp(actions)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/models/torch/torch_distributions.py\", line 467, in logp\n",
      "    flat_logps = tree.map_structure(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/tree/__init__.py\", line 435, in map_structure\n",
      "    [func(*args) for args in zip(*map(flatten, structures))])\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/tree/__init__.py\", line 435, in <listcomp>\n",
      "    [func(*args) for args in zip(*map(flatten, structures))])\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/models/torch/torch_distributions.py\", line 465, in map_\n",
      "    return dist.logp(val)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/models/torch/torch_distributions.py\", line 321, in logp\n",
      "    logps = torch.stack([cat.log_prob(act) for cat, act in zip(self._cats, value)])\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/models/torch/torch_distributions.py\", line 321, in <listcomp>\n",
      "    logps = torch.stack([cat.log_prob(act) for cat, act in zip(self._cats, value)])\n",
      "AttributeError: 'TorchCategorical' object has no attribute 'log_prob'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=53220, ip=172.17.0.3, actor_id=109191e7d37e2b1c3d2c203401000000, repr=PPO)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 517, in __init__\n",
      "    super().__init__(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 169, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 639, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 179, in __init__\n",
      "    raise e.args[0].args[2]\n",
      "AttributeError: 'TorchCategorical' object has no attribute 'log_prob'\n",
      "2023-08-07 13:54:35,731\tERROR tune.py:1144 -- Trials did not complete: [PPO_CustomEnvironment_e9642_00000]\n",
      "2023-08-07 13:54:35,733\tINFO tune.py:1148 -- Total run time: 13.67 seconds (13.62 seconds for the tuning loop).\n",
      "2023-08-07 13:54:35,736\tWARNING experiment_analysis.py:916 -- Failed to read the results for 1 trials:\n",
      "- /home/tcazalet/ray_results/PPO/PPO_CustomEnvironment_e9642_00000_0_2023-08-07_13-54-22\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.run = \"PPO\"\n",
    "            self.framework = \"torch\"\n",
    "            self.as_test = False\n",
    "            self.stop_iters = 50\n",
    "            self.stop_timesteps = 100000\n",
    "            self.stop_reward = 0.1\n",
    "            self.local_mode = False\n",
    "    \n",
    "    args = Args()\n",
    "    print(f\"Running with following options: {args.__dict__}\")\n",
    "\n",
    "    ray.init(local_mode=args.local_mode)\n",
    "    env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "        .framework(args.framework)\n",
    "        .rollouts(num_rollout_workers=1)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "\n",
    "\n",
    "    config[\"env\"] = CustomEnvironment\n",
    "    config[\"env_config\"] = run_config[\"env\"]\n",
    "\n",
    "    print(config)\n",
    "    config.multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": (None, env.observation_space, env.action_space, {}),\n",
    "            \"predator\": (None, env.observation_space, env.action_space, {}),\n",
    "        },\n",
    "        policy_mapping_fn = lambda x: \"prey\" if x <=env.num_prey else \"predator\",\n",
    "    )\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "    }\n",
    "    \n",
    "    # automated run with Tune and grid search and TensorBoard\n",
    "    print(\"Training automatically with Ray Tune\")\n",
    "    tuner = tune.Tuner(\n",
    "        args.run,\n",
    "        param_space=config.to_dict(),\n",
    "        run_config=air.RunConfig(stop=stop),\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        print(\"Checking if learning goals were achieved\")\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c932c0d-b372-4d4a-bafb-706cfa2cfb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MARL2_env",
   "language": "python",
   "name": "marl2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
