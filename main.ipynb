{
 "cells": [
  {
   "cell_type": "raw",
   "id": "35f7cf6e-8176-4d10-baa5-bf9d4249deee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T09:17:14.582582Z",
     "iopub.status.busy": "2023-08-07T09:17:14.582238Z",
     "iopub.status.idle": "2023-08-07T09:18:35.123919Z",
     "shell.execute_reply": "2023-08-07T09:18:35.123341Z",
     "shell.execute_reply.started": "2023-08-07T09:17:14.582559Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Example of a custom gym environment. Run this example for a demo.\n",
    "\n",
    "This example shows the usage of:\n",
    "  - a custom environment\n",
    "  - Ray Tune for grid search to try different learning rates\n",
    "\n",
    "You can visualize experiment results in ~/ray_results using TensorBoard.\n",
    "\n",
    "Run example with defaults:\n",
    "$ python custom_env.py\n",
    "For CLI options:\n",
    "$ python custom_env.py --help\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
    "\n",
    "    You can configure the length of the corridor via the env config.\"\"\"\n",
    "\n",
    "    def __init__(self, config: EnvContext):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(0.0, self.end_pos, shape=(1,), dtype=np.float32)\n",
    "        # Set the seed. This is only used for the final (reach goal) reward.\n",
    "        self.reset(seed=config.worker_index * config.num_workers)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        random.seed(seed)\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos], {}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        done = truncated = self.cur_pos >= self.end_pos\n",
    "        # Produce a random reward when we reach the goal.\n",
    "        return (\n",
    "            [self.cur_pos],\n",
    "            random.random() * 2 if done else -0.1,\n",
    "            done,\n",
    "            truncated,\n",
    "            {},\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.run = \"PPO\"\n",
    "            self.framework = \"torch\"\n",
    "            self.as_test = False\n",
    "            self.stop_iters = 50\n",
    "            self.stop_timesteps = 100000\n",
    "            self.stop_reward = 0.1\n",
    "            self.no_tune = False\n",
    "            self.local_mode = False\n",
    "    \n",
    "    args = Args()\n",
    "    print(f\"Running with following options: {args.__dict__}\")\n",
    "    print(f\"Running with following CLI options: {args}\")\n",
    "\n",
    "    ray.init(local_mode=args.local_mode)\n",
    "\n",
    "    # Can also register the env creator function explicitly with:\n",
    "    # register_env(\"corridor\", lambda config: SimpleCorridor(config))\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        # or \"corridor\" if registered above\n",
    "        .environment(SimpleCorridor, env_config={\"corridor_length\": 5})\n",
    "        .framework(args.framework)\n",
    "        .rollouts(num_rollout_workers=1)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "    }\n",
    "\n",
    "    if args.no_tune:\n",
    "        # manual training with train loop using PPO and fixed learning rate\n",
    "        if args.run != \"PPO\":\n",
    "            raise ValueError(\"Only support --run PPO with --no-tune.\")\n",
    "        print(\"Running manual train loop without Ray Tune.\")\n",
    "        # use fixed learning rate instead of grid search (needs tune)\n",
    "        config.lr = 1e-3\n",
    "        algo = config.build()\n",
    "        # run manual training loop and print results after each iteration\n",
    "        for _ in range(args.stop_iters):\n",
    "            result = algo.train()\n",
    "            print(pretty_print(result))\n",
    "            # stop training of the target train steps or reward are reached\n",
    "            if (\n",
    "                result[\"timesteps_total\"] >= args.stop_timesteps\n",
    "                or result[\"episode_reward_mean\"] >= args.stop_reward\n",
    "            ):\n",
    "                break\n",
    "        algo.stop()\n",
    "    else:\n",
    "        # automated run with Tune and grid search and TensorBoard\n",
    "        print(\"Training automatically with Ray Tune\")\n",
    "        tuner = tune.Tuner(\n",
    "            args.run,\n",
    "            param_space=config.to_dict(),\n",
    "            run_config=air.RunConfig(stop=stop),\n",
    "        )\n",
    "        results = tuner.fit()\n",
    "\n",
    "        if args.as_test:\n",
    "            print(\"Checking if learning goals were achieved\")\n",
    "            check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0c34bc-e6a9-4d82-a6ef-10429c1a45d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T14:11:32.631025Z",
     "iopub.status.busy": "2023-08-07T14:11:32.630660Z",
     "iopub.status.idle": "2023-08-07T14:12:04.192136Z",
     "shell.execute_reply": "2023-08-07T14:12:04.191572Z",
     "shell.execute_reply.started": "2023-08-07T14:11:32.631006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-07 14:12:02</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:20.60        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.1/31.3 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/12 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnvironment_54e44_00000</td><td style=\"text-align: right;\">           1</td><td>/home/tcazalet/ray_results/PPO/PPO_CustomEnvironment_54e44_00000_0_2023-08-07_14-11-41/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnvironment_54e44_00000</td><td>ERROR   </td><td>172.17.0.3:56671</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 14:11:41,427\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "2023-08-07 14:11:41,431\tWARNING algorithm_config.py:2548 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(pid=56671)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:49,516\tWARNING algorithm_config.py:2534 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:49,517\tWARNING algorithm_config.py:656 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:49,517\tWARNING algorithm_config.py:2548 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.\n",
      "\u001b[2m\u001b[36m(pid=56725)\u001b[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,299\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,299\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_modelv2.TFModelV2` has been deprecated. Use `ray.rllib.core.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,317\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.misc.normc_initializer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,385\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,385\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,385\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,408\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,409\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiCategorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,410\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.Categorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.Categorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,411\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.TFActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,502\tWARNING deprecation.py:50 -- DeprecationWarning: `TFPolicy` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,647\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,647\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,649\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=56725)\u001b[0m 2023-08-07 14:11:56,651\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m Trainable.setup took 12.426 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,139\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,139\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_modelv2.TFModelV2` has been deprecated. Use `ray.rllib.core.rl_module.RLModule` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,144\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.misc.normc_initializer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,202\tWARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,202\tWARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,202\tWARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,221\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,222\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.MultiCategorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfMultiCategorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,223\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.Categorical` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.Categorical` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,224\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.tf.tf_action_dist.TFActionDistribution` has been deprecated. Use `ray.rllib.models.tf.tf_distributions.TfDistribution` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,447\tWARNING deprecation.py:50 -- DeprecationWarning: `TFPolicy` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,508\tWARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,508\tWARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,510\tWARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:11:59,512\tWARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!\n",
      "2023-08-07 14:12:02,013\tERROR tune_controller.py:911 -- Trial task failed for trial PPO_CustomEnvironment_54e44_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/_private/worker.py\", line 2493, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::PPO.train()\u001b[39m (pid=56671, ip=172.17.0.3, actor_id=768a48b34e61602c6400fdb901000000, repr=PPO)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 375, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 372, in train\n",
      "    result = self.step()\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 851, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py\", line 2835, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo.py\", line 429, in training_step\n",
      "    train_batch = synchronous_parallel_sample(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/execution/rollout_ops.py\", line 85, in synchronous_parallel_sample\n",
      "    sample_batches = worker_set.foreach_worker(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 680, in foreach_worker\n",
      "    handle_remote_call_result_errors(remote_results, self._ignore_worker_failures)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\", line 76, in handle_remote_call_result_errors\n",
      "    raise r.get()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=56725, ip=172.17.0.3, actor_id=ff40ea5b6840b3799a8bc2ca01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0692f7c3d0>)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "    lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 696, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "    item = next(self._env_runner)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n",
      "    outputs = self.step()\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 370, in step\n",
      "    active_envs, to_eval, outputs = self._process_observations(\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 536, in _process_observations\n",
      "    policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "  File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/episode_v2.py\", line 120, in policy_for\n",
      "    policy_id = self._agent_to_policy[agent_id] = self.policy_mapping_fn(\n",
      "TypeError: <lambda>() got an unexpected keyword argument 'worker'\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m 2023-08-07 14:12:02,008\tERROR actor_manager.py:500 -- Ray error, taking actor 1 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=56725, ip=172.17.0.3, actor_id=ff40ea5b6840b3799a8bc2ca01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0692f7c3d0>)\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/execution/rollout_ops.py\", line 86, in <lambda>\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     lambda w: w.sample(), local_worker=False, healthy_only=True\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 696, in sample\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     batches = [self.input_reader.next()]\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     batches = [self.get_data()]\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py\", line 277, in get_data\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     item = next(self._env_runner)\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 344, in run\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     outputs = self.step()\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 370, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     active_envs, to_eval, outputs = self._process_observations(\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/env_runner_v2.py\", line 536, in _process_observations\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     policy_id: PolicyID = episode.policy_for(agent_id)\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m   File \"/project_ghent/MARL2/MARL2_env/lib/python3.8/site-packages/ray/rllib/evaluation/episode_v2.py\", line 120, in policy_for\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m     policy_id = self._agent_to_policy[agent_id] = self.policy_mapping_fn(\n",
      "\u001b[2m\u001b[36m(PPO pid=56671)\u001b[0m TypeError: <lambda>() got an unexpected keyword argument 'worker'\n",
      "2023-08-07 14:12:02,673\tERROR tune.py:1144 -- Trials did not complete: [PPO_CustomEnvironment_54e44_00000]\n",
      "2023-08-07 14:12:02,674\tINFO tune.py:1148 -- Total run time: 21.27 seconds (20.59 seconds for the tuning loop).\n",
      "2023-08-07 14:12:02,678\tWARNING experiment_analysis.py:916 -- Failed to read the results for 1 trials:\n",
      "- /home/tcazalet/ray_results/PPO/PPO_CustomEnvironment_54e44_00000_0_2023-08-07_14-11-41\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray import air, tune\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import get_trainable_cls\n",
    "\n",
    "from custom_env import CustomEnvironment\n",
    "from config import run_config\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.run = \"PPO\"\n",
    "            self.framework = \"torch\" \n",
    "            self.as_test = False\n",
    "            self.stop_iters = 50\n",
    "            self.stop_timesteps = 100000\n",
    "            self.stop_reward = 0.1\n",
    "            self.local_mode = False\n",
    "    \n",
    "    args = Args()\n",
    "    print(f\"Running with following options: {args.__dict__}\")\n",
    "\n",
    "    ray.init(local_mode=args.local_mode)\n",
    "    env = CustomEnvironment(run_config[\"env\"])\n",
    "\n",
    "    config = (\n",
    "        get_trainable_cls(args.run)\n",
    "        .get_default_config()\n",
    "        .environment(CustomEnvironment, env_config=run_config[\"env\"])\n",
    "        .framework(args.framework)\n",
    "        .rollouts(num_rollout_workers=1)\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    "    )\n",
    "    \n",
    "    config.multi_agent(\n",
    "        policies= {\n",
    "            \"prey\": (None, env.observation_space, env.action_space, {}),\n",
    "            \"predator\": (None, env.observation_space, env.action_space, {}),\n",
    "        },\n",
    "        policy_mapping_fn = lambda x: \"prey\" if x <=env.num_prey else \"predator\",\n",
    "    )\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "    }\n",
    "    \n",
    "    # automated run with Tune and grid search and TensorBoard\n",
    "    print(\"Training automatically with Ray Tune\")\n",
    "    tuner = tune.Tuner(\n",
    "        args.run,\n",
    "        param_space=config.to_dict(),\n",
    "        run_config=air.RunConfig(stop=stop),\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    if args.as_test:\n",
    "        print(\"Checking if learning goals were achieved\")\n",
    "        check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c932c0d-b372-4d4a-bafb-706cfa2cfb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MARL2_env",
   "language": "python",
   "name": "marl2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
